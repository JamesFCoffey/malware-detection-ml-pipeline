{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "42d0d100-0c02-4025-9b2c-ca6a5eb101c7",
   "metadata": {},
   "source": [
    "# Explore Features\n",
    "\n",
    "This notebook is designed to ingest data, clean the dataset, conduct exploratory data analysis, engineer features, and establish a data preprocessing step within the machine learning pipeline.\n",
    "\n",
    "## Setup Notebook\n",
    "\n",
    "### Connect to EMR Cluster for Spark Session"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "08dfa9cd-2009-4775-9c3f-3cc133cee635",
   "metadata": {
    "kernelspec": {
     "display_name": "SparkMagic PySpark",
     "language": "python",
     "name": "pysparkkernel"
    },
    "language_info": {
     "codemirror_mode": {
      "name": "python",
      "version": 3
     },
     "file_extension": ".py",
     "mimetype": "text/x-python",
     "name": "pyspark",
     "pygments_lexer": "python3"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The sagemaker_studio_analytics_extension.magics extension is already loaded. To reload it, use:\n",
      "  %reload_ext sagemaker_studio_analytics_extension.magics\n",
      "Successfully read emr cluster(j-19Q76C3OC32QD) details\n",
      "Initiating EMR connection..\n",
      "Starting Spark application\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<table>\n",
       "<tr><th>ID</th><th>YARN Application ID</th><th>Kind</th><th>State</th><th>Spark UI</th><th>Driver log</th><th>User</th><th>Current session?</th></tr><tr><td>0</td><td>application_1709421197197_0002</td><td>pyspark</td><td>idle</td><td><a target=\"_blank\" href=\"http://ip-10-0-20-75.ec2.internal:20888/proxy/application_1709421197197_0002/\">Link</a></td><td><a target=\"_blank\" href=\"http://ip-10-0-20-252.ec2.internal:8042/node/containerlogs/container_1709421197197_0002_01_000001/livy\">Link</a></td><td>None</td><td>✔</td></tr></table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SparkSession available as 'spark'.\n",
      "{\"namespace\": \"sagemaker-analytics\", \"cluster_id\": \"j-19Q76C3OC32QD\", \"error_message\": null, \"success\": true, \"service\": \"emr\", \"operation\": \"connect\"}\n"
     ]
    }
   ],
   "source": [
    "%load_ext sagemaker_studio_analytics_extension.magics\n",
    "%sm_analytics emr connect --verify-certificate False --cluster-id j-19Q76C3OC32QD --auth-type None --language python  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b8fec5ee-f552-42d0-b339-00bd96a780bc",
   "metadata": {},
   "source": [
    "### Import Necessary Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "id": "fc0b9c39-d959-4576-97dc-2b9bde0f5a83",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import (\n",
    "    from_unixtime,\n",
    "    to_timestamp,\n",
    "    min,\n",
    "    max,\n",
    "    sum,\n",
    "    avg,\n",
    "    col,\n",
    "    countDistinct,\n",
    "    broadcast,\n",
    "    date_trunc,\n",
    "    count,\n",
    ")\n",
    "from pyspark.sql import Window\n",
    "import pyspark.sql.functions as F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "id": "51ef4814-f792-43c6-8403-532b4b7c36fe",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "'3.3.0-amzn-1'"
     ]
    }
   ],
   "source": [
    "spark = (\n",
    "    SparkSession.builder.appName('malware-detection')\n",
    "    .getOrCreate()\n",
    ")\n",
    "spark.sparkContext.setLogLevel('ERROR')\n",
    "spark.sparkContext.version"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "962b99c5-ff63-4810-a730-1da27d9a33e1",
   "metadata": {},
   "source": [
    "## Ingest Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "id": "251d6329-15d5-43f5-b1e0-175de2cb2f55",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import boto3\n",
    "\n",
    "s3 = boto3.resource('s3')\n",
    "bucket = s3.Bucket('datasets-20240228')\n",
    "datasets = []\n",
    "for object in bucket.objects.all():\n",
    "    datasets.append('s3://' + object.bucket_name + '/' + object.key)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "id": "83ce7afb-47ce-46e7-b6ea-5edd596b2619",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------------+------------------+---------------+---------+---------------+---------+-----+-------+--------+----------+----------+----------+----------+----------+------------+-------+---------+-------------+---------+-------------+--------------+---------+--------------------+\n",
      "|                 ts|               uid|      id.orig_h|id.orig_p|      id.resp_h|id.resp_p|proto|service|duration|orig_bytes|resp_bytes|conn_state|local_orig|local_resp|missed_bytes|history|orig_pkts|orig_ip_bytes|resp_pkts|resp_ip_bytes|tunnel_parents|    label|      detailed-label|\n",
      "+-------------------+------------------+---------------+---------+---------------+---------+-----+-------+--------+----------+----------+----------+----------+----------+------------+-------+---------+-------------+---------+-------------+--------------+---------+--------------------+\n",
      "|1.525879831015811E9|CUmrqr4svHuSXJy5z7|192.168.100.103|  51524.0| 65.127.233.163|     23.0|  tcp|      -|2.999051|         0|         0|        S0|         -|         -|         0.0|      S|      3.0|        180.0|      0.0|          0.0|             -|Malicious|PartOfAHorizontal...|\n",
      "|1.525879831025055E9|CH98aB3s1kJeq6SFOc|192.168.100.103|  56305.0|  63.150.16.171|     23.0|  tcp|      -|       -|         -|         -|        S0|         -|         -|         0.0|      S|      1.0|         60.0|      0.0|          0.0|             -|Malicious|PartOfAHorizontal...|\n",
      "|1.525879831045045E9| C3GBTkINvXNjVGtN5|192.168.100.103|  41101.0|   111.40.23.49|     23.0|  tcp|      -|       -|         -|         -|        S0|         -|         -|         0.0|      S|      1.0|         60.0|      0.0|          0.0|             -|Malicious|PartOfAHorizontal...|\n",
      "| 1.52587983201624E9| CDe43c1PtgynajGI6|192.168.100.103|  60905.0|131.174.215.147|     23.0|  tcp|      -|2.998796|         0|         0|        S0|         -|         -|         0.0|      S|      3.0|        180.0|      0.0|          0.0|             -|Malicious|PartOfAHorizontal...|\n",
      "|1.525879832024985E9|CJaDcG3MZzvf1YVYI4|192.168.100.103|  44301.0|    91.42.47.63|     23.0|  tcp|      -|       -|         -|         -|        S0|         -|         -|         0.0|      S|      1.0|         60.0|      0.0|          0.0|             -|Malicious|PartOfAHorizontal...|\n",
      "+-------------------+------------------+---------------+---------+---------------+---------+-----+-------+--------+----------+----------+----------+----------+----------+------------+-------+---------+-------------+---------+-------------+--------------+---------+--------------------+\n",
      "only showing top 5 rows"
     ]
    }
   ],
   "source": [
    "df = spark.read.option('delimiter', '|').csv(datasets, inferSchema = True, header = True)\n",
    "df.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "id": "8622fac5-4f8d-41dc-bd79-a2e6068efc82",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- ts: double (nullable = true)\n",
      " |-- uid: string (nullable = true)\n",
      " |-- id.orig_h: string (nullable = true)\n",
      " |-- id.orig_p: double (nullable = true)\n",
      " |-- id.resp_h: string (nullable = true)\n",
      " |-- id.resp_p: double (nullable = true)\n",
      " |-- proto: string (nullable = true)\n",
      " |-- service: string (nullable = true)\n",
      " |-- duration: string (nullable = true)\n",
      " |-- orig_bytes: string (nullable = true)\n",
      " |-- resp_bytes: string (nullable = true)\n",
      " |-- conn_state: string (nullable = true)\n",
      " |-- local_orig: string (nullable = true)\n",
      " |-- local_resp: string (nullable = true)\n",
      " |-- missed_bytes: double (nullable = true)\n",
      " |-- history: string (nullable = true)\n",
      " |-- orig_pkts: double (nullable = true)\n",
      " |-- orig_ip_bytes: double (nullable = true)\n",
      " |-- resp_pkts: double (nullable = true)\n",
      " |-- resp_ip_bytes: double (nullable = true)\n",
      " |-- tunnel_parents: string (nullable = true)\n",
      " |-- label: string (nullable = true)\n",
      " |-- detailed-label: string (nullable = true)"
     ]
    }
   ],
   "source": [
    "df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "id": "dd26725b-615e-497c-a17a-074cb80625a0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(25011003, 23)"
     ]
    }
   ],
   "source": [
    "df.count(), len(df.columns)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "60793690-80cf-4304-b386-7418443816c3",
   "metadata": {},
   "source": [
    "## Clean the Data\n",
    "\n",
    "### Ensure Features Have Clear and Obvious Meanings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "id": "77dd5571-2c47-42ed-bb50-76364823dff5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------------+------------------+---------------+-----------+---------------+---------+-----+-------+--------+------------+----------+----------+------------+----------+------------+-------+-----------+---------------+---------+-------------+--------------+---------+--------------------+\n",
      "|     unix_timestamp|         unique_id|      source_ip|source_port|        dest_ip|dest_port|proto|service|duration|source_bytes|dest_bytes|conn_state|local_source|local_dest|missed_bytes|history|source_pkts|source_ip_bytes|dest_pkts|dest_ip_bytes|tunnel_parents|    label|      detailed-label|\n",
      "+-------------------+------------------+---------------+-----------+---------------+---------+-----+-------+--------+------------+----------+----------+------------+----------+------------+-------+-----------+---------------+---------+-------------+--------------+---------+--------------------+\n",
      "|1.525879831015811E9|CUmrqr4svHuSXJy5z7|192.168.100.103|    51524.0| 65.127.233.163|     23.0|  tcp|      -|2.999051|           0|         0|        S0|           -|         -|         0.0|      S|        3.0|          180.0|      0.0|          0.0|             -|Malicious|PartOfAHorizontal...|\n",
      "|1.525879831025055E9|CH98aB3s1kJeq6SFOc|192.168.100.103|    56305.0|  63.150.16.171|     23.0|  tcp|      -|       -|           -|         -|        S0|           -|         -|         0.0|      S|        1.0|           60.0|      0.0|          0.0|             -|Malicious|PartOfAHorizontal...|\n",
      "|1.525879831045045E9| C3GBTkINvXNjVGtN5|192.168.100.103|    41101.0|   111.40.23.49|     23.0|  tcp|      -|       -|           -|         -|        S0|           -|         -|         0.0|      S|        1.0|           60.0|      0.0|          0.0|             -|Malicious|PartOfAHorizontal...|\n",
      "| 1.52587983201624E9| CDe43c1PtgynajGI6|192.168.100.103|    60905.0|131.174.215.147|     23.0|  tcp|      -|2.998796|           0|         0|        S0|           -|         -|         0.0|      S|        3.0|          180.0|      0.0|          0.0|             -|Malicious|PartOfAHorizontal...|\n",
      "|1.525879832024985E9|CJaDcG3MZzvf1YVYI4|192.168.100.103|    44301.0|    91.42.47.63|     23.0|  tcp|      -|       -|           -|         -|        S0|           -|         -|         0.0|      S|        1.0|           60.0|      0.0|          0.0|             -|Malicious|PartOfAHorizontal...|\n",
      "+-------------------+------------------+---------------+-----------+---------------+---------+-----+-------+--------+------------+----------+----------+------------+----------+------------+-------+-----------+---------------+---------+-------------+--------------+---------+--------------------+\n",
      "only showing top 5 rows"
     ]
    }
   ],
   "source": [
    "df = df.toDF(\n",
    "    'unix_timestamp',\n",
    "    'unique_id',\n",
    "    'source_ip',\n",
    "    'source_port',\n",
    "    'dest_ip',\n",
    "    'dest_port',\n",
    "    'proto',\n",
    "    'service',\n",
    "    'duration',\n",
    "    'source_bytes',\n",
    "    'dest_bytes',\n",
    "    'conn_state',\n",
    "    'local_source',\n",
    "    'local_dest',\n",
    "    'missed_bytes',\n",
    "    'history',\n",
    "    'source_pkts',\n",
    "    'source_ip_bytes',\n",
    "    'dest_pkts',\n",
    "    'dest_ip_bytes',\n",
    "    'tunnel_parents',\n",
    "    'label',\n",
    "    'detailed-label',\n",
    ")\n",
    "df.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "id": "f5ae3489-0cb4-4cdd-ac6b-0f1527032f4b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "df = df.withColumn('datetime', from_unixtime('unix_timestamp')).withColumn('datetime', to_timestamp('datetime'))\n",
    "df = df.drop('unix_timestamp')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "id": "48d58942-af1f-4ad2-9831-081e36a54bc9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "df = df.replace('-', None)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a871265-f0c5-41dd-a23d-b8be3846ea61",
   "metadata": {},
   "source": [
    "### Drop Duplicates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "id": "3b078b05-b1d9-41f6-80bc-c7fcf5c0fc9e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "df = df.dropDuplicates()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "027f1851-07b1-4a11-b0b3-11664d328e7d",
   "metadata": {},
   "source": [
    "### Drop Static Columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "id": "789a7d05-c98d-437e-bdd0-8890c66c075e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+---------+-----------+--------+---------+-----+-------+--------+------------+----------+----------+------------+----------+------------+-------+-----------+---------------+---------+-------------+--------------+-----+--------------+\n",
      "|unique_id|source_ip|source_port| dest_ip|dest_port|proto|service|duration|source_bytes|dest_bytes|conn_state|local_source|local_dest|missed_bytes|history|source_pkts|source_ip_bytes|dest_pkts|dest_ip_bytes|tunnel_parents|label|detailed-label|\n",
      "+---------+---------+-----------+--------+---------+-----+-------+--------+------------+----------+----------+------------+----------+------------+-------+-----------+---------------+---------+-------------+--------------+-----+--------------+\n",
      "| 25011003|    21442|      65536|11654579|    65435|    3|      6| 1045962|       66148|       885|        13|           0|         0|          23|    263|        207|           1858|      129|         1889|             0|    7|             6|\n",
      "+---------+---------+-----------+--------+---------+-----+-------+--------+------------+----------+----------+------------+----------+------------+-------+-----------+---------------+---------+-------------+--------------+-----+--------------+"
     ]
    }
   ],
   "source": [
    "columns_to_analyze = [\n",
    "    'unique_id',\n",
    "    'source_ip',\n",
    "    'source_port',\n",
    "    'dest_ip',\n",
    "    'dest_port',\n",
    "    'proto',\n",
    "    'service',\n",
    "    'duration',\n",
    "    'source_bytes',\n",
    "    'dest_bytes',\n",
    "    'conn_state',\n",
    "    'local_source',\n",
    "    'local_dest',\n",
    "    'missed_bytes',\n",
    "    'history',\n",
    "    'source_pkts',\n",
    "    'source_ip_bytes',\n",
    "    'dest_pkts',\n",
    "    'dest_ip_bytes',\n",
    "    'tunnel_parents',\n",
    "    'label',\n",
    "    'detailed-label',\n",
    "]\n",
    "df.agg(*(countDistinct(col(i)).alias(i) for i in columns_to_analyze)).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "970e25bd-375b-4681-b185-ac6e4056a55c",
   "metadata": {},
   "source": [
    "There are no static columns (`countDistinct = 1`) to drop."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b2f530ff-0cf8-4261-8a99-43b50d9d4100",
   "metadata": {},
   "source": [
    "### Drop Empty Columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "id": "2257b53a-891e-4c12-8d7b-e17dde8b9dbb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+---------+-----------+-------+---------+-----+--------+--------+------------+----------+----------+------------+----------+------------+-------+-----------+---------------+---------+-------------+--------------+-----+--------------+\n",
      "|unique_id|source_ip|source_port|dest_ip|dest_port|proto| service|duration|source_bytes|dest_bytes|conn_state|local_source|local_dest|missed_bytes|history|source_pkts|source_ip_bytes|dest_pkts|dest_ip_bytes|tunnel_parents|label|detailed-label|\n",
      "+---------+---------+-----------+-------+---------+-----+--------+--------+------------+----------+----------+------------+----------+------------+-------+-----------+---------------+---------+-------------+--------------+-----+--------------+\n",
      "|        0|        0|          0|      0|        0|    0|24993006|15272073|    15272073|  15272073|         0|    25011003|  25011003|           0|  25116|          0|              0|        0|            0|      25011003|    0|      17954112|\n",
      "+---------+---------+-----------+-------+---------+-----+--------+--------+------------+----------+----------+------------+----------+------------+-------+-----------+---------------+---------+-------------+--------------+-----+--------------+"
     ]
    }
   ],
   "source": [
    "df.select([count(F.when(F.isnan(i) | col(i).isNull(), i)).alias(i) for i in columns_to_analyze]).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 164,
   "id": "eb58dc11-425e-4aa2-b103-a608fe29fdc7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "empty_columns = ['local_source', 'local_dest', 'tunnel_parents']\n",
    "df = df.drop(*empty_columns)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8251df59-72e3-4297-bcf1-e16add05b457",
   "metadata": {},
   "source": [
    "### Drop Unnecessary Columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 165,
   "id": "02e487b7-ac53-4969-a5c5-ff9783bea6ff",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "unnecessary_columns = ['unique_id', 'detailed-label']\n",
    "df = df.drop(*unnecessary_columns)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "97b3e62b-bc2b-4a0c-80be-6967fa5b5780",
   "metadata": {},
   "source": [
    "### Convert columns to appropriate data types"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 166,
   "id": "67d7670b-7b8b-4311-89b2-31bb32b1702b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "df = df.withColumns({\n",
    "    'source_ip': col('source_ip').cast('string'),\n",
    "    'source_port': col('source_port').cast('string'),\n",
    "    'dest_ip': col('dest_ip').cast('string'),\n",
    "    'dest_port': col('dest_port').cast('string'),\n",
    "    'proto': col('proto').cast('string'),\n",
    "    'service': col('service').cast('string'),\n",
    "    'duration': col('duration').cast('double'),\n",
    "    'source_bytes': col('source_bytes').cast('double'),\n",
    "    'dest_bytes': col('dest_bytes').cast('double'),\n",
    "    'conn_state': col('conn_state').cast('string'),\n",
    "    'missed_bytes': col('missed_bytes').cast('double'),\n",
    "    'history': col('history').cast('string'),\n",
    "    'source_pkts': col('source_pkts').cast('double'),\n",
    "    'source_ip_bytes': col('source_ip_bytes').cast('double'),\n",
    "    'dest_pkts': col('dest_pkts').cast('double'),\n",
    "    'dest_ip_bytes': col('dest_ip_bytes').cast('double'),\n",
    "    'label': col('label').cast('string'),\n",
    "})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ad43b88-4d91-45e0-8900-da207111d657",
   "metadata": {},
   "source": [
    "## Exploratory Data Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 167,
   "id": "bd41078f-af2c-461a-ab22-dfe771a77f93",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "categorical = ['source_ip', 'source_port', 'dest_ip', 'dest_port', 'proto', 'service', 'conn_state', 'history', 'label']\n",
    "numeric = ['duration', 'source_bytes', 'dest_bytes', 'missed_bytes',  'source_pkts', 'source_ip_bytes', 'dest_pkts', 'dest_ip_bytes']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "87b61b7b-0ae6-4182-baca-c7136d06760e",
   "metadata": {},
   "source": [
    "### Numeric"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 168,
   "id": "2cd09f1c-b00b-444b-8ffd-24f2e5b9d77a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+------------------+--------------------+--------------------+------------------+------------------+-----------------+------------------+------------------+\n",
      "|summary|          duration|        source_bytes|          dest_bytes|      missed_bytes|       source_pkts|  source_ip_bytes|         dest_pkts|     dest_ip_bytes|\n",
      "+-------+------------------+--------------------+--------------------+------------------+------------------+-----------------+------------------+------------------+\n",
      "|  count|           9738930|             9738930|             9738930|          25011003|          25011003|         25011003|          25011003|          25011003|\n",
      "|   mean|3.3814935718510615|2.1988843553368482E8|   3455.710982212625| 76.32837799427716|13.503795469537947|590.0188299925437|0.0146530708904397|1.7540757961605937|\n",
      "| stddev|60.005832067196046| 2.730752543441623E9|1.0182852901891654E7|381679.91286032816|17511.696998324758|687810.3200257953|  4.18936507908476|376.19547708056814|\n",
      "|    min|            1.0E-6|                 0.0|                 0.0|               0.0|               0.0|              0.0|               0.0|               0.0|\n",
      "|    25%|          2.997257|                 0.0|                 0.0|               0.0|               1.0|             40.0|               0.0|               0.0|\n",
      "|    50%|          3.111741|                 0.0|                 0.0|               0.0|               1.0|             60.0|               0.0|               0.0|\n",
      "|    75%|          3.144734|                 0.0|                 0.0|               0.0|               3.0|            180.0|               0.0|               0.0|\n",
      "|    max|      93280.030966|     6.6205578295E10|     3.1720511878E10|      1.90881948E9|       6.6027354E7|    1.914793266E9|            9613.0|          520116.0|\n",
      "+-------+------------------+--------------------+--------------------+------------------+------------------+-----------------+------------------+------------------+"
     ]
    }
   ],
   "source": [
    "df.select(*numeric).summary().show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 169,
   "id": "a4a13bb7-6dca-4d78-b66c-b83bca780a65",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "df = df.withColumns({\n",
    "    'is_duration_defined': F.when(F.isnan('duration') | col('duration').isNull(), 0.0).otherwise(1.0),\n",
    "    'is_source_bytes_defined': F.when(F.isnan('source_bytes') | col('source_bytes').isNull(), 0.0).otherwise(1.0),\n",
    "    'is_dest_bytes_defined': F.when(F.isnan('dest_bytes') | col('dest_bytes').isNull(), 0.0).otherwise(1.0),\n",
    "})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 170,
   "id": "263cbfd7-c05f-4188-ba7a-0d1b2654633a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "df = df.fillna({\n",
    "    'duration': df.select(avg('duration')).first()[0],\n",
    "    'source_bytes': df.select(avg('source_bytes')).first()[0],\n",
    "    'dest_bytes': df.select(avg('dest_bytes')).first()[0],\n",
    "})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "28ee7944-1ab9-44e0-9440-f0d32cccecf3",
   "metadata": {},
   "source": [
    "### Categorical"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 171,
   "id": "509eb271-af2d-46fc-9af7-60f067ca7d85",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+--------+\n",
      "|proto|count   |\n",
      "+-----+--------+\n",
      "|udp  |442494  |\n",
      "|tcp  |24543393|\n",
      "|icmp |25116   |\n",
      "+-----+--------+"
     ]
    }
   ],
   "source": [
    "df.groupBy('proto').count().show(truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "5025038c-a7a7-42e2-9fd5-75eb8699fa61",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+--------+\n",
      "|service|count   |\n",
      "+-------+--------+\n",
      "|null   |24993006|\n",
      "|dns    |7033    |\n",
      "|http   |3396    |\n",
      "|irc    |1647    |\n",
      "|ssl    |11      |\n",
      "|ssh    |5899    |\n",
      "|dhcp   |11      |\n",
      "+-------+--------+"
     ]
    }
   ],
   "source": [
    "df.groupBy('service').count().show(truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "19ce4912-ad5c-4550-892e-2f6aeafa0082",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+--------+\n",
      "|conn_state|count   |\n",
      "+----------+--------+\n",
      "|RSTOS0    |2119852 |\n",
      "|S0        |19151121|\n",
      "|S1        |131     |\n",
      "|RSTO      |514     |\n",
      "|S2        |136     |\n",
      "|OTH       |3685336 |\n",
      "|RSTR      |1917    |\n",
      "|S3        |2458    |\n",
      "|SF        |33267   |\n",
      "|REJ       |16072   |\n",
      "|SH        |120     |\n",
      "|RSTRH     |78      |\n",
      "|SHR       |1       |\n",
      "+----------+--------+"
     ]
    }
   ],
   "source": [
    "df.groupBy('conn_state').count().show(truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "3cb3ac3d-d537-4b8f-829f-78b82d06ad76",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+--------+\n",
      "|history    |count   |\n",
      "+-----------+--------+\n",
      "|ShAdDaf    |2263    |\n",
      "|null       |25116   |\n",
      "|ShADdattFfR|8       |\n",
      "|D          |422864  |\n",
      "|ShADfFa    |110     |\n",
      "|R          |341     |\n",
      "|ShAdDaftF  |151     |\n",
      "|ShAdDaFRfRR|11      |\n",
      "|ShAdDaF    |28      |\n",
      "|ShAdDaTFf  |23      |\n",
      "|ShAdDaFR   |17      |\n",
      "|ShADafF    |740     |\n",
      "|DTT        |65534   |\n",
      "|S          |18728237|\n",
      "|ShADadR    |7       |\n",
      "|ShAdFaRf   |1       |\n",
      "|SI         |10      |\n",
      "|^c         |2013    |\n",
      "|ShAdDfFr   |228     |\n",
      "|ShADadfF   |1394    |\n",
      "+-----------+--------+\n",
      "only showing top 20 rows"
     ]
    }
   ],
   "source": [
    "df.groupBy('history').count().show(truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "d8115eaf-3948-4974-bf32-351892f2a3ca",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------------------------------+-------+\n",
      "|label                                |count  |\n",
      "+-------------------------------------+-------+\n",
      "|Malicious                            |7055007|\n",
      "|Malicious   PartOfAHorizontalPortScan|3386241|\n",
      "|Malicious   DDoS                     |5778154|\n",
      "|Benign                               |8780158|\n",
      "|Malicious   Attack                   |2755   |\n",
      "|Malicious   C&C                      |8685   |\n",
      "|Malicious   FileDownload             |3      |\n",
      "+-------------------------------------+-------+"
     ]
    }
   ],
   "source": [
    "df.groupBy('label').count().show(truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 172,
   "id": "712315f5-5843-4cb3-8295-8ffde7a4e17d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "df = df.withColumns({\n",
    "    'is_service_defined': F.when(F.isnan('service') | col('service').isNull(), 0.0).otherwise(1.0),\n",
    "    'is_history_defined': F.when(F.isnan('history') | col('history').isNull(), 0.0).otherwise(1.0),\n",
    "}).fillna({'service': 'missing', 'history': 'missing'})\n",
    "\n",
    "set_as_other = df.groupBy('history').count().where('count < 5').toPandas()['history'].tolist()\n",
    "df = df.withColumn('history', F.when(F.col('history').isin(set_as_other), 'other').otherwise(col('history')))\n",
    "df = df.where(\"label in ('Benign', 'Malicious')\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 173,
   "id": "a21a7b1c-401c-44dd-a70c-879003651688",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "def window_ops(column: str, operation: str ='count', time: str = 'datetime', minutes: int = 1, partition_by = None):\n",
    "    \"\"\"Counts or averages via a rolling aggregate over a specified window.\n",
    "\n",
    "    Args:\n",
    "        column (str): The column to perform the aggregation on.\n",
    "        operation (str): The aggregation operation ('count', 'sum', 'avg').\n",
    "        time (str): The column containing timestamp data.\n",
    "        minutes (int): The window size in minutes for the rolling aggregate.\n",
    "        partition_by (str, optional): The column to partition by. Defaults to None, in which case it uses `column`.\n",
    "\n",
    "    Returns:\n",
    "        Column: A PySpark Column object representing the rolling aggregate.\n",
    "\n",
    "    Raises:\n",
    "        ValueError: If the specified operation is not supported.\n",
    "    \"\"\"\n",
    "\n",
    "    if partition_by is None:\n",
    "        partition_by = column\n",
    "\n",
    "    window = Window.partitionBy(F.col(partition_by)).orderBy(F.col(time).cast('long')).rangeBetween(-60 * minutes, -1)\n",
    "\n",
    "    operations = {\n",
    "        'count': F.count(column).over(window),\n",
    "        'avg': F.avg(column).over(window),\n",
    "    }\n",
    "\n",
    "    if operation in operations:\n",
    "        return operations[operation]\n",
    "    else:\n",
    "        raise ValueError(f\"{operation} not defined\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 174,
   "id": "0492b94e-a7e6-4a5c-b060-f4a0035f82cc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------+-----------+---------------+---------+-----+-------+------------------+--------------------+-----------------+----------+------------+-------+-----------+---------------+---------+-------------+------+-------------------+-------------------+-----------------------+---------------------+------------------+------------------+----------------+-----------------+\n",
      "|    source_ip|source_port|        dest_ip|dest_port|proto|service|          duration|        source_bytes|       dest_bytes|conn_state|missed_bytes|history|source_pkts|source_ip_bytes|dest_pkts|dest_ip_bytes| label|           datetime|is_duration_defined|is_source_bytes_defined|is_dest_bytes_defined|is_service_defined|is_history_defined|visits_src_ip_1m|visits_src_ip_30m|\n",
      "+-------------+-----------+---------------+---------+-----+-------+------------------+--------------------+-----------------+----------+------------+-------+-----------+---------------+---------+-------------+------+-------------------+-------------------+-----------------------+---------------------+------------------+------------------+----------------+-----------------+\n",
      "|  1.1.153.232|        3.0|192.168.100.103|      3.0| icmp|missing|3.3814935718510615|2.1988843553368482E8|3455.710982212625|       OTH|         0.0|missing|        1.0|           68.0|      0.0|          0.0|Benign|2018-05-10 14:18:42|                0.0|                    0.0|                  0.0|               0.0|               0.0|               0|                0|\n",
      "|  1.109.51.12|        3.0|192.168.100.103|      3.0| icmp|missing|3.3814935718510615|2.1988843553368482E8|3455.710982212625|       OTH|         0.0|missing|        1.0|           68.0|      0.0|          0.0|Benign|2018-05-11 14:06:46|                0.0|                    0.0|                  0.0|               0.0|               0.0|               0|                0|\n",
      "| 1.160.85.183|        3.0|192.168.100.103|      3.0| icmp|missing|3.3814935718510615|2.1988843553368482E8|3455.710982212625|       OTH|         0.0|missing|        1.0|           68.0|      0.0|          0.0|Benign|2018-05-12 13:06:44|                0.0|                    0.0|                  0.0|               0.0|               0.0|               0|                0|\n",
      "|1.164.147.110|        3.0|192.168.100.103|      3.0| icmp|missing|3.3814935718510615|2.1988843553368482E8|3455.710982212625|       OTH|         0.0|missing|        1.0|           68.0|      0.0|          0.0|Benign|2018-05-09 15:51:29|                0.0|                    0.0|                  0.0|               0.0|               0.0|               0|                0|\n",
      "| 1.168.74.188|        3.0|192.168.100.103|      3.0| icmp|missing|3.3814935718510615|2.1988843553368482E8|3455.710982212625|       OTH|         0.0|missing|        1.0|           68.0|      0.0|          0.0|Benign|2018-05-11 13:34:30|                0.0|                    0.0|                  0.0|               0.0|               0.0|               0|                0|\n",
      "+-------------+-----------+---------------+---------+-----+-------+------------------+--------------------+-----------------+----------+------------+-------+-----------+---------------+---------+-------------+------+-------------------+-------------------+-----------------------+---------------------+------------------+------------------+----------------+-----------------+\n",
      "only showing top 5 rows"
     ]
    }
   ],
   "source": [
    "df = df.withColumns({\n",
    "    'visits_src_ip_1m': window_ops('source_ip', minutes=1),\n",
    "    'visits_src_ip_30m': window_ops('source_ip', minutes=30),\n",
    "})\n",
    "df.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 175,
   "id": "f3cdd24d-378d-4eab-9d50-c09c1b73e8c9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "df = df.drop('dest_ip', 'datetime')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 176,
   "id": "0ca114f6-6e8e-44f9-93f1-d09eb868898c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from pyspark.ml.feature import StringIndexer\n",
    "\n",
    "indexer = StringIndexer(inputCols=['proto', 'service', 'conn_state', 'history'],\n",
    "                        outputCols=['protoIdx', 'serviceIdx', 'conn_stateIdx', 'historyIdx'])\n",
    "df = indexer.fit(df).transform(df).drop('proto', 'service', 'conn_state', 'history')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 189,
   "id": "d860fb94-467f-4a82-b7e4-6194f0d90871",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "def oneHot(df, column):\n",
    "    values = df.select(column).distinct().toPandas()[column].tolist()\n",
    "    values.sort()\n",
    "    values.pop(0)\n",
    "\n",
    "    new_columns = {}\n",
    "    for value in values:\n",
    "        new_columns[f'{column}_{value}'] = F.when(col(column) == value, 1.0).otherwise(0.0)\n",
    "    return new_columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 190,
   "id": "aeda8936-42a1-415c-9b7c-ac710716b488",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "df = df.withColumns(oneHot(df, 'protoIdx')).drop('protoIdx')\n",
    "df = df.withColumns(oneHot(df, 'serviceIdx')).drop('serviceIdx')\n",
    "df = df.withColumns(oneHot(df, 'conn_stateIdx')).drop('conn_stateIdx')\n",
    "df = df.withColumns(oneHot(df, 'historyIdx')).drop('historyIdx')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d6b0c0c6-74db-468d-999a-3c203d9f8e09",
   "metadata": {},
   "outputs": [],
   "source": [
    "# encoder = OneHotEncoder(inputCols=['protoIdx', 'serviceIdx', 'conn_stateIdx', 'historyIdx'],\n",
    "#                         outputCols=['protoVec', 'serviceVec', 'conn_stateVec', 'historyVec'])\n",
    "# df = encoder.fit(df).transform(df).drop('protoIdx', 'serviceIdx', 'conn_stateIdx', 'historyIdx')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "df25fbdb-ecbc-4ce7-9d5b-52b04cc07f1c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------------+----------------+--------------------+\n",
      "|      source_ip|count(source_ip)|             percent|\n",
      "+---------------+----------------+--------------------+\n",
      "|192.168.100.113|            8222| 0.11654134432467608|\n",
      "|192.168.100.111|         6355745|   90.08842939489642|\n",
      "|    192.168.2.5|          151566|   2.148346557274855|\n",
      "|192.168.100.103|          539473|   7.646668529173678|\n",
      "|    192.168.2.1|               1|1.417433037274095E-5|\n",
      "+---------------+----------------+--------------------+"
     ]
    }
   ],
   "source": [
    "mal_total = df.select('source_ip').where('label = \"Malicious\"').agg(count('source_ip')).first()[0]\n",
    "df.select('source_ip') \\\n",
    "  .where('label = \"Malicious\"') \\\n",
    "  .groupBy('source_ip') \\\n",
    "  .agg(count('source_ip')) \\\n",
    "  .withColumn('percent', (F.col('count(source_ip)') / mal_total) * 100 ) \\\n",
    "  .show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 181,
   "id": "596aedb9-12cf-42e5-b26f-d4ce504c280a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "df = df.withColumn('label', F.when(F.col('label') == 'Malicious', 1.0).otherwise(0.0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 182,
   "id": "0d292fab-7ae9-49fb-a7de-21126cef3320",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------+-----------+---------+------------------+--------------------+-----------------+------------+-----------+---------------+---------+-------------+-----+-------------------+-----------------------+---------------------+------------------+------------------+----------------+-----------------+--------+----------+-------------+----------+\n",
      "|    source_ip|source_port|dest_port|          duration|        source_bytes|       dest_bytes|missed_bytes|source_pkts|source_ip_bytes|dest_pkts|dest_ip_bytes|label|is_duration_defined|is_source_bytes_defined|is_dest_bytes_defined|is_service_defined|is_history_defined|visits_src_ip_1m|visits_src_ip_30m|protoIdx|serviceIdx|conn_stateIdx|historyIdx|\n",
      "+-------------+-----------+---------+------------------+--------------------+-----------------+------------+-----------+---------------+---------+-------------+-----+-------------------+-----------------------+---------------------+------------------+------------------+----------------+-----------------+--------+----------+-------------+----------+\n",
      "|  1.1.153.232|        3.0|      3.0|3.3814935718510615|2.1988843553368482E8|3455.710982212625|         0.0|        1.0|           68.0|      0.0|          0.0|  0.0|                0.0|                    0.0|                  0.0|               0.0|               0.0|               0|                0|     2.0|       0.0|          2.0|       2.0|\n",
      "|  1.109.51.12|        3.0|      3.0|3.3814935718510615|2.1988843553368482E8|3455.710982212625|         0.0|        1.0|           68.0|      0.0|          0.0|  0.0|                0.0|                    0.0|                  0.0|               0.0|               0.0|               0|                0|     2.0|       0.0|          2.0|       2.0|\n",
      "| 1.160.85.183|        3.0|      3.0|3.3814935718510615|2.1988843553368482E8|3455.710982212625|         0.0|        1.0|           68.0|      0.0|          0.0|  0.0|                0.0|                    0.0|                  0.0|               0.0|               0.0|               0|                0|     2.0|       0.0|          2.0|       2.0|\n",
      "|1.164.147.110|        3.0|      3.0|3.3814935718510615|2.1988843553368482E8|3455.710982212625|         0.0|        1.0|           68.0|      0.0|          0.0|  0.0|                0.0|                    0.0|                  0.0|               0.0|               0.0|               0|                0|     2.0|       0.0|          2.0|       2.0|\n",
      "| 1.168.74.188|        3.0|      3.0|3.3814935718510615|2.1988843553368482E8|3455.710982212625|         0.0|        1.0|           68.0|      0.0|          0.0|  0.0|                0.0|                    0.0|                  0.0|               0.0|               0.0|               0|                0|     2.0|       0.0|          2.0|       2.0|\n",
      "+-------------+-----------+---------+------------------+--------------------+-----------------+------------+-----------+---------------+---------+-------------+-----+-------------------+-----------------------+---------------------+------------------+------------------+----------------+-----------------+--------+----------+-------------+----------+\n",
      "only showing top 5 rows"
     ]
    }
   ],
   "source": [
    "df.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 191,
   "id": "a37fe331-7656-470c-9cd8-af7ae76c0383",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "df = df.withColumns({\n",
    "    'duration': F.log(F.col('duration') + 1),\n",
    "    'source_bytes': F.log(F.col('source_bytes') + 1),\n",
    "    'dest_bytes': F.log(F.col('dest_bytes') + 1),\n",
    "    'missed_bytes': F.log(F.col('missed_bytes') + 1),\n",
    "    'source_pkts': F.log(F.col('source_pkts') + 1),\n",
    "    'source_ip_bytes': F.log(F.col('source_ip_bytes') + 1),\n",
    "    'dest_pkts': F.log(F.col('dest_pkts') + 1),\n",
    "    'dest_ip_bytes': F.log(F.col('dest_ip_bytes') + 1),\n",
    "})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 184,
   "id": "2eb4996e-89d0-4b5d-99ea-c8101926e443",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "min_max_df = df.agg(\n",
    "    F.min(\"visits_src_ip_1m\").alias(\"min_visits_1m\"),\n",
    "    F.max(\"visits_src_ip_1m\").alias(\"max_visits_1m\"),\n",
    "    F.min(\"visits_src_ip_30m\").alias(\"min_visits_30m\"),\n",
    "    F.max(\"visits_src_ip_30m\").alias(\"max_visits_30m\"),\n",
    ").collect()[0]\n",
    "\n",
    "min_visits_1m = min_max_df['min_visits_1m']\n",
    "max_visits_1m = min_max_df['max_visits_1m']\n",
    "min_visits_30m = min_max_df['min_visits_30m']\n",
    "max_visits_30m = min_max_df['max_visits_30m']\n",
    "\n",
    "df = df.withColumns({\n",
    "    'visits_src_ip_1m': (F.col('visits_src_ip_1m') - min_visits_1m) / (max_visits_1m - min_visits_1m),\n",
    "    'visits_src_ip_30m': (F.col('visits_src_ip_30m') - min_visits_30m) / (max_visits_30m - min_visits_30m),\n",
    "})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 185,
   "id": "06591560-a39d-4ddb-b091-910656a94af9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "df = df.withColumn('source_ip_hash', F.hash('source_ip'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 186,
   "id": "a02a09f0-523a-4d41-9c7a-d3bb4bae3d85",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Row(source_ip='192.168.100.111', source_ip_hash=-259977465)"
     ]
    }
   ],
   "source": [
    "df.select('source_ip', 'source_ip_hash') \\\n",
    "  .where('source_ip = \"192.168.100.111\"') \\\n",
    "  .first()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 187,
   "id": "f914171a-4403-445b-9c4b-c49bc0dc304b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------------+--------------+\n",
      "|      source_ip|source_ip_hash|\n",
      "+---------------+--------------+\n",
      "|192.168.100.103|   -1113320195|\n",
      "|192.168.100.111|    -259977465|\n",
      "|    192.168.2.1|    1594593777|\n",
      "|    192.168.2.5|   -1139944252|\n",
      "|192.168.100.113|    -251951490|\n",
      "+---------------+--------------+"
     ]
    }
   ],
   "source": [
    "df.select('source_ip', 'source_ip_hash') \\\n",
    "  .where('label = 1.0') \\\n",
    "  .distinct() \\\n",
    "  .show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "id": "77fc6ceb-4a1b-42b7-8ba6-8c99c880b52b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------------+-------------------+\n",
      "|min(source_ip_hash)|max(source_ip_hash)|\n",
      "+-------------------+-------------------+\n",
      "|        -2147205793|         2147268527|\n",
      "+-------------------+-------------------+"
     ]
    }
   ],
   "source": [
    "df.agg(\n",
    "    min('source_ip_hash'),\n",
    "    max('source_ip_hash'),\n",
    ").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "id": "15ed4401-74f8-41e7-bffc-079d77be00bc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------------+-----------------+\n",
      "|count(source_ip)|          percent|\n",
      "+----------------+-----------------+\n",
      "|         1164431|7.353450374530357|\n",
      "+----------------+-----------------+"
     ]
    }
   ],
   "source": [
    "total = df.agg(count('source_ip')).first()[0]\n",
    "df.where('source_ip_hash < -381107000').agg(count('source_ip')) \\\n",
    "  .withColumn('percent', (F.col('count(source_ip)') / total) * 100 ).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "id": "a714a3d6-d66e-4bd0-b799-4a1ffec2cad6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------------+-------------------+\n",
      "|count(source_ip)|            percent|\n",
      "+----------------+-------------------+\n",
      "|           13671|0.08633317051006416|\n",
      "+----------------+-------------------+"
     ]
    }
   ],
   "source": [
    "df.where('source_ip_hash = -251951490').agg(count('source_ip')) \\\n",
    "  .withColumn('percent', (F.col('count(source_ip)') / total) * 100 ).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 188,
   "id": "9fa6aad7-ad33-4646-b944-e96dd5c50c3b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------------+--------------------+\n",
      "|count(source_ip)|             percent|\n",
      "+----------------+--------------------+\n",
      "|              10|6.315058921078498E-5|\n",
      "+----------------+--------------------+"
     ]
    }
   ],
   "source": [
    "df.where('source_ip_hash = 1594593777').agg(count('source_ip')) \\\n",
    "  .withColumn('percent', (F.col('count(source_ip)') / total) * 100 ).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 192,
   "id": "54b2f639-0776-4b8a-96e3-3a3a04a2287b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "df = df.withColumn('test_set', F.when(F.col('source_ip_hash') < -381107000, 1.0).otherwise(0.0))\n",
    "df = df.drop('source_ip', 'source_ip_hash')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 194,
   "id": "12f49423-6c80-4696-8a6b-bbe02574fbbe",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(15835165, 134)"
     ]
    }
   ],
   "source": [
    "df.count(), len(df.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 202,
   "id": "475be4c6-7777-414a-a845-4861cb955d9e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "df_test = df.filter(F.col('test_set') == 1.0)\n",
    "df_train = df.filter(F.col('test_set') != 1.0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 203,
   "id": "5ae5c539-000b-4d27-aa01-ef87160e00ae",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1164431, 134)"
     ]
    }
   ],
   "source": [
    "df_test.count(), len(df_test.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 204,
   "id": "de0d5d7f-ab0b-4c28-ab7a-251e959c226d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(14670734, 134)"
     ]
    }
   ],
   "source": [
    "df_train.count(), len(df_train.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 207,
   "id": "f1d87ff8-b67d-4e73-942b-f56c9fd88248",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from datetime import datetime\n",
    "\n",
    "current_datetime = datetime.now().strftime(\"%Y%m%d-%H%M%SUTC\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 240,
   "id": "e3fd09e6-8697-41f6-a319-a4d38c616bac",
   "metadata": {},
   "outputs": [],
   "source": [
    "%local\n",
    "import boto3\n",
    "boto3.client('s3').upload_file('./test.txt', 'datasets-20240228', 'test.txt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 243,
   "id": "0640fd96-964f-4e6e-aba4-779f1542c028",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "43a66bcd593243baa7bb2d29a8e7a8bb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Interrupted by user\n"
     ]
    }
   ],
   "source": [
    "df_test.coalesce(1).write.format('csv').option('header', 'true').mode('overwrite').save(f's3://{object.bucket_name}/df_test_{current_datetime}.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 244,
   "id": "4923adfa-eea6-4fcb-ac82-ff0e045b6b38",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "test_rdd = df_test.rdd.map(lambda x: x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 245,
   "id": "bf9eaf95-b91f-4d2f-b300-c144d96c8375",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "def csv_line(data):\n",
    "    r = \",\".join(str(d) for d in data[1])\n",
    "    return str(data[0]) + \",\" + r"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 246,
   "id": "b1c6c71f-8405-4d35-8b81-e4448aee92c8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "test_lines = test_rdd.map(csv_line)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 248,
   "id": "e8e22eb0-b25e-47fc-94e4-1ee0562c8a06",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f30ce492c8194bba9a216d01b6c5e9f1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Interrupted by user\n"
     ]
    }
   ],
   "source": [
    "test_lines.saveAsTextFile('test.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 220,
   "id": "b6887957-6c7e-4b92-b509-379193a33e38",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "An error was encountered:\n",
      "An error occurred while calling o3544.csv.\n",
      ": org.apache.spark.SparkException: Job aborted.\n",
      "\tat org.apache.spark.sql.errors.QueryExecutionErrors$.jobAbortedError(QueryExecutionErrors.scala:638)\n",
      "\tat org.apache.spark.sql.execution.datasources.FileFormatWriter$.write(FileFormatWriter.scala:279)\n",
      "\tat org.apache.spark.sql.execution.datasources.InsertIntoHadoopFsRelationCommand.run(InsertIntoHadoopFsRelationCommand.scala:193)\n",
      "\tat org.apache.spark.sql.execution.command.DataWritingCommandExec.sideEffectResult$lzycompute(commands.scala:113)\n",
      "\tat org.apache.spark.sql.execution.command.DataWritingCommandExec.sideEffectResult(commands.scala:111)\n",
      "\tat org.apache.spark.sql.execution.command.DataWritingCommandExec.executeCollect(commands.scala:125)\n",
      "\tat org.apache.spark.sql.execution.QueryExecution$$anonfun$eagerlyExecuteCommands$1.$anonfun$applyOrElse$1(QueryExecution.scala:103)\n",
      "\tat org.apache.spark.sql.catalyst.QueryPlanningTracker$.withTracker(QueryPlanningTracker.scala:107)\n",
      "\tat org.apache.spark.sql.execution.SQLExecution$.withTracker(SQLExecution.scala:224)\n",
      "\tat org.apache.spark.sql.execution.SQLExecution$.executeQuery$1(SQLExecution.scala:114)\n",
      "\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$7(SQLExecution.scala:139)\n",
      "\tat org.apache.spark.sql.catalyst.QueryPlanningTracker$.withTracker(QueryPlanningTracker.scala:107)\n",
      "\tat org.apache.spark.sql.execution.SQLExecution$.withTracker(SQLExecution.scala:224)\n",
      "\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$6(SQLExecution.scala:139)\n",
      "\tat org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:245)\n",
      "\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$1(SQLExecution.scala:138)\n",
      "\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:779)\n",
      "\tat org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:68)\n",
      "\tat org.apache.spark.sql.execution.QueryExecution$$anonfun$eagerlyExecuteCommands$1.applyOrElse(QueryExecution.scala:100)\n",
      "\tat org.apache.spark.sql.execution.QueryExecution$$anonfun$eagerlyExecuteCommands$1.applyOrElse(QueryExecution.scala:96)\n",
      "\tat org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$transformDownWithPruning$1(TreeNode.scala:615)\n",
      "\tat org.apache.spark.sql.catalyst.trees.CurrentOrigin$.withOrigin(TreeNode.scala:177)\n",
      "\tat org.apache.spark.sql.catalyst.trees.TreeNode.transformDownWithPruning(TreeNode.scala:615)\n",
      "\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.org$apache$spark$sql$catalyst$plans$logical$AnalysisHelper$$super$transformDownWithPruning(LogicalPlan.scala:30)\n",
      "\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.transformDownWithPruning(AnalysisHelper.scala:267)\n",
      "\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.transformDownWithPruning$(AnalysisHelper.scala:263)\n",
      "\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.transformDownWithPruning(LogicalPlan.scala:30)\n",
      "\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.transformDownWithPruning(LogicalPlan.scala:30)\n",
      "\tat org.apache.spark.sql.catalyst.trees.TreeNode.transformDown(TreeNode.scala:591)\n",
      "\tat org.apache.spark.sql.execution.QueryExecution.eagerlyExecuteCommands(QueryExecution.scala:96)\n",
      "\tat org.apache.spark.sql.execution.QueryExecution.commandExecuted$lzycompute(QueryExecution.scala:83)\n",
      "\tat org.apache.spark.sql.execution.QueryExecution.commandExecuted(QueryExecution.scala:81)\n",
      "\tat org.apache.spark.sql.execution.QueryExecution.assertCommandExecuted(QueryExecution.scala:124)\n",
      "\tat org.apache.spark.sql.DataFrameWriter.runCommand(DataFrameWriter.scala:860)\n",
      "\tat org.apache.spark.sql.DataFrameWriter.saveToV1Source(DataFrameWriter.scala:390)\n",
      "\tat org.apache.spark.sql.DataFrameWriter.saveInternal(DataFrameWriter.scala:363)\n",
      "\tat org.apache.spark.sql.DataFrameWriter.save(DataFrameWriter.scala:239)\n",
      "\tat org.apache.spark.sql.DataFrameWriter.csv(DataFrameWriter.scala:851)\n",
      "\tat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n",
      "\tat sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\n",
      "\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n",
      "\tat java.lang.reflect.Method.invoke(Method.java:498)\n",
      "\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n",
      "\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)\n",
      "\tat py4j.Gateway.invoke(Gateway.java:282)\n",
      "\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n",
      "\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n",
      "\tat py4j.GatewayConnection.run(GatewayConnection.java:238)\n",
      "\tat java.lang.Thread.run(Thread.java:750)\n",
      "Caused by: org.apache.spark.SparkException: Job aborted due to stage failure: Task 0 in stage 739.0 failed 4 times, most recent failure: Lost task 0.3 in stage 739.0 (TID 10540) (ip-10-0-20-248.ec2.internal executor 40): org.apache.spark.SparkException: Task failed while writing rows.\n",
      "\tat org.apache.spark.sql.errors.QueryExecutionErrors$.taskFailedWhileWritingRowsError(QueryExecutionErrors.scala:642)\n",
      "\tat org.apache.spark.sql.execution.datasources.FileFormatWriter$.executeTask(FileFormatWriter.scala:349)\n",
      "\tat org.apache.spark.sql.execution.datasources.FileFormatWriter$.$anonfun$write$21(FileFormatWriter.scala:257)\n",
      "\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)\n",
      "\tat org.apache.spark.scheduler.Task.run(Task.scala:138)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:548)\n",
      "\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1516)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:551)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n",
      "\tat java.lang.Thread.run(Thread.java:750)\n",
      "Caused by: java.lang.IllegalStateException: Error closing the output.\n",
      "\tat com.univocity.parsers.common.AbstractWriter.close(AbstractWriter.java:1000)\n",
      "\tat org.apache.spark.sql.catalyst.csv.UnivocityGenerator.close(UnivocityGenerator.scala:123)\n",
      "\tat org.apache.spark.sql.execution.datasources.csv.CsvOutputWriter.close(CsvOutputWriter.scala:48)\n",
      "\tat org.apache.spark.sql.execution.datasources.FileFormatDataWriter.releaseCurrentWriter(FileFormatDataWriter.scala:64)\n",
      "\tat org.apache.spark.sql.execution.datasources.FileFormatDataWriter.releaseResources(FileFormatDataWriter.scala:75)\n",
      "\tat org.apache.spark.sql.execution.datasources.FileFormatDataWriter.commit(FileFormatDataWriter.scala:105)\n",
      "\tat org.apache.spark.sql.execution.datasources.FileFormatWriter$.$anonfun$executeTask$1(FileFormatWriter.scala:333)\n",
      "\tat org.apache.spark.util.Utils$.tryWithSafeFinallyAndFailureCallbacks(Utils.scala:1550)\n",
      "\tat org.apache.spark.sql.execution.datasources.FileFormatWriter$.executeTask(FileFormatWriter.scala:339)\n",
      "\t... 9 more\n",
      "Caused by: java.io.IOException: Error closing multipart upload\n",
      "\tat com.amazon.ws.emr.hadoop.fs.s3n.MultipartUploadOutputStream.doMultiPartUpload(MultipartUploadOutputStream.java:448)\n",
      "\tat com.amazon.ws.emr.hadoop.fs.s3n.MultipartUploadOutputStream.close(MultipartUploadOutputStream.java:428)\n",
      "\tat org.apache.hadoop.fs.FSDataOutputStream$PositionCache.close(FSDataOutputStream.java:78)\n",
      "\tat org.apache.hadoop.fs.FSDataOutputStream.close(FSDataOutputStream.java:107)\n",
      "\tat sun.nio.cs.StreamEncoder.implClose(StreamEncoder.java:320)\n",
      "\tat sun.nio.cs.StreamEncoder.close(StreamEncoder.java:149)\n",
      "\tat java.io.OutputStreamWriter.close(OutputStreamWriter.java:233)\n",
      "\tat com.univocity.parsers.common.AbstractWriter.close(AbstractWriter.java:996)\n",
      "\t... 17 more\n",
      "Caused by: com.amazon.ws.emr.hadoop.fs.shaded.com.amazonaws.services.s3.model.AmazonS3Exception: Access Denied (Service: Amazon S3; Status Code: 403; Error Code: AccessDenied; Request ID: 6N9MC1592RSR9CPY; S3 Extended Request ID: cwZYc4stEiRHunE2gwzGioO51194p0C/IptbKc4B5a3N0gV4peNrXn7gzZe37OJt9BoQ3KbJVitfWSsaXLewNjiuMtB1c21vUz9hP/lgY1M=; Proxy: null), S3 Extended Request ID: cwZYc4stEiRHunE2gwzGioO51194p0C/IptbKc4B5a3N0gV4peNrXn7gzZe37OJt9BoQ3KbJVitfWSsaXLewNjiuMtB1c21vUz9hP/lgY1M=\n",
      "\tat com.amazon.ws.emr.hadoop.fs.shaded.com.amazonaws.http.AmazonHttpClient$RequestExecutor.handleErrorResponse(AmazonHttpClient.java:1879)\n",
      "\tat com.amazon.ws.emr.hadoop.fs.shaded.com.amazonaws.http.AmazonHttpClient$RequestExecutor.handleServiceErrorResponse(AmazonHttpClient.java:1418)\n",
      "\tat com.amazon.ws.emr.hadoop.fs.shaded.com.amazonaws.http.AmazonHttpClient$RequestExecutor.executeOneRequest(AmazonHttpClient.java:1387)\n",
      "\tat com.amazon.ws.emr.hadoop.fs.shaded.com.amazonaws.http.AmazonHttpClient$RequestExecutor.executeHelper(AmazonHttpClient.java:1157)\n",
      "\tat com.amazon.ws.emr.hadoop.fs.shaded.com.amazonaws.http.AmazonHttpClient$RequestExecutor.doExecute(AmazonHttpClient.java:814)\n",
      "\tat com.amazon.ws.emr.hadoop.fs.shaded.com.amazonaws.http.AmazonHttpClient$RequestExecutor.executeWithTimer(AmazonHttpClient.java:781)\n",
      "\tat com.amazon.ws.emr.hadoop.fs.shaded.com.amazonaws.http.AmazonHttpClient$RequestExecutor.execute(AmazonHttpClient.java:755)\n",
      "\tat com.amazon.ws.emr.hadoop.fs.shaded.com.amazonaws.http.AmazonHttpClient$RequestExecutor.access$500(AmazonHttpClient.java:715)\n",
      "\tat com.amazon.ws.emr.hadoop.fs.shaded.com.amazonaws.http.AmazonHttpClient$RequestExecutionBuilderImpl.execute(AmazonHttpClient.java:697)\n",
      "\tat com.amazon.ws.emr.hadoop.fs.shaded.com.amazonaws.http.AmazonHttpClient.execute(AmazonHttpClient.java:561)\n",
      "\tat com.amazon.ws.emr.hadoop.fs.shaded.com.amazonaws.http.AmazonHttpClient.execute(AmazonHttpClient.java:541)\n",
      "\tat com.amazon.ws.emr.hadoop.fs.shaded.com.amazonaws.services.s3.AmazonS3Client.invoke(AmazonS3Client.java:5456)\n",
      "\tat com.amazon.ws.emr.hadoop.fs.shaded.com.amazonaws.services.s3.AmazonS3Client.invoke(AmazonS3Client.java:5403)\n",
      "\tat com.amazon.ws.emr.hadoop.fs.shaded.com.amazonaws.services.s3.AmazonS3Client.initiateMultipartUpload(AmazonS3Client.java:3727)\n",
      "\tat com.amazon.ws.emr.hadoop.fs.s3.lite.call.InitiateMultipartUploadCall.perform(InitiateMultipartUploadCall.java:27)\n",
      "\tat com.amazon.ws.emr.hadoop.fs.s3.lite.call.InitiateMultipartUploadCall.perform(InitiateMultipartUploadCall.java:13)\n",
      "\tat com.amazon.ws.emr.hadoop.fs.s3.lite.executor.GlobalS3Executor$CallPerformer.call(GlobalS3Executor.java:111)\n",
      "\tat com.amazon.ws.emr.hadoop.fs.s3.lite.executor.GlobalS3Executor.execute(GlobalS3Executor.java:138)\n",
      "\tat com.amazon.ws.emr.hadoop.fs.s3.lite.AmazonS3LiteClient.invoke(AmazonS3LiteClient.java:191)\n",
      "\tat com.amazon.ws.emr.hadoop.fs.s3.lite.AmazonS3LiteClient.invoke(AmazonS3LiteClient.java:186)\n",
      "\tat com.amazon.ws.emr.hadoop.fs.s3.lite.AmazonS3LiteClient.initiateMultipartUpload(AmazonS3LiteClient.java:147)\n",
      "\tat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n",
      "\tat sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\n",
      "\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n",
      "\tat java.lang.reflect.Method.invoke(Method.java:498)\n",
      "\tat org.apache.hadoop.io.retry.RetryInvocationHandler.invokeMethod(RetryInvocationHandler.java:422)\n",
      "\tat org.apache.hadoop.io.retry.RetryInvocationHandler$Call.invokeMethod(RetryInvocationHandler.java:165)\n",
      "\tat org.apache.hadoop.io.retry.RetryInvocationHandler$Call.invoke(RetryInvocationHandler.java:157)\n",
      "\tat org.apache.hadoop.io.retry.RetryInvocationHandler$Call.invokeOnce(RetryInvocationHandler.java:95)\n",
      "\tat org.apache.hadoop.io.retry.RetryInvocationHandler.invoke(RetryInvocationHandler.java:359)\n",
      "\tat com.sun.proxy.$Proxy47.initiateMultipartUpload(Unknown Source)\n",
      "\tat com.amazon.ws.emr.hadoop.fs.s3n.MultipartUploadOutputStream.ensureMultipartUploadIsInitiated(MultipartUploadOutputStream.java:559)\n",
      "\tat com.amazon.ws.emr.hadoop.fs.s3n.MultipartUploadOutputStream.uploadSinglePartWithMultipartUpload(MultipartUploadOutputStream.java:406)\n",
      "\tat com.amazon.ws.emr.hadoop.fs.s3n.MultipartUploadOutputStream.doMultiPartUpload(MultipartUploadOutputStream.java:443)\n",
      "\t... 24 more\n",
      "\n",
      "Driver stacktrace:\n",
      "\tat org.apache.spark.scheduler.DAGScheduler.failJobAndIndependentStages(DAGScheduler.scala:2863)\n",
      "\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2(DAGScheduler.scala:2799)\n",
      "\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2$adapted(DAGScheduler.scala:2798)\n",
      "\tat scala.collection.mutable.ResizableArray.foreach(ResizableArray.scala:62)\n",
      "\tat scala.collection.mutable.ResizableArray.foreach$(ResizableArray.scala:55)\n",
      "\tat scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:49)\n",
      "\tat org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:2798)\n",
      "\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1(DAGScheduler.scala:1239)\n",
      "\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1$adapted(DAGScheduler.scala:1239)\n",
      "\tat scala.Option.foreach(Option.scala:407)\n",
      "\tat org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:1239)\n",
      "\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:3051)\n",
      "\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2993)\n",
      "\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2982)\n",
      "\tat org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:49)\n",
      "\tat org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:1009)\n",
      "\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2229)\n",
      "\tat org.apache.spark.sql.execution.datasources.FileFormatWriter$.write(FileFormatWriter.scala:246)\n",
      "\t... 47 more\n",
      "Caused by: org.apache.spark.SparkException: Task failed while writing rows.\n",
      "\tat org.apache.spark.sql.errors.QueryExecutionErrors$.taskFailedWhileWritingRowsError(QueryExecutionErrors.scala:642)\n",
      "\tat org.apache.spark.sql.execution.datasources.FileFormatWriter$.executeTask(FileFormatWriter.scala:349)\n",
      "\tat org.apache.spark.sql.execution.datasources.FileFormatWriter$.$anonfun$write$21(FileFormatWriter.scala:257)\n",
      "\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)\n",
      "\tat org.apache.spark.scheduler.Task.run(Task.scala:138)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:548)\n",
      "\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1516)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:551)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n",
      "\t... 1 more\n",
      "Caused by: java.lang.IllegalStateException: Error closing the output.\n",
      "\tat com.univocity.parsers.common.AbstractWriter.close(AbstractWriter.java:1000)\n",
      "\tat org.apache.spark.sql.catalyst.csv.UnivocityGenerator.close(UnivocityGenerator.scala:123)\n",
      "\tat org.apache.spark.sql.execution.datasources.csv.CsvOutputWriter.close(CsvOutputWriter.scala:48)\n",
      "\tat org.apache.spark.sql.execution.datasources.FileFormatDataWriter.releaseCurrentWriter(FileFormatDataWriter.scala:64)\n",
      "\tat org.apache.spark.sql.execution.datasources.FileFormatDataWriter.releaseResources(FileFormatDataWriter.scala:75)\n",
      "\tat org.apache.spark.sql.execution.datasources.FileFormatDataWriter.commit(FileFormatDataWriter.scala:105)\n",
      "\tat org.apache.spark.sql.execution.datasources.FileFormatWriter$.$anonfun$executeTask$1(FileFormatWriter.scala:333)\n",
      "\tat org.apache.spark.util.Utils$.tryWithSafeFinallyAndFailureCallbacks(Utils.scala:1550)\n",
      "\tat org.apache.spark.sql.execution.datasources.FileFormatWriter$.executeTask(FileFormatWriter.scala:339)\n",
      "\t... 9 more\n",
      "Caused by: java.io.IOException: Error closing multipart upload\n",
      "\tat com.amazon.ws.emr.hadoop.fs.s3n.MultipartUploadOutputStream.doMultiPartUpload(MultipartUploadOutputStream.java:448)\n",
      "\tat com.amazon.ws.emr.hadoop.fs.s3n.MultipartUploadOutputStream.close(MultipartUploadOutputStream.java:428)\n",
      "\tat org.apache.hadoop.fs.FSDataOutputStream$PositionCache.close(FSDataOutputStream.java:78)\n",
      "\tat org.apache.hadoop.fs.FSDataOutputStream.close(FSDataOutputStream.java:107)\n",
      "\tat sun.nio.cs.StreamEncoder.implClose(StreamEncoder.java:320)\n",
      "\tat sun.nio.cs.StreamEncoder.close(StreamEncoder.java:149)\n",
      "\tat java.io.OutputStreamWriter.close(OutputStreamWriter.java:233)\n",
      "\tat com.univocity.parsers.common.AbstractWriter.close(AbstractWriter.java:996)\n",
      "\t... 17 more\n",
      "Caused by: com.amazon.ws.emr.hadoop.fs.shaded.com.amazonaws.services.s3.model.AmazonS3Exception: Access Denied (Service: Amazon S3; Status Code: 403; Error Code: AccessDenied; Request ID: 6N9MC1592RSR9CPY; S3 Extended Request ID: cwZYc4stEiRHunE2gwzGioO51194p0C/IptbKc4B5a3N0gV4peNrXn7gzZe37OJt9BoQ3KbJVitfWSsaXLewNjiuMtB1c21vUz9hP/lgY1M=; Proxy: null), S3 Extended Request ID: cwZYc4stEiRHunE2gwzGioO51194p0C/IptbKc4B5a3N0gV4peNrXn7gzZe37OJt9BoQ3KbJVitfWSsaXLewNjiuMtB1c21vUz9hP/lgY1M=\n",
      "\tat com.amazon.ws.emr.hadoop.fs.shaded.com.amazonaws.http.AmazonHttpClient$RequestExecutor.handleErrorResponse(AmazonHttpClient.java:1879)\n",
      "\tat com.amazon.ws.emr.hadoop.fs.shaded.com.amazonaws.http.AmazonHttpClient$RequestExecutor.handleServiceErrorResponse(AmazonHttpClient.java:1418)\n",
      "\tat com.amazon.ws.emr.hadoop.fs.shaded.com.amazonaws.http.AmazonHttpClient$RequestExecutor.executeOneRequest(AmazonHttpClient.java:1387)\n",
      "\tat com.amazon.ws.emr.hadoop.fs.shaded.com.amazonaws.http.AmazonHttpClient$RequestExecutor.executeHelper(AmazonHttpClient.java:1157)\n",
      "\tat com.amazon.ws.emr.hadoop.fs.shaded.com.amazonaws.http.AmazonHttpClient$RequestExecutor.doExecute(AmazonHttpClient.java:814)\n",
      "\tat com.amazon.ws.emr.hadoop.fs.shaded.com.amazonaws.http.AmazonHttpClient$RequestExecutor.executeWithTimer(AmazonHttpClient.java:781)\n",
      "\tat com.amazon.ws.emr.hadoop.fs.shaded.com.amazonaws.http.AmazonHttpClient$RequestExecutor.execute(AmazonHttpClient.java:755)\n",
      "\tat com.amazon.ws.emr.hadoop.fs.shaded.com.amazonaws.http.AmazonHttpClient$RequestExecutor.access$500(AmazonHttpClient.java:715)\n",
      "\tat com.amazon.ws.emr.hadoop.fs.shaded.com.amazonaws.http.AmazonHttpClient$RequestExecutionBuilderImpl.execute(AmazonHttpClient.java:697)\n",
      "\tat com.amazon.ws.emr.hadoop.fs.shaded.com.amazonaws.http.AmazonHttpClient.execute(AmazonHttpClient.java:561)\n",
      "\tat com.amazon.ws.emr.hadoop.fs.shaded.com.amazonaws.http.AmazonHttpClient.execute(AmazonHttpClient.java:541)\n",
      "\tat com.amazon.ws.emr.hadoop.fs.shaded.com.amazonaws.services.s3.AmazonS3Client.invoke(AmazonS3Client.java:5456)\n",
      "\tat com.amazon.ws.emr.hadoop.fs.shaded.com.amazonaws.services.s3.AmazonS3Client.invoke(AmazonS3Client.java:5403)\n",
      "\tat com.amazon.ws.emr.hadoop.fs.shaded.com.amazonaws.services.s3.AmazonS3Client.initiateMultipartUpload(AmazonS3Client.java:3727)\n",
      "\tat com.amazon.ws.emr.hadoop.fs.s3.lite.call.InitiateMultipartUploadCall.perform(InitiateMultipartUploadCall.java:27)\n",
      "\tat com.amazon.ws.emr.hadoop.fs.s3.lite.call.InitiateMultipartUploadCall.perform(InitiateMultipartUploadCall.java:13)\n",
      "\tat com.amazon.ws.emr.hadoop.fs.s3.lite.executor.GlobalS3Executor$CallPerformer.call(GlobalS3Executor.java:111)\n",
      "\tat com.amazon.ws.emr.hadoop.fs.s3.lite.executor.GlobalS3Executor.execute(GlobalS3Executor.java:138)\n",
      "\tat com.amazon.ws.emr.hadoop.fs.s3.lite.AmazonS3LiteClient.invoke(AmazonS3LiteClient.java:191)\n",
      "\tat com.amazon.ws.emr.hadoop.fs.s3.lite.AmazonS3LiteClient.invoke(AmazonS3LiteClient.java:186)\n",
      "\tat com.amazon.ws.emr.hadoop.fs.s3.lite.AmazonS3LiteClient.initiateMultipartUpload(AmazonS3LiteClient.java:147)\n",
      "\tat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n",
      "\tat sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\n",
      "\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n",
      "\tat java.lang.reflect.Method.invoke(Method.java:498)\n",
      "\tat org.apache.hadoop.io.retry.RetryInvocationHandler.invokeMethod(RetryInvocationHandler.java:422)\n",
      "\tat org.apache.hadoop.io.retry.RetryInvocationHandler$Call.invokeMethod(RetryInvocationHandler.java:165)\n",
      "\tat org.apache.hadoop.io.retry.RetryInvocationHandler$Call.invoke(RetryInvocationHandler.java:157)\n",
      "\tat org.apache.hadoop.io.retry.RetryInvocationHandler$Call.invokeOnce(RetryInvocationHandler.java:95)\n",
      "\tat org.apache.hadoop.io.retry.RetryInvocationHandler.invoke(RetryInvocationHandler.java:359)\n",
      "\tat com.sun.proxy.$Proxy47.initiateMultipartUpload(Unknown Source)\n",
      "\tat com.amazon.ws.emr.hadoop.fs.s3n.MultipartUploadOutputStream.ensureMultipartUploadIsInitiated(MultipartUploadOutputStream.java:559)\n",
      "\tat com.amazon.ws.emr.hadoop.fs.s3n.MultipartUploadOutputStream.uploadSinglePartWithMultipartUpload(MultipartUploadOutputStream.java:406)\n",
      "\tat com.amazon.ws.emr.hadoop.fs.s3n.MultipartUploadOutputStream.doMultiPartUpload(MultipartUploadOutputStream.java:443)\n",
      "\t... 24 more\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/mnt/yarn/usercache/livy/appcache/application_1709421197197_0002/container_1709421197197_0002_01_000001/pyspark.zip/pyspark/sql/readwriter.py\", line 1240, in csv\n",
      "    self._jwrite.csv(path)\n",
      "  File \"/mnt/yarn/usercache/livy/appcache/application_1709421197197_0002/container_1709421197197_0002_01_000001/py4j-0.10.9.5-src.zip/py4j/java_gateway.py\", line 1322, in __call__\n",
      "    answer, self.gateway_client, self.target_id, self.name)\n",
      "  File \"/mnt/yarn/usercache/livy/appcache/application_1709421197197_0002/container_1709421197197_0002_01_000001/pyspark.zip/pyspark/sql/utils.py\", line 190, in deco\n",
      "    return f(*a, **kw)\n",
      "  File \"/mnt/yarn/usercache/livy/appcache/application_1709421197197_0002/container_1709421197197_0002_01_000001/py4j-0.10.9.5-src.zip/py4j/protocol.py\", line 328, in get_return_value\n",
      "    format(target_id, \".\", name), value)\n",
      "py4j.protocol.Py4JJavaError: An error occurred while calling o3544.csv.\n",
      ": org.apache.spark.SparkException: Job aborted.\n",
      "\tat org.apache.spark.sql.errors.QueryExecutionErrors$.jobAbortedError(QueryExecutionErrors.scala:638)\n",
      "\tat org.apache.spark.sql.execution.datasources.FileFormatWriter$.write(FileFormatWriter.scala:279)\n",
      "\tat org.apache.spark.sql.execution.datasources.InsertIntoHadoopFsRelationCommand.run(InsertIntoHadoopFsRelationCommand.scala:193)\n",
      "\tat org.apache.spark.sql.execution.command.DataWritingCommandExec.sideEffectResult$lzycompute(commands.scala:113)\n",
      "\tat org.apache.spark.sql.execution.command.DataWritingCommandExec.sideEffectResult(commands.scala:111)\n",
      "\tat org.apache.spark.sql.execution.command.DataWritingCommandExec.executeCollect(commands.scala:125)\n",
      "\tat org.apache.spark.sql.execution.QueryExecution$$anonfun$eagerlyExecuteCommands$1.$anonfun$applyOrElse$1(QueryExecution.scala:103)\n",
      "\tat org.apache.spark.sql.catalyst.QueryPlanningTracker$.withTracker(QueryPlanningTracker.scala:107)\n",
      "\tat org.apache.spark.sql.execution.SQLExecution$.withTracker(SQLExecution.scala:224)\n",
      "\tat org.apache.spark.sql.execution.SQLExecution$.executeQuery$1(SQLExecution.scala:114)\n",
      "\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$7(SQLExecution.scala:139)\n",
      "\tat org.apache.spark.sql.catalyst.QueryPlanningTracker$.withTracker(QueryPlanningTracker.scala:107)\n",
      "\tat org.apache.spark.sql.execution.SQLExecution$.withTracker(SQLExecution.scala:224)\n",
      "\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$6(SQLExecution.scala:139)\n",
      "\tat org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:245)\n",
      "\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$1(SQLExecution.scala:138)\n",
      "\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:779)\n",
      "\tat org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:68)\n",
      "\tat org.apache.spark.sql.execution.QueryExecution$$anonfun$eagerlyExecuteCommands$1.applyOrElse(QueryExecution.scala:100)\n",
      "\tat org.apache.spark.sql.execution.QueryExecution$$anonfun$eagerlyExecuteCommands$1.applyOrElse(QueryExecution.scala:96)\n",
      "\tat org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$transformDownWithPruning$1(TreeNode.scala:615)\n",
      "\tat org.apache.spark.sql.catalyst.trees.CurrentOrigin$.withOrigin(TreeNode.scala:177)\n",
      "\tat org.apache.spark.sql.catalyst.trees.TreeNode.transformDownWithPruning(TreeNode.scala:615)\n",
      "\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.org$apache$spark$sql$catalyst$plans$logical$AnalysisHelper$$super$transformDownWithPruning(LogicalPlan.scala:30)\n",
      "\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.transformDownWithPruning(AnalysisHelper.scala:267)\n",
      "\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.transformDownWithPruning$(AnalysisHelper.scala:263)\n",
      "\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.transformDownWithPruning(LogicalPlan.scala:30)\n",
      "\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.transformDownWithPruning(LogicalPlan.scala:30)\n",
      "\tat org.apache.spark.sql.catalyst.trees.TreeNode.transformDown(TreeNode.scala:591)\n",
      "\tat org.apache.spark.sql.execution.QueryExecution.eagerlyExecuteCommands(QueryExecution.scala:96)\n",
      "\tat org.apache.spark.sql.execution.QueryExecution.commandExecuted$lzycompute(QueryExecution.scala:83)\n",
      "\tat org.apache.spark.sql.execution.QueryExecution.commandExecuted(QueryExecution.scala:81)\n",
      "\tat org.apache.spark.sql.execution.QueryExecution.assertCommandExecuted(QueryExecution.scala:124)\n",
      "\tat org.apache.spark.sql.DataFrameWriter.runCommand(DataFrameWriter.scala:860)\n",
      "\tat org.apache.spark.sql.DataFrameWriter.saveToV1Source(DataFrameWriter.scala:390)\n",
      "\tat org.apache.spark.sql.DataFrameWriter.saveInternal(DataFrameWriter.scala:363)\n",
      "\tat org.apache.spark.sql.DataFrameWriter.save(DataFrameWriter.scala:239)\n",
      "\tat org.apache.spark.sql.DataFrameWriter.csv(DataFrameWriter.scala:851)\n",
      "\tat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n",
      "\tat sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\n",
      "\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n",
      "\tat java.lang.reflect.Method.invoke(Method.java:498)\n",
      "\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n",
      "\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)\n",
      "\tat py4j.Gateway.invoke(Gateway.java:282)\n",
      "\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n",
      "\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n",
      "\tat py4j.GatewayConnection.run(GatewayConnection.java:238)\n",
      "\tat java.lang.Thread.run(Thread.java:750)\n",
      "Caused by: org.apache.spark.SparkException: Job aborted due to stage failure: Task 0 in stage 739.0 failed 4 times, most recent failure: Lost task 0.3 in stage 739.0 (TID 10540) (ip-10-0-20-248.ec2.internal executor 40): org.apache.spark.SparkException: Task failed while writing rows.\n",
      "\tat org.apache.spark.sql.errors.QueryExecutionErrors$.taskFailedWhileWritingRowsError(QueryExecutionErrors.scala:642)\n",
      "\tat org.apache.spark.sql.execution.datasources.FileFormatWriter$.executeTask(FileFormatWriter.scala:349)\n",
      "\tat org.apache.spark.sql.execution.datasources.FileFormatWriter$.$anonfun$write$21(FileFormatWriter.scala:257)\n",
      "\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)\n",
      "\tat org.apache.spark.scheduler.Task.run(Task.scala:138)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:548)\n",
      "\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1516)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:551)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n",
      "\tat java.lang.Thread.run(Thread.java:750)\n",
      "Caused by: java.lang.IllegalStateException: Error closing the output.\n",
      "\tat com.univocity.parsers.common.AbstractWriter.close(AbstractWriter.java:1000)\n",
      "\tat org.apache.spark.sql.catalyst.csv.UnivocityGenerator.close(UnivocityGenerator.scala:123)\n",
      "\tat org.apache.spark.sql.execution.datasources.csv.CsvOutputWriter.close(CsvOutputWriter.scala:48)\n",
      "\tat org.apache.spark.sql.execution.datasources.FileFormatDataWriter.releaseCurrentWriter(FileFormatDataWriter.scala:64)\n",
      "\tat org.apache.spark.sql.execution.datasources.FileFormatDataWriter.releaseResources(FileFormatDataWriter.scala:75)\n",
      "\tat org.apache.spark.sql.execution.datasources.FileFormatDataWriter.commit(FileFormatDataWriter.scala:105)\n",
      "\tat org.apache.spark.sql.execution.datasources.FileFormatWriter$.$anonfun$executeTask$1(FileFormatWriter.scala:333)\n",
      "\tat org.apache.spark.util.Utils$.tryWithSafeFinallyAndFailureCallbacks(Utils.scala:1550)\n",
      "\tat org.apache.spark.sql.execution.datasources.FileFormatWriter$.executeTask(FileFormatWriter.scala:339)\n",
      "\t... 9 more\n",
      "Caused by: java.io.IOException: Error closing multipart upload\n",
      "\tat com.amazon.ws.emr.hadoop.fs.s3n.MultipartUploadOutputStream.doMultiPartUpload(MultipartUploadOutputStream.java:448)\n",
      "\tat com.amazon.ws.emr.hadoop.fs.s3n.MultipartUploadOutputStream.close(MultipartUploadOutputStream.java:428)\n",
      "\tat org.apache.hadoop.fs.FSDataOutputStream$PositionCache.close(FSDataOutputStream.java:78)\n",
      "\tat org.apache.hadoop.fs.FSDataOutputStream.close(FSDataOutputStream.java:107)\n",
      "\tat sun.nio.cs.StreamEncoder.implClose(StreamEncoder.java:320)\n",
      "\tat sun.nio.cs.StreamEncoder.close(StreamEncoder.java:149)\n",
      "\tat java.io.OutputStreamWriter.close(OutputStreamWriter.java:233)\n",
      "\tat com.univocity.parsers.common.AbstractWriter.close(AbstractWriter.java:996)\n",
      "\t... 17 more\n",
      "Caused by: com.amazon.ws.emr.hadoop.fs.shaded.com.amazonaws.services.s3.model.AmazonS3Exception: Access Denied (Service: Amazon S3; Status Code: 403; Error Code: AccessDenied; Request ID: 6N9MC1592RSR9CPY; S3 Extended Request ID: cwZYc4stEiRHunE2gwzGioO51194p0C/IptbKc4B5a3N0gV4peNrXn7gzZe37OJt9BoQ3KbJVitfWSsaXLewNjiuMtB1c21vUz9hP/lgY1M=; Proxy: null), S3 Extended Request ID: cwZYc4stEiRHunE2gwzGioO51194p0C/IptbKc4B5a3N0gV4peNrXn7gzZe37OJt9BoQ3KbJVitfWSsaXLewNjiuMtB1c21vUz9hP/lgY1M=\n",
      "\tat com.amazon.ws.emr.hadoop.fs.shaded.com.amazonaws.http.AmazonHttpClient$RequestExecutor.handleErrorResponse(AmazonHttpClient.java:1879)\n",
      "\tat com.amazon.ws.emr.hadoop.fs.shaded.com.amazonaws.http.AmazonHttpClient$RequestExecutor.handleServiceErrorResponse(AmazonHttpClient.java:1418)\n",
      "\tat com.amazon.ws.emr.hadoop.fs.shaded.com.amazonaws.http.AmazonHttpClient$RequestExecutor.executeOneRequest(AmazonHttpClient.java:1387)\n",
      "\tat com.amazon.ws.emr.hadoop.fs.shaded.com.amazonaws.http.AmazonHttpClient$RequestExecutor.executeHelper(AmazonHttpClient.java:1157)\n",
      "\tat com.amazon.ws.emr.hadoop.fs.shaded.com.amazonaws.http.AmazonHttpClient$RequestExecutor.doExecute(AmazonHttpClient.java:814)\n",
      "\tat com.amazon.ws.emr.hadoop.fs.shaded.com.amazonaws.http.AmazonHttpClient$RequestExecutor.executeWithTimer(AmazonHttpClient.java:781)\n",
      "\tat com.amazon.ws.emr.hadoop.fs.shaded.com.amazonaws.http.AmazonHttpClient$RequestExecutor.execute(AmazonHttpClient.java:755)\n",
      "\tat com.amazon.ws.emr.hadoop.fs.shaded.com.amazonaws.http.AmazonHttpClient$RequestExecutor.access$500(AmazonHttpClient.java:715)\n",
      "\tat com.amazon.ws.emr.hadoop.fs.shaded.com.amazonaws.http.AmazonHttpClient$RequestExecutionBuilderImpl.execute(AmazonHttpClient.java:697)\n",
      "\tat com.amazon.ws.emr.hadoop.fs.shaded.com.amazonaws.http.AmazonHttpClient.execute(AmazonHttpClient.java:561)\n",
      "\tat com.amazon.ws.emr.hadoop.fs.shaded.com.amazonaws.http.AmazonHttpClient.execute(AmazonHttpClient.java:541)\n",
      "\tat com.amazon.ws.emr.hadoop.fs.shaded.com.amazonaws.services.s3.AmazonS3Client.invoke(AmazonS3Client.java:5456)\n",
      "\tat com.amazon.ws.emr.hadoop.fs.shaded.com.amazonaws.services.s3.AmazonS3Client.invoke(AmazonS3Client.java:5403)\n",
      "\tat com.amazon.ws.emr.hadoop.fs.shaded.com.amazonaws.services.s3.AmazonS3Client.initiateMultipartUpload(AmazonS3Client.java:3727)\n",
      "\tat com.amazon.ws.emr.hadoop.fs.s3.lite.call.InitiateMultipartUploadCall.perform(InitiateMultipartUploadCall.java:27)\n",
      "\tat com.amazon.ws.emr.hadoop.fs.s3.lite.call.InitiateMultipartUploadCall.perform(InitiateMultipartUploadCall.java:13)\n",
      "\tat com.amazon.ws.emr.hadoop.fs.s3.lite.executor.GlobalS3Executor$CallPerformer.call(GlobalS3Executor.java:111)\n",
      "\tat com.amazon.ws.emr.hadoop.fs.s3.lite.executor.GlobalS3Executor.execute(GlobalS3Executor.java:138)\n",
      "\tat com.amazon.ws.emr.hadoop.fs.s3.lite.AmazonS3LiteClient.invoke(AmazonS3LiteClient.java:191)\n",
      "\tat com.amazon.ws.emr.hadoop.fs.s3.lite.AmazonS3LiteClient.invoke(AmazonS3LiteClient.java:186)\n",
      "\tat com.amazon.ws.emr.hadoop.fs.s3.lite.AmazonS3LiteClient.initiateMultipartUpload(AmazonS3LiteClient.java:147)\n",
      "\tat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n",
      "\tat sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\n",
      "\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n",
      "\tat java.lang.reflect.Method.invoke(Method.java:498)\n",
      "\tat org.apache.hadoop.io.retry.RetryInvocationHandler.invokeMethod(RetryInvocationHandler.java:422)\n",
      "\tat org.apache.hadoop.io.retry.RetryInvocationHandler$Call.invokeMethod(RetryInvocationHandler.java:165)\n",
      "\tat org.apache.hadoop.io.retry.RetryInvocationHandler$Call.invoke(RetryInvocationHandler.java:157)\n",
      "\tat org.apache.hadoop.io.retry.RetryInvocationHandler$Call.invokeOnce(RetryInvocationHandler.java:95)\n",
      "\tat org.apache.hadoop.io.retry.RetryInvocationHandler.invoke(RetryInvocationHandler.java:359)\n",
      "\tat com.sun.proxy.$Proxy47.initiateMultipartUpload(Unknown Source)\n",
      "\tat com.amazon.ws.emr.hadoop.fs.s3n.MultipartUploadOutputStream.ensureMultipartUploadIsInitiated(MultipartUploadOutputStream.java:559)\n",
      "\tat com.amazon.ws.emr.hadoop.fs.s3n.MultipartUploadOutputStream.uploadSinglePartWithMultipartUpload(MultipartUploadOutputStream.java:406)\n",
      "\tat com.amazon.ws.emr.hadoop.fs.s3n.MultipartUploadOutputStream.doMultiPartUpload(MultipartUploadOutputStream.java:443)\n",
      "\t... 24 more\n",
      "\n",
      "Driver stacktrace:\n",
      "\tat org.apache.spark.scheduler.DAGScheduler.failJobAndIndependentStages(DAGScheduler.scala:2863)\n",
      "\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2(DAGScheduler.scala:2799)\n",
      "\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2$adapted(DAGScheduler.scala:2798)\n",
      "\tat scala.collection.mutable.ResizableArray.foreach(ResizableArray.scala:62)\n",
      "\tat scala.collection.mutable.ResizableArray.foreach$(ResizableArray.scala:55)\n",
      "\tat scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:49)\n",
      "\tat org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:2798)\n",
      "\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1(DAGScheduler.scala:1239)\n",
      "\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1$adapted(DAGScheduler.scala:1239)\n",
      "\tat scala.Option.foreach(Option.scala:407)\n",
      "\tat org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:1239)\n",
      "\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:3051)\n",
      "\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2993)\n",
      "\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2982)\n",
      "\tat org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:49)\n",
      "\tat org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:1009)\n",
      "\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2229)\n",
      "\tat org.apache.spark.sql.execution.datasources.FileFormatWriter$.write(FileFormatWriter.scala:246)\n",
      "\t... 47 more\n",
      "Caused by: org.apache.spark.SparkException: Task failed while writing rows.\n",
      "\tat org.apache.spark.sql.errors.QueryExecutionErrors$.taskFailedWhileWritingRowsError(QueryExecutionErrors.scala:642)\n",
      "\tat org.apache.spark.sql.execution.datasources.FileFormatWriter$.executeTask(FileFormatWriter.scala:349)\n",
      "\tat org.apache.spark.sql.execution.datasources.FileFormatWriter$.$anonfun$write$21(FileFormatWriter.scala:257)\n",
      "\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)\n",
      "\tat org.apache.spark.scheduler.Task.run(Task.scala:138)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:548)\n",
      "\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1516)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:551)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n",
      "\t... 1 more\n",
      "Caused by: java.lang.IllegalStateException: Error closing the output.\n",
      "\tat com.univocity.parsers.common.AbstractWriter.close(AbstractWriter.java:1000)\n",
      "\tat org.apache.spark.sql.catalyst.csv.UnivocityGenerator.close(UnivocityGenerator.scala:123)\n",
      "\tat org.apache.spark.sql.execution.datasources.csv.CsvOutputWriter.close(CsvOutputWriter.scala:48)\n",
      "\tat org.apache.spark.sql.execution.datasources.FileFormatDataWriter.releaseCurrentWriter(FileFormatDataWriter.scala:64)\n",
      "\tat org.apache.spark.sql.execution.datasources.FileFormatDataWriter.releaseResources(FileFormatDataWriter.scala:75)\n",
      "\tat org.apache.spark.sql.execution.datasources.FileFormatDataWriter.commit(FileFormatDataWriter.scala:105)\n",
      "\tat org.apache.spark.sql.execution.datasources.FileFormatWriter$.$anonfun$executeTask$1(FileFormatWriter.scala:333)\n",
      "\tat org.apache.spark.util.Utils$.tryWithSafeFinallyAndFailureCallbacks(Utils.scala:1550)\n",
      "\tat org.apache.spark.sql.execution.datasources.FileFormatWriter$.executeTask(FileFormatWriter.scala:339)\n",
      "\t... 9 more\n",
      "Caused by: java.io.IOException: Error closing multipart upload\n",
      "\tat com.amazon.ws.emr.hadoop.fs.s3n.MultipartUploadOutputStream.doMultiPartUpload(MultipartUploadOutputStream.java:448)\n",
      "\tat com.amazon.ws.emr.hadoop.fs.s3n.MultipartUploadOutputStream.close(MultipartUploadOutputStream.java:428)\n",
      "\tat org.apache.hadoop.fs.FSDataOutputStream$PositionCache.close(FSDataOutputStream.java:78)\n",
      "\tat org.apache.hadoop.fs.FSDataOutputStream.close(FSDataOutputStream.java:107)\n",
      "\tat sun.nio.cs.StreamEncoder.implClose(StreamEncoder.java:320)\n",
      "\tat sun.nio.cs.StreamEncoder.close(StreamEncoder.java:149)\n",
      "\tat java.io.OutputStreamWriter.close(OutputStreamWriter.java:233)\n",
      "\tat com.univocity.parsers.common.AbstractWriter.close(AbstractWriter.java:996)\n",
      "\t... 17 more\n",
      "Caused by: com.amazon.ws.emr.hadoop.fs.shaded.com.amazonaws.services.s3.model.AmazonS3Exception: Access Denied (Service: Amazon S3; Status Code: 403; Error Code: AccessDenied; Request ID: 6N9MC1592RSR9CPY; S3 Extended Request ID: cwZYc4stEiRHunE2gwzGioO51194p0C/IptbKc4B5a3N0gV4peNrXn7gzZe37OJt9BoQ3KbJVitfWSsaXLewNjiuMtB1c21vUz9hP/lgY1M=; Proxy: null), S3 Extended Request ID: cwZYc4stEiRHunE2gwzGioO51194p0C/IptbKc4B5a3N0gV4peNrXn7gzZe37OJt9BoQ3KbJVitfWSsaXLewNjiuMtB1c21vUz9hP/lgY1M=\n",
      "\tat com.amazon.ws.emr.hadoop.fs.shaded.com.amazonaws.http.AmazonHttpClient$RequestExecutor.handleErrorResponse(AmazonHttpClient.java:1879)\n",
      "\tat com.amazon.ws.emr.hadoop.fs.shaded.com.amazonaws.http.AmazonHttpClient$RequestExecutor.handleServiceErrorResponse(AmazonHttpClient.java:1418)\n",
      "\tat com.amazon.ws.emr.hadoop.fs.shaded.com.amazonaws.http.AmazonHttpClient$RequestExecutor.executeOneRequest(AmazonHttpClient.java:1387)\n",
      "\tat com.amazon.ws.emr.hadoop.fs.shaded.com.amazonaws.http.AmazonHttpClient$RequestExecutor.executeHelper(AmazonHttpClient.java:1157)\n",
      "\tat com.amazon.ws.emr.hadoop.fs.shaded.com.amazonaws.http.AmazonHttpClient$RequestExecutor.doExecute(AmazonHttpClient.java:814)\n",
      "\tat com.amazon.ws.emr.hadoop.fs.shaded.com.amazonaws.http.AmazonHttpClient$RequestExecutor.executeWithTimer(AmazonHttpClient.java:781)\n",
      "\tat com.amazon.ws.emr.hadoop.fs.shaded.com.amazonaws.http.AmazonHttpClient$RequestExecutor.execute(AmazonHttpClient.java:755)\n",
      "\tat com.amazon.ws.emr.hadoop.fs.shaded.com.amazonaws.http.AmazonHttpClient$RequestExecutor.access$500(AmazonHttpClient.java:715)\n",
      "\tat com.amazon.ws.emr.hadoop.fs.shaded.com.amazonaws.http.AmazonHttpClient$RequestExecutionBuilderImpl.execute(AmazonHttpClient.java:697)\n",
      "\tat com.amazon.ws.emr.hadoop.fs.shaded.com.amazonaws.http.AmazonHttpClient.execute(AmazonHttpClient.java:561)\n",
      "\tat com.amazon.ws.emr.hadoop.fs.shaded.com.amazonaws.http.AmazonHttpClient.execute(AmazonHttpClient.java:541)\n",
      "\tat com.amazon.ws.emr.hadoop.fs.shaded.com.amazonaws.services.s3.AmazonS3Client.invoke(AmazonS3Client.java:5456)\n",
      "\tat com.amazon.ws.emr.hadoop.fs.shaded.com.amazonaws.services.s3.AmazonS3Client.invoke(AmazonS3Client.java:5403)\n",
      "\tat com.amazon.ws.emr.hadoop.fs.shaded.com.amazonaws.services.s3.AmazonS3Client.initiateMultipartUpload(AmazonS3Client.java:3727)\n",
      "\tat com.amazon.ws.emr.hadoop.fs.s3.lite.call.InitiateMultipartUploadCall.perform(InitiateMultipartUploadCall.java:27)\n",
      "\tat com.amazon.ws.emr.hadoop.fs.s3.lite.call.InitiateMultipartUploadCall.perform(InitiateMultipartUploadCall.java:13)\n",
      "\tat com.amazon.ws.emr.hadoop.fs.s3.lite.executor.GlobalS3Executor$CallPerformer.call(GlobalS3Executor.java:111)\n",
      "\tat com.amazon.ws.emr.hadoop.fs.s3.lite.executor.GlobalS3Executor.execute(GlobalS3Executor.java:138)\n",
      "\tat com.amazon.ws.emr.hadoop.fs.s3.lite.AmazonS3LiteClient.invoke(AmazonS3LiteClient.java:191)\n",
      "\tat com.amazon.ws.emr.hadoop.fs.s3.lite.AmazonS3LiteClient.invoke(AmazonS3LiteClient.java:186)\n",
      "\tat com.amazon.ws.emr.hadoop.fs.s3.lite.AmazonS3LiteClient.initiateMultipartUpload(AmazonS3LiteClient.java:147)\n",
      "\tat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n",
      "\tat sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\n",
      "\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n",
      "\tat java.lang.reflect.Method.invoke(Method.java:498)\n",
      "\tat org.apache.hadoop.io.retry.RetryInvocationHandler.invokeMethod(RetryInvocationHandler.java:422)\n",
      "\tat org.apache.hadoop.io.retry.RetryInvocationHandler$Call.invokeMethod(RetryInvocationHandler.java:165)\n",
      "\tat org.apache.hadoop.io.retry.RetryInvocationHandler$Call.invoke(RetryInvocationHandler.java:157)\n",
      "\tat org.apache.hadoop.io.retry.RetryInvocationHandler$Call.invokeOnce(RetryInvocationHandler.java:95)\n",
      "\tat org.apache.hadoop.io.retry.RetryInvocationHandler.invoke(RetryInvocationHandler.java:359)\n",
      "\tat com.sun.proxy.$Proxy47.initiateMultipartUpload(Unknown Source)\n",
      "\tat com.amazon.ws.emr.hadoop.fs.s3n.MultipartUploadOutputStream.ensureMultipartUploadIsInitiated(MultipartUploadOutputStream.java:559)\n",
      "\tat com.amazon.ws.emr.hadoop.fs.s3n.MultipartUploadOutputStream.uploadSinglePartWithMultipartUpload(MultipartUploadOutputStream.java:406)\n",
      "\tat com.amazon.ws.emr.hadoop.fs.s3n.MultipartUploadOutputStream.doMultiPartUpload(MultipartUploadOutputStream.java:443)\n",
      "\t... 24 more\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_test.write.option('header', True).csv(f's3://{object.bucket_name}/df_test_{current_datetime}.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 218,
   "id": "78ec44e5-1ba8-4e99-8ee7-082b7550e35a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "An error was encountered:\n",
      "An error occurred while calling o3532.csv.\n",
      ": java.nio.file.AccessDeniedException: df_train_20240303-085141UTC.csv/_temporary/0: PUT 0-byte object  on df_train_20240303-085141UTC.csv/_temporary/0: com.amazonaws.services.s3.model.AmazonS3Exception: Access Denied (Service: Amazon S3; Status Code: 403; Error Code: AccessDenied; Request ID: 9QJH8SRFCFZA38C5; S3 Extended Request ID: kYVzJWms2wMpmNvGWeeXYULtxUsUgob7cvtOXojaYT1jlf7rrYmCFS+ARGV94Vwsv73X4GPla4w=; Proxy: null), S3 Extended Request ID: kYVzJWms2wMpmNvGWeeXYULtxUsUgob7cvtOXojaYT1jlf7rrYmCFS+ARGV94Vwsv73X4GPla4w=:AccessDenied\n",
      "\tat org.apache.hadoop.fs.s3a.S3AUtils.translateException(S3AUtils.java:255)\n",
      "\tat org.apache.hadoop.fs.s3a.Invoker.once(Invoker.java:119)\n",
      "\tat org.apache.hadoop.fs.s3a.Invoker.lambda$retry$4(Invoker.java:322)\n",
      "\tat org.apache.hadoop.fs.s3a.Invoker.retryUntranslated(Invoker.java:414)\n",
      "\tat org.apache.hadoop.fs.s3a.Invoker.retry(Invoker.java:318)\n",
      "\tat org.apache.hadoop.fs.s3a.Invoker.retry(Invoker.java:293)\n",
      "\tat org.apache.hadoop.fs.s3a.S3AFileSystem.createEmptyObject(S3AFileSystem.java:4532)\n",
      "\tat org.apache.hadoop.fs.s3a.S3AFileSystem.access$1900(S3AFileSystem.java:259)\n",
      "\tat org.apache.hadoop.fs.s3a.S3AFileSystem$MkdirOperationCallbacksImpl.createFakeDirectory(S3AFileSystem.java:3461)\n",
      "\tat org.apache.hadoop.fs.s3a.impl.MkdirOperation.execute(MkdirOperation.java:121)\n",
      "\tat org.apache.hadoop.fs.s3a.impl.MkdirOperation.execute(MkdirOperation.java:45)\n",
      "\tat org.apache.hadoop.fs.s3a.impl.ExecutingStoreOperation.apply(ExecutingStoreOperation.java:76)\n",
      "\tat org.apache.hadoop.fs.statistics.impl.IOStatisticsBinding.lambda$trackDurationOfOperation$5(IOStatisticsBinding.java:499)\n",
      "\tat org.apache.hadoop.fs.statistics.impl.IOStatisticsBinding.trackDuration(IOStatisticsBinding.java:444)\n",
      "\tat org.apache.hadoop.fs.s3a.S3AFileSystem.trackDurationAndSpan(S3AFileSystem.java:2337)\n",
      "\tat org.apache.hadoop.fs.s3a.S3AFileSystem.trackDurationAndSpan(S3AFileSystem.java:2356)\n",
      "\tat org.apache.hadoop.fs.s3a.S3AFileSystem.mkdirs(S3AFileSystem.java:3428)\n",
      "\tat org.apache.hadoop.fs.FileSystem.mkdirs(FileSystem.java:2449)\n",
      "\tat org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter.setupJob(FileOutputCommitter.java:359)\n",
      "\tat org.apache.spark.internal.io.HadoopMapReduceCommitProtocol.setupJob(HadoopMapReduceCommitProtocol.scala:162)\n",
      "\tat org.apache.spark.sql.execution.datasources.SQLEmrOptimizedCommitProtocol.setupJob(SQLEmrOptimizedCommitProtocol.scala:101)\n",
      "\tat org.apache.spark.sql.execution.datasources.FileFormatWriter$.write(FileFormatWriter.scala:209)\n",
      "\tat org.apache.spark.sql.execution.datasources.InsertIntoHadoopFsRelationCommand.run(InsertIntoHadoopFsRelationCommand.scala:193)\n",
      "\tat org.apache.spark.sql.execution.command.DataWritingCommandExec.sideEffectResult$lzycompute(commands.scala:113)\n",
      "\tat org.apache.spark.sql.execution.command.DataWritingCommandExec.sideEffectResult(commands.scala:111)\n",
      "\tat org.apache.spark.sql.execution.command.DataWritingCommandExec.executeCollect(commands.scala:125)\n",
      "\tat org.apache.spark.sql.execution.QueryExecution$$anonfun$eagerlyExecuteCommands$1.$anonfun$applyOrElse$1(QueryExecution.scala:103)\n",
      "\tat org.apache.spark.sql.catalyst.QueryPlanningTracker$.withTracker(QueryPlanningTracker.scala:107)\n",
      "\tat org.apache.spark.sql.execution.SQLExecution$.withTracker(SQLExecution.scala:224)\n",
      "\tat org.apache.spark.sql.execution.SQLExecution$.executeQuery$1(SQLExecution.scala:114)\n",
      "\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$7(SQLExecution.scala:139)\n",
      "\tat org.apache.spark.sql.catalyst.QueryPlanningTracker$.withTracker(QueryPlanningTracker.scala:107)\n",
      "\tat org.apache.spark.sql.execution.SQLExecution$.withTracker(SQLExecution.scala:224)\n",
      "\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$6(SQLExecution.scala:139)\n",
      "\tat org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:245)\n",
      "\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$1(SQLExecution.scala:138)\n",
      "\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:779)\n",
      "\tat org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:68)\n",
      "\tat org.apache.spark.sql.execution.QueryExecution$$anonfun$eagerlyExecuteCommands$1.applyOrElse(QueryExecution.scala:100)\n",
      "\tat org.apache.spark.sql.execution.QueryExecution$$anonfun$eagerlyExecuteCommands$1.applyOrElse(QueryExecution.scala:96)\n",
      "\tat org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$transformDownWithPruning$1(TreeNode.scala:615)\n",
      "\tat org.apache.spark.sql.catalyst.trees.CurrentOrigin$.withOrigin(TreeNode.scala:177)\n",
      "\tat org.apache.spark.sql.catalyst.trees.TreeNode.transformDownWithPruning(TreeNode.scala:615)\n",
      "\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.org$apache$spark$sql$catalyst$plans$logical$AnalysisHelper$$super$transformDownWithPruning(LogicalPlan.scala:30)\n",
      "\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.transformDownWithPruning(AnalysisHelper.scala:267)\n",
      "\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.transformDownWithPruning$(AnalysisHelper.scala:263)\n",
      "\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.transformDownWithPruning(LogicalPlan.scala:30)\n",
      "\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.transformDownWithPruning(LogicalPlan.scala:30)\n",
      "\tat org.apache.spark.sql.catalyst.trees.TreeNode.transformDown(TreeNode.scala:591)\n",
      "\tat org.apache.spark.sql.execution.QueryExecution.eagerlyExecuteCommands(QueryExecution.scala:96)\n",
      "\tat org.apache.spark.sql.execution.QueryExecution.commandExecuted$lzycompute(QueryExecution.scala:83)\n",
      "\tat org.apache.spark.sql.execution.QueryExecution.commandExecuted(QueryExecution.scala:81)\n",
      "\tat org.apache.spark.sql.execution.QueryExecution.assertCommandExecuted(QueryExecution.scala:124)\n",
      "\tat org.apache.spark.sql.DataFrameWriter.runCommand(DataFrameWriter.scala:860)\n",
      "\tat org.apache.spark.sql.DataFrameWriter.saveToV1Source(DataFrameWriter.scala:390)\n",
      "\tat org.apache.spark.sql.DataFrameWriter.saveInternal(DataFrameWriter.scala:363)\n",
      "\tat org.apache.spark.sql.DataFrameWriter.save(DataFrameWriter.scala:239)\n",
      "\tat org.apache.spark.sql.DataFrameWriter.csv(DataFrameWriter.scala:851)\n",
      "\tat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n",
      "\tat sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\n",
      "\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n",
      "\tat java.lang.reflect.Method.invoke(Method.java:498)\n",
      "\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n",
      "\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)\n",
      "\tat py4j.Gateway.invoke(Gateway.java:282)\n",
      "\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n",
      "\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n",
      "\tat py4j.GatewayConnection.run(GatewayConnection.java:238)\n",
      "\tat java.lang.Thread.run(Thread.java:750)\n",
      "Caused by: com.amazonaws.services.s3.model.AmazonS3Exception: Access Denied (Service: Amazon S3; Status Code: 403; Error Code: AccessDenied; Request ID: 9QJH8SRFCFZA38C5; S3 Extended Request ID: kYVzJWms2wMpmNvGWeeXYULtxUsUgob7cvtOXojaYT1jlf7rrYmCFS+ARGV94Vwsv73X4GPla4w=; Proxy: null), S3 Extended Request ID: kYVzJWms2wMpmNvGWeeXYULtxUsUgob7cvtOXojaYT1jlf7rrYmCFS+ARGV94Vwsv73X4GPla4w=\n",
      "\tat com.amazonaws.http.AmazonHttpClient$RequestExecutor.handleErrorResponse(AmazonHttpClient.java:1879)\n",
      "\tat com.amazonaws.http.AmazonHttpClient$RequestExecutor.handleServiceErrorResponse(AmazonHttpClient.java:1418)\n",
      "\tat com.amazonaws.http.AmazonHttpClient$RequestExecutor.executeOneRequest(AmazonHttpClient.java:1387)\n",
      "\tat com.amazonaws.http.AmazonHttpClient$RequestExecutor.executeHelper(AmazonHttpClient.java:1157)\n",
      "\tat com.amazonaws.http.AmazonHttpClient$RequestExecutor.doExecute(AmazonHttpClient.java:814)\n",
      "\tat com.amazonaws.http.AmazonHttpClient$RequestExecutor.executeWithTimer(AmazonHttpClient.java:781)\n",
      "\tat com.amazonaws.http.AmazonHttpClient$RequestExecutor.execute(AmazonHttpClient.java:755)\n",
      "\tat com.amazonaws.http.AmazonHttpClient$RequestExecutor.access$500(AmazonHttpClient.java:715)\n",
      "\tat com.amazonaws.http.AmazonHttpClient$RequestExecutionBuilderImpl.execute(AmazonHttpClient.java:697)\n",
      "\tat com.amazonaws.http.AmazonHttpClient.execute(AmazonHttpClient.java:561)\n",
      "\tat com.amazonaws.http.AmazonHttpClient.execute(AmazonHttpClient.java:541)\n",
      "\tat com.amazonaws.services.s3.AmazonS3Client.invoke(AmazonS3Client.java:5456)\n",
      "\tat com.amazonaws.services.s3.AmazonS3Client.invoke(AmazonS3Client.java:5403)\n",
      "\tat com.amazonaws.services.s3.AmazonS3Client.access$300(AmazonS3Client.java:421)\n",
      "\tat com.amazonaws.services.s3.AmazonS3Client$PutObjectStrategy.invokeServiceCall(AmazonS3Client.java:6532)\n",
      "\tat com.amazonaws.services.s3.AmazonS3Client.uploadObject(AmazonS3Client.java:1861)\n",
      "\tat com.amazonaws.services.s3.AmazonS3Client.putObject(AmazonS3Client.java:1821)\n",
      "\tat org.apache.hadoop.fs.s3a.S3AFileSystem.lambda$putObjectDirect$17(S3AFileSystem.java:2877)\n",
      "\tat org.apache.hadoop.fs.statistics.impl.IOStatisticsBinding.trackDurationOfSupplier(IOStatisticsBinding.java:604)\n",
      "\tat org.apache.hadoop.fs.s3a.S3AFileSystem.putObjectDirect(S3AFileSystem.java:2874)\n",
      "\tat org.apache.hadoop.fs.s3a.S3AFileSystem.lambda$createEmptyObject$32(S3AFileSystem.java:4534)\n",
      "\tat org.apache.hadoop.fs.s3a.Invoker.once(Invoker.java:117)\n",
      "\t... 67 more\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/mnt/yarn/usercache/livy/appcache/application_1709421197197_0002/container_1709421197197_0002_01_000001/pyspark.zip/pyspark/sql/readwriter.py\", line 1240, in csv\n",
      "    self._jwrite.csv(path)\n",
      "  File \"/mnt/yarn/usercache/livy/appcache/application_1709421197197_0002/container_1709421197197_0002_01_000001/py4j-0.10.9.5-src.zip/py4j/java_gateway.py\", line 1322, in __call__\n",
      "    answer, self.gateway_client, self.target_id, self.name)\n",
      "  File \"/mnt/yarn/usercache/livy/appcache/application_1709421197197_0002/container_1709421197197_0002_01_000001/pyspark.zip/pyspark/sql/utils.py\", line 190, in deco\n",
      "    return f(*a, **kw)\n",
      "  File \"/mnt/yarn/usercache/livy/appcache/application_1709421197197_0002/container_1709421197197_0002_01_000001/py4j-0.10.9.5-src.zip/py4j/protocol.py\", line 328, in get_return_value\n",
      "    format(target_id, \".\", name), value)\n",
      "py4j.protocol.Py4JJavaError: An error occurred while calling o3532.csv.\n",
      ": java.nio.file.AccessDeniedException: df_train_20240303-085141UTC.csv/_temporary/0: PUT 0-byte object  on df_train_20240303-085141UTC.csv/_temporary/0: com.amazonaws.services.s3.model.AmazonS3Exception: Access Denied (Service: Amazon S3; Status Code: 403; Error Code: AccessDenied; Request ID: 9QJH8SRFCFZA38C5; S3 Extended Request ID: kYVzJWms2wMpmNvGWeeXYULtxUsUgob7cvtOXojaYT1jlf7rrYmCFS+ARGV94Vwsv73X4GPla4w=; Proxy: null), S3 Extended Request ID: kYVzJWms2wMpmNvGWeeXYULtxUsUgob7cvtOXojaYT1jlf7rrYmCFS+ARGV94Vwsv73X4GPla4w=:AccessDenied\n",
      "\tat org.apache.hadoop.fs.s3a.S3AUtils.translateException(S3AUtils.java:255)\n",
      "\tat org.apache.hadoop.fs.s3a.Invoker.once(Invoker.java:119)\n",
      "\tat org.apache.hadoop.fs.s3a.Invoker.lambda$retry$4(Invoker.java:322)\n",
      "\tat org.apache.hadoop.fs.s3a.Invoker.retryUntranslated(Invoker.java:414)\n",
      "\tat org.apache.hadoop.fs.s3a.Invoker.retry(Invoker.java:318)\n",
      "\tat org.apache.hadoop.fs.s3a.Invoker.retry(Invoker.java:293)\n",
      "\tat org.apache.hadoop.fs.s3a.S3AFileSystem.createEmptyObject(S3AFileSystem.java:4532)\n",
      "\tat org.apache.hadoop.fs.s3a.S3AFileSystem.access$1900(S3AFileSystem.java:259)\n",
      "\tat org.apache.hadoop.fs.s3a.S3AFileSystem$MkdirOperationCallbacksImpl.createFakeDirectory(S3AFileSystem.java:3461)\n",
      "\tat org.apache.hadoop.fs.s3a.impl.MkdirOperation.execute(MkdirOperation.java:121)\n",
      "\tat org.apache.hadoop.fs.s3a.impl.MkdirOperation.execute(MkdirOperation.java:45)\n",
      "\tat org.apache.hadoop.fs.s3a.impl.ExecutingStoreOperation.apply(ExecutingStoreOperation.java:76)\n",
      "\tat org.apache.hadoop.fs.statistics.impl.IOStatisticsBinding.lambda$trackDurationOfOperation$5(IOStatisticsBinding.java:499)\n",
      "\tat org.apache.hadoop.fs.statistics.impl.IOStatisticsBinding.trackDuration(IOStatisticsBinding.java:444)\n",
      "\tat org.apache.hadoop.fs.s3a.S3AFileSystem.trackDurationAndSpan(S3AFileSystem.java:2337)\n",
      "\tat org.apache.hadoop.fs.s3a.S3AFileSystem.trackDurationAndSpan(S3AFileSystem.java:2356)\n",
      "\tat org.apache.hadoop.fs.s3a.S3AFileSystem.mkdirs(S3AFileSystem.java:3428)\n",
      "\tat org.apache.hadoop.fs.FileSystem.mkdirs(FileSystem.java:2449)\n",
      "\tat org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter.setupJob(FileOutputCommitter.java:359)\n",
      "\tat org.apache.spark.internal.io.HadoopMapReduceCommitProtocol.setupJob(HadoopMapReduceCommitProtocol.scala:162)\n",
      "\tat org.apache.spark.sql.execution.datasources.SQLEmrOptimizedCommitProtocol.setupJob(SQLEmrOptimizedCommitProtocol.scala:101)\n",
      "\tat org.apache.spark.sql.execution.datasources.FileFormatWriter$.write(FileFormatWriter.scala:209)\n",
      "\tat org.apache.spark.sql.execution.datasources.InsertIntoHadoopFsRelationCommand.run(InsertIntoHadoopFsRelationCommand.scala:193)\n",
      "\tat org.apache.spark.sql.execution.command.DataWritingCommandExec.sideEffectResult$lzycompute(commands.scala:113)\n",
      "\tat org.apache.spark.sql.execution.command.DataWritingCommandExec.sideEffectResult(commands.scala:111)\n",
      "\tat org.apache.spark.sql.execution.command.DataWritingCommandExec.executeCollect(commands.scala:125)\n",
      "\tat org.apache.spark.sql.execution.QueryExecution$$anonfun$eagerlyExecuteCommands$1.$anonfun$applyOrElse$1(QueryExecution.scala:103)\n",
      "\tat org.apache.spark.sql.catalyst.QueryPlanningTracker$.withTracker(QueryPlanningTracker.scala:107)\n",
      "\tat org.apache.spark.sql.execution.SQLExecution$.withTracker(SQLExecution.scala:224)\n",
      "\tat org.apache.spark.sql.execution.SQLExecution$.executeQuery$1(SQLExecution.scala:114)\n",
      "\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$7(SQLExecution.scala:139)\n",
      "\tat org.apache.spark.sql.catalyst.QueryPlanningTracker$.withTracker(QueryPlanningTracker.scala:107)\n",
      "\tat org.apache.spark.sql.execution.SQLExecution$.withTracker(SQLExecution.scala:224)\n",
      "\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$6(SQLExecution.scala:139)\n",
      "\tat org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:245)\n",
      "\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$1(SQLExecution.scala:138)\n",
      "\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:779)\n",
      "\tat org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:68)\n",
      "\tat org.apache.spark.sql.execution.QueryExecution$$anonfun$eagerlyExecuteCommands$1.applyOrElse(QueryExecution.scala:100)\n",
      "\tat org.apache.spark.sql.execution.QueryExecution$$anonfun$eagerlyExecuteCommands$1.applyOrElse(QueryExecution.scala:96)\n",
      "\tat org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$transformDownWithPruning$1(TreeNode.scala:615)\n",
      "\tat org.apache.spark.sql.catalyst.trees.CurrentOrigin$.withOrigin(TreeNode.scala:177)\n",
      "\tat org.apache.spark.sql.catalyst.trees.TreeNode.transformDownWithPruning(TreeNode.scala:615)\n",
      "\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.org$apache$spark$sql$catalyst$plans$logical$AnalysisHelper$$super$transformDownWithPruning(LogicalPlan.scala:30)\n",
      "\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.transformDownWithPruning(AnalysisHelper.scala:267)\n",
      "\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.transformDownWithPruning$(AnalysisHelper.scala:263)\n",
      "\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.transformDownWithPruning(LogicalPlan.scala:30)\n",
      "\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.transformDownWithPruning(LogicalPlan.scala:30)\n",
      "\tat org.apache.spark.sql.catalyst.trees.TreeNode.transformDown(TreeNode.scala:591)\n",
      "\tat org.apache.spark.sql.execution.QueryExecution.eagerlyExecuteCommands(QueryExecution.scala:96)\n",
      "\tat org.apache.spark.sql.execution.QueryExecution.commandExecuted$lzycompute(QueryExecution.scala:83)\n",
      "\tat org.apache.spark.sql.execution.QueryExecution.commandExecuted(QueryExecution.scala:81)\n",
      "\tat org.apache.spark.sql.execution.QueryExecution.assertCommandExecuted(QueryExecution.scala:124)\n",
      "\tat org.apache.spark.sql.DataFrameWriter.runCommand(DataFrameWriter.scala:860)\n",
      "\tat org.apache.spark.sql.DataFrameWriter.saveToV1Source(DataFrameWriter.scala:390)\n",
      "\tat org.apache.spark.sql.DataFrameWriter.saveInternal(DataFrameWriter.scala:363)\n",
      "\tat org.apache.spark.sql.DataFrameWriter.save(DataFrameWriter.scala:239)\n",
      "\tat org.apache.spark.sql.DataFrameWriter.csv(DataFrameWriter.scala:851)\n",
      "\tat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n",
      "\tat sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\n",
      "\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n",
      "\tat java.lang.reflect.Method.invoke(Method.java:498)\n",
      "\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n",
      "\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)\n",
      "\tat py4j.Gateway.invoke(Gateway.java:282)\n",
      "\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n",
      "\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n",
      "\tat py4j.GatewayConnection.run(GatewayConnection.java:238)\n",
      "\tat java.lang.Thread.run(Thread.java:750)\n",
      "Caused by: com.amazonaws.services.s3.model.AmazonS3Exception: Access Denied (Service: Amazon S3; Status Code: 403; Error Code: AccessDenied; Request ID: 9QJH8SRFCFZA38C5; S3 Extended Request ID: kYVzJWms2wMpmNvGWeeXYULtxUsUgob7cvtOXojaYT1jlf7rrYmCFS+ARGV94Vwsv73X4GPla4w=; Proxy: null), S3 Extended Request ID: kYVzJWms2wMpmNvGWeeXYULtxUsUgob7cvtOXojaYT1jlf7rrYmCFS+ARGV94Vwsv73X4GPla4w=\n",
      "\tat com.amazonaws.http.AmazonHttpClient$RequestExecutor.handleErrorResponse(AmazonHttpClient.java:1879)\n",
      "\tat com.amazonaws.http.AmazonHttpClient$RequestExecutor.handleServiceErrorResponse(AmazonHttpClient.java:1418)\n",
      "\tat com.amazonaws.http.AmazonHttpClient$RequestExecutor.executeOneRequest(AmazonHttpClient.java:1387)\n",
      "\tat com.amazonaws.http.AmazonHttpClient$RequestExecutor.executeHelper(AmazonHttpClient.java:1157)\n",
      "\tat com.amazonaws.http.AmazonHttpClient$RequestExecutor.doExecute(AmazonHttpClient.java:814)\n",
      "\tat com.amazonaws.http.AmazonHttpClient$RequestExecutor.executeWithTimer(AmazonHttpClient.java:781)\n",
      "\tat com.amazonaws.http.AmazonHttpClient$RequestExecutor.execute(AmazonHttpClient.java:755)\n",
      "\tat com.amazonaws.http.AmazonHttpClient$RequestExecutor.access$500(AmazonHttpClient.java:715)\n",
      "\tat com.amazonaws.http.AmazonHttpClient$RequestExecutionBuilderImpl.execute(AmazonHttpClient.java:697)\n",
      "\tat com.amazonaws.http.AmazonHttpClient.execute(AmazonHttpClient.java:561)\n",
      "\tat com.amazonaws.http.AmazonHttpClient.execute(AmazonHttpClient.java:541)\n",
      "\tat com.amazonaws.services.s3.AmazonS3Client.invoke(AmazonS3Client.java:5456)\n",
      "\tat com.amazonaws.services.s3.AmazonS3Client.invoke(AmazonS3Client.java:5403)\n",
      "\tat com.amazonaws.services.s3.AmazonS3Client.access$300(AmazonS3Client.java:421)\n",
      "\tat com.amazonaws.services.s3.AmazonS3Client$PutObjectStrategy.invokeServiceCall(AmazonS3Client.java:6532)\n",
      "\tat com.amazonaws.services.s3.AmazonS3Client.uploadObject(AmazonS3Client.java:1861)\n",
      "\tat com.amazonaws.services.s3.AmazonS3Client.putObject(AmazonS3Client.java:1821)\n",
      "\tat org.apache.hadoop.fs.s3a.S3AFileSystem.lambda$putObjectDirect$17(S3AFileSystem.java:2877)\n",
      "\tat org.apache.hadoop.fs.statistics.impl.IOStatisticsBinding.trackDurationOfSupplier(IOStatisticsBinding.java:604)\n",
      "\tat org.apache.hadoop.fs.s3a.S3AFileSystem.putObjectDirect(S3AFileSystem.java:2874)\n",
      "\tat org.apache.hadoop.fs.s3a.S3AFileSystem.lambda$createEmptyObject$32(S3AFileSystem.java:4534)\n",
      "\tat org.apache.hadoop.fs.s3a.Invoker.once(Invoker.java:117)\n",
      "\t... 67 more\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_train.write.option('header', True).csv(f's3a://{object.bucket_name}/df_train_{current_datetime}.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6bf9d7fb-c819-496c-8b06-1b492af3145b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "SparkMagic PySpark",
   "language": "python",
   "name": "pysparkkernel"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "python",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "pyspark",
   "pygments_lexer": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
