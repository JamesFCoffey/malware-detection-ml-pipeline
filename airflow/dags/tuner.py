from datetime import datetime

# airflow operators
import airflow
from airflow.models import DAG
from airflow.operators.dummy_operator import DummyOperator
from airflow.operators.python_operator import PythonOperator, BranchPythonOperator

# airflow sagemaker operators
from airflow.providers.amazon.aws.operators.sagemaker import SageMakerTuningOperator, SageMakerTransformOperator, SageMakerEndpointOperator
from airflow.providers.amazon.aws.hooks.s3 import S3Hook

# sagemaker sdk
import boto3
import sagemaker
from sagemaker.sklearn.estimator import SKLearn
from sagemaker.tuner import ContinuousParameter, HyperparameterTuner, TuningJobCompletionCriteriaConfig

# airflow sagemaker configuration
from sagemaker.workflow.airflow import tuning_config, transform_config_from_estimator, deploy_config_from_estimator

# ml workflow specific
import pandas as pd
import s3fs
from sklearn.metrics import average_precision_score
import config as cfg

# =============================================================================
# functions
# =============================================================================

def get_sagemaker_role_arn(role_name, region_name):
    iam = boto3.client('iam', region_name=region_name)
    response = iam.get_role(RoleName=role_name)
    return response['Role']['Arn']

def get_latest_csv_s3_path(bucket_name, prefix):
    '''
    Finds the latest CSV file in the specified S3 bucket and prefix using Airflow's S3Hook.

    Args:
        bucket_name (str): The name of the S3 bucket.
        prefix (str): The prefix path within the S3 bucket.

    Returns:
        str: The S3 path of the latest CSV file. Returns None if no CSV files are found.
    '''
    # Initialize an S3Hook
    s3_hook = S3Hook()

    # List objects within the bucket and prefix
    list_objects = s3_hook.list_keys(bucket_name=bucket_name, prefix=prefix, delimiter='/')

    # Filter out objects that are not CSV files
    csv_files = [obj for obj in list_objects if obj.endswith('.csv')] if list_objects else []

    # Initialize an empty list to hold (last_modified, key) tuples
    csv_files_details = []

    for csv_file in csv_files:
        # Get the object's metadata
        obj_metadata = s3_hook.get_key(key=csv_file, bucket_name=bucket_name)
        # Append a tuple of (last_modified, key) for each CSV file
        csv_files_details.append((obj_metadata.last_modified, csv_file))

    # Sort the list of tuples by the last_modified timestamp in descending order
    csv_files_details.sort(key=lambda x: x[0], reverse=True)

    # Get the most recent CSV file's key (if there are any CSV files)
    if csv_files_details:
        latest_csv_key = csv_files_details[0][1]
        # Construct the S3 path for the latest CSV file
        return f's3://{bucket_name}/{latest_csv_key}'

    return None

def evaluate_model(bucket_name):
    s3 = boto3.client('s3')

    # Get filenames
    pred_filename = f"{get_latest_csv_s3_path(bucket_name, 'test/').split('/')[-1]}.out"
    groundtruth_filename = get_latest_csv_s3_path(bucket_name, 'groundtruth/').split('/')[-1]

    # Download prediction file from S3
    s3.download_file(bucket_name, f'predictions/{pred_filename}', pred_filename)
    
    # Process prediction file to get predictions
    with open(pred_filename, 'r') as file:
        file_content = file.read().strip()
    file_content = file_content[1:-1]
    file_content = file_content.split('][')
    file_content = ', '.join(file_content)
    file_content = file_content.split(', ')
    pred = pd.Series([float(i) for i in file_content])

    # Download groundtruth and load into pandas series
    s3.download_file(bucket_name, f'groundtruth/{groundtruth_filename}', groundtruth_filename)
    y_true = pd.read_csv(groundtruth_filename, header=None, dtype='float').squeeze()

    # Calculate average precision score
    score = average_precision_score(y_true, pred)

    # Download benchmark.txt and compare
    s3.download_file(bucket_name, 'benchmark/benchmark.txt', 'benchmark.txt')
    with open('benchmark.txt', 'r') as file:
        benchmark = float(file.read().strip())

    return score > benchmark

def decide_next_task(**kwargs):
    ti = kwargs['ti']
    result = ti.xcom_pull(task_ids='evaluate_model_task')
    if result:
        return 'deploy_sagemaker_endpoint'
    else:
        return 'send_failure_email'

# =============================================================================
# setting up training, tuning and transform configuration
# =============================================================================


# read config file
config = cfg.config

# set configuration for tasks
sess = sagemaker.Session(default_bucket = 'datasets-20240228')
# role = sagemaker.get_execution_role()
role = 'arn:aws:iam::<ACCOUNT_ID>:role/service-role/AmazonSageMaker-ExecutionRole-20240224T215408'
region = sagemaker.Session().boto_region_name

bucket_name = 'datasets-20240228'
train_path = get_latest_csv_s3_path(bucket_name, 'train/')
validation_path = get_latest_csv_s3_path(bucket_name, 'validation/')
test_path = get_latest_csv_s3_path(bucket_name, 'test/')

train_df = pd.read_csv(train_path, nrows=1)

hyperparameters = {
    'experiment_name': 'malware-detection',
    'features': ' '.join(list(train_df.drop(['test_set', 'label'], axis=1).columns)),
    'target': 'label',
    'train-file': train_path.split('/')[-1],
    'validation-file': validation_path.split('/')[-1],
}

metric_definitions = [{'Name': 'avg-precision', 'Regex': 'avg-precision: ([0-9.]+).*$'}]

# Create estimator
estimator = SKLearn(
    role=role,
    instance_count=1,
    instance_type='ml.m5.xlarge',
    source_dir='airflow/pipeline',
    entry_point='train.py',
    git_config = {'repo': 'https://github.com/JamesFCoffey/malware-detection-ml-pipeline.git'},
    hyperparameters=hyperparameters,
    metric_definitions=metric_definitions,
    framework_version='1.2-1',
    py_version='py3'
)

# create tuner
tuner = HyperparameterTuner(
    estimator=estimator,
    objective_metric_name = 'avg-precision',
    objective_type = 'Maximize',
    hyperparameter_ranges = {
        'alpha': ContinuousParameter(0.00001, 0.001),
        'l1_ratio': ContinuousParameter(0.0, 1.0),
        },
    metric_definitions = [{'Name': 'avg-precision', 'Regex': 'avg-precision: ([0-9.]+).*$'}],
    max_jobs=2,
    max_parallel_jobs=2,
    completion_criteria_config = TuningJobCompletionCriteriaConfig(complete_on_convergence=True)
)

# create tuning config
tuner_config = tuning_config(
    tuner=tuner,
    inputs={'train': train_path,
            'validation': validation_path})

# create transform config
transform_config = transform_config_from_estimator(
    estimator=estimator,
    task_id='model_tuning',
    task_type='tuning',
    instance_count=1,
    instance_type='ml.m5.xlarge',
    data=test_path,
    data_type='S3Prefix',
    content_type='text/csv',
    strategy='MultiRecord',
    split_type='Line'
)

# Because of partial_fit, a new submit directory is created for each chunk.
# This corrects the Jinja code to directly take the 'sagemaker_submit_directory'
# from the tuning step instead of using 'sagemaker_job_name'. Otherwise, a you
# get an error: 'An error occurred (404) when calling the HeadObject operation:
# Not Found'.
transform_config['Model']['PrimaryContainer']['Environment'][
    'SAGEMAKER_SUBMIT_DIRECTORY'
] = "{{ ti.xcom_pull(task_ids='model_tuning')['Tuning']['TrainingJobDefinition']['StaticHyperParameters']['sagemaker_submit_directory'].strip('\"') }}"

# create endpoint deployment config
deploy_config = deploy_config_from_estimator(
    estimator=estimator,
    task_id='model_tuning',
    task_type='tuning',
    initial_instance_count=1,
    instance_type='ml.m5.xlarge'
)

# Because of partial_fit, a new submit directory is created for each chunk.
# This corrects the Jinja code to directly take the 'sagemaker_submit_directory'
# from the tuning step instead of using 'sagemaker_job_name'. Otherwise, a you
# get an error: 'An error occurred (404) when calling the HeadObject operation:
# Not Found'.
deploy_config['Model']['PrimaryContainer']['Environment'][
    'SAGEMAKER_SUBMIT_DIRECTORY'
] = "{{ ti.xcom_pull(task_ids='model_tuning')['Tuning']['TrainingJobDefinition']['StaticHyperParameters']['sagemaker_submit_directory'].strip('\"') }}"

# =============================================================================
# define airflow DAG and tasks
# =============================================================================

# define airflow DAG

with DAG(
    dag_id='sagemaker-ml-pipeline',
    schedule_interval=None,
    start_date=datetime(2021, 1, 1),
    tags=['mlops'],
    catchup=False,
) as dag:
    # set the tasks in the DAG
    
    # launch sagemaker hyperparameter job and wait until it completes
    tune_model_task = SageMakerTuningOperator(
        task_id='model_tuning',
        config=tuner_config,
        aws_conn_id='airflow-sagemaker',
        wait_for_completion=True,
        check_interval=30
    )

    # launch sagemaker batch transform job and wait until it completes
    batch_transform_task = SageMakerTransformOperator(
        task_id='predicting',
        config=transform_config,
        aws_conn_id='airflow-sagemaker',
        wait_for_completion=True,
        check_interval=30
    )

    evaluate_model_task = PythonOperator(
        task_id='evaluate_model_task',
        python_callable=evaluate_model,
        op_args=[bucket_name]
    )

    branch_task = BranchPythonOperator(
        task_id='branch_task',
        python_callable=decide_next_task,
        provide_context=True
    )

    deploy_endpoint_task = SageMakerEndpointOperator(
        task_id='deploy_sagemaker_endpoint',
        config=deploy_config
    )

    end_pipeline = DummyOperator(
        task_id='end_pipeline'
    )

    # set the dependencies between tasks

    tune_model_task >> batch_transform_task >> evaluate_model_task
    evaluate_model_task >> branch_task
    branch_task >> deploy_endpoint_task >> end_pipeline
    branch_task >> end_pipeline