from datetime import datetime

# airflow operators
import airflow
from airflow.models import DAG
from airflow.utils.trigger_rule import TriggerRule
from airflow.operators.python_operator import BranchPythonOperator

# airflow sagemaker operators
from airflow.providers.amazon.aws.operators.sagemaker import SageMakerTuningOperator
from airflow.providers.amazon.aws.operators.sagemaker import SageMakerTransformOperator
from airflow.providers.amazon.aws.hooks.s3 import S3Hook
# from airflow.contrib.hooks.aws_hook import AwsHook

# sagemaker sdk
import boto3
import sagemaker
from sagemaker.sklearn.estimator import SKLearn
from sagemaker.amazon.amazon_estimator import get_image_uri
from sagemaker.tuner import ContinuousParameter, HyperparameterTuner, TuningJobCompletionCriteriaConfig

# airflow sagemaker configuration
from sagemaker.workflow.airflow import tuning_config
from sagemaker.workflow.airflow import transform_config_from_estimator

# ml workflow specific
import pandas as pd
import s3fs
import config as cfg

# =============================================================================
# functions
# =============================================================================

def get_sagemaker_role_arn(role_name, region_name):
    iam = boto3.client('iam', region_name=region_name)
    response = iam.get_role(RoleName=role_name)
    return response['Role']['Arn']

def get_latest_csv_s3_path(bucket_name, prefix):
    """
    Finds the latest CSV file in the specified S3 bucket and prefix using Airflow's S3Hook.

    Args:
        bucket_name (str): The name of the S3 bucket.
        prefix (str): The prefix path within the S3 bucket.

    Returns:
        str: The S3 path of the latest CSV file. Returns None if no CSV files are found.
    """
    # Initialize an S3Hook
    s3_hook = S3Hook()

    # List objects within the bucket and prefix
    list_objects = s3_hook.list_keys(bucket_name=bucket_name, prefix=prefix, delimiter='/')

    # Filter out objects that are not CSV files
    csv_files = [obj for obj in list_objects if obj.endswith('.csv')] if list_objects else []

    # Initialize an empty list to hold (last_modified, key) tuples
    csv_files_details = []

    for csv_file in csv_files:
        # Get the object's metadata
        obj_metadata = s3_hook.get_key(key=csv_file, bucket_name=bucket_name)
        # Append a tuple of (last_modified, key) for each CSV file
        csv_files_details.append((obj_metadata.last_modified, csv_file))

    # Sort the list of tuples by the last_modified timestamp in descending order
    csv_files_details.sort(key=lambda x: x[0], reverse=True)

    # Get the most recent CSV file's key (if there are any CSV files)
    if csv_files_details:
        latest_csv_key = csv_files_details[0][1]
        # Construct the S3 path for the latest CSV file
        return f"s3://{bucket_name}/{latest_csv_key}"

    return None

# =============================================================================
# setting up training, tuning and transform configuration
# =============================================================================


# read config file
config = cfg.config

# set configuration for tasks
sess = sagemaker.Session(default_bucket = 'datasets-20240228')
# role = sagemaker.get_execution_role()
role = 'arn:aws:iam::<ACCOUNT_ID>:role/service-role/AmazonSageMaker-ExecutionRole-20240224T215408'
region = sagemaker.Session().boto_region_name

bucket_name = 'datasets-20240228'
train_path = get_latest_csv_s3_path(bucket_name, 'train/')
test_path = get_latest_csv_s3_path(bucket_name, 'test/')

train_df = pd.read_csv(train_path, nrows=1)

hyperparameters = {
    'experiment_name': 'malware-detection',
    'features': ' '.join(list(train_df.drop(['test_set', 'label'], axis=1).columns)),
    'target': 'label',
    'train-file': train_path.split('/')[-1],
    'test-file': test_path.split('/')[-1],
}

metric_definitions = [{'Name': 'avg-precision', 'Regex': 'avg-precision: ([0-9.]+).*$'}]

# Create estimator
estimator = SKLearn(
    role=role,
    instance_count=1,
    instance_type='ml.m5.xlarge',
    source_dir='airflow/pipeline',
    entry_point='train.py',
    git_config = {'repo': 'https://github.com/JamesFCoffey/malware-detection-ml-pipeline.git'},
    hyperparameters=hyperparameters,
    metric_definitions=metric_definitions,
    framework_version='1.0-1',
    py_version='py3'
)

# create tuner
tuner = HyperparameterTuner(
    estimator=estimator,
    objective_metric_name = 'avg-precision',
    objective_type = 'Maximize',
    hyperparameter_ranges = {
        'alpha': ContinuousParameter(0.00001, 0.001),
        'l1_ratio': ContinuousParameter(0.0, 1.0),
        },
    metric_definitions = [{'Name': 'avg-precision', 'Regex': 'avg-precision: ([0-9.]+).*$'}],
    max_jobs=2,
    max_parallel_jobs=2,
    completion_criteria_config = TuningJobCompletionCriteriaConfig(complete_on_convergence=True)
)

# create tuning config
tuner_config = tuning_config(
    tuner=tuner,
    inputs={'train': train_path,
            'test': test_path})

# create transform config
transform_config = transform_config_from_estimator(
    estimator=estimator,
    task_id='model_tuning',
    task_type='tuning',
    instance_count=1,
    instance_type='ml.m5.xlarge',
    data=test_path,
    data_type='S3Prefix',
    content_type='text/csv',
    output_path='s3://datasets-20240228/transform/'
)

# =============================================================================
# define airflow DAG and tasks
# =============================================================================

# define airflow DAG

with DAG(
    dag_id='sagemaker-ml-pipeline',
    schedule_interval=None,
    start_date=datetime(2021, 1, 1),
    tags=['mlops'],
    catchup=False,
) as dag:
    # set the tasks in the DAG
    
    # launch sagemaker hyperparameter job and wait until it completes
    tune_model_task = SageMakerTuningOperator(
        task_id='model_tuning',
        config=tuner_config,
        aws_conn_id='airflow-sagemaker',
        wait_for_completion=True,
        check_interval=30
    )

    # launch sagemaker batch transform job and wait until it completes
    batch_transform_task = SageMakerTransformOperator(
        task_id='predicting',
        config=transform_config,
        aws_conn_id='airflow-sagemaker',
        wait_for_completion=True,
        check_interval=30
    )

    # set the dependencies between tasks

    tune_model_task >> batch_transform_task