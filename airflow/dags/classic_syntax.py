from datetime import datetime

from utils.config import config
from utils.helper_functions import (
    correct_configs,
    decide_next_task,
    evaluate_model,
    paths_to_string,
    prepare_configs,
)

from airflow.models import DAG
from airflow.operators.dummy_operator import DummyOperator
from airflow.operators.python_operator import BranchPythonOperator, PythonOperator
from airflow.providers.amazon.aws.operators.emr import (
    EmrServerlessCreateApplicationOperator,
    EmrServerlessDeleteApplicationOperator,
    EmrServerlessStartJobOperator,
)
from airflow.providers.amazon.aws.operators.s3 import S3ListOperator
from airflow.providers.amazon.aws.operators.sagemaker import (
    SageMakerEndpointOperator,
    SageMakerTransformOperator,
    SageMakerTuningOperator,
)

# Use the configuration from config.py
ACCOUNT_ID = config["account_id"]
AIRFLOW_BUCKET = config["airflow_bucket"]
DATASET_BUCKET = config["dataset_bucket"]
EMR_ROLE_ARN = config["emr_role_arn"]
DEFAULT_MONITORING_CONFIG = config["default_monitoring_config"]
SAGEMAKER_ROLE_ARN = config["sagemaker_role_arn"]

# Define the DAG and set its properties.
with DAG(
    dag_id="sagemaker-ml-pipeline",
    schedule_interval=None,
    start_date=datetime(2021, 1, 1),
    tags=["mlops"],
    catchup=False,
) as dag:
    s3_files = S3ListOperator(
        task_id="list_3s_files",
        bucket=DATASET_BUCKET,
        prefix="raw/",
        do_xcom_push=True,
    )

    paths_string = PythonOperator(
        task_id="paths_string",
        python_callable=paths_to_string,
        op_args=[DATASET_BUCKET],
        provide_context=True,
    )

    create_app = EmrServerlessCreateApplicationOperator(
        task_id="create_spark_app",
        job_type="SPARK",
        release_label="emr-6.8.0",
        config={"name": "ml_preprocess"},
    )

    application_id = create_app.output

    spark_job = EmrServerlessStartJobOperator(
        task_id="preprocess_data",
        application_id=application_id,
        execution_role_arn=EMR_ROLE_ARN,
        job_driver={
            "sparkSubmit": {
                "entryPoint": f"s3://{AIRFLOW_BUCKET}/pipeline/preprocess.py",
                "entryPointArguments": [
                    DATASET_BUCKET,
                    "{{ ti.xcom_pull(task_ids=['paths_string']) }}",
                ],
                "sparkSubmitParameters": (
                    f"--conf spark.archives=s3://{AIRFLOW_BUCKET}/venv/pyspark_venv.tar.gz#environment "
                    "--conf spark.emr-serverless.driverEnv.PYSPARK_DRIVER_PYTHON=./environment/bin/python "
                    "--conf spark.emr-serverless.driverEnv.PYSPARK_PYTHON=./environment/bin/python "
                    "--conf spark.executorEnv.PYSPARK_PYTHON=./environment/bin/python"
                ),
            }
        },
        waiter_countdown=120 * 60,
        waiter_check_interval_seconds=2 * 60,
        configuration_overrides=DEFAULT_MONITORING_CONFIG,
    )

    delete_app = EmrServerlessDeleteApplicationOperator(
        task_id="delete_app",
        application_id=application_id,
        trigger_rule="all_done",
    )

    prepare_configs_task = PythonOperator(
        task_id="prepare_configs",
        python_callable=prepare_configs,
        op_args=[DATASET_BUCKET, SAGEMAKER_ROLE_ARN],
        provide_context=True,
    )

    # Launch sagemaker hyperparameter job and wait until it completes
    tune_model_task = SageMakerTuningOperator(
        task_id="model_tuning",
        config=prepare_configs_task.output["tuner_config"],
        aws_conn_id="airflow-sagemaker",
        wait_for_completion=True,
        check_interval=30,
    )

    correct_configs_task = PythonOperator(
        task_id="correct_configs",
        python_callable=correct_configs,
        provide_context=True,
    )

    # Launch sagemaker batch transform job and wait until it completes
    batch_transform_task = SageMakerTransformOperator(
        task_id="predicting",
        config=correct_configs_task.output["transform_config"],
        aws_conn_id="airflow-sagemaker",
        wait_for_completion=True,
        check_interval=30,
    )

    evaluate_model_task = PythonOperator(
        task_id="evaluate_model_task",
        python_callable=evaluate_model,
        op_args=[DATASET_BUCKET],
        provide_context=True,
    )

    branch_task = BranchPythonOperator(
        task_id="branch_task",
        python_callable=decide_next_task,
        provide_context=True,
    )

    deploy_endpoint_task = SageMakerEndpointOperator(
        task_id="deploy_sagemaker_endpoint",
        config=correct_configs_task.output["deploy_config"],
    )

    end_pipeline = DummyOperator(task_id="end_pipeline")

    # Set the task dependencies.
    s3_files >> paths_string >> spark_job >> prepare_configs_task >> tune_model_task
    create_app >> spark_job >> delete_app
    (
        tune_model_task
        >> correct_configs_task
        >> batch_transform_task
        >> evaluate_model_task
    )
    evaluate_model_task >> branch_task
    branch_task >> deploy_endpoint_task >> end_pipeline
    branch_task >> end_pipeline
