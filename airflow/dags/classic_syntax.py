from datetime import datetime

# airflow operators
from airflow.models import DAG
from airflow.operators.dummy_operator import DummyOperator
from airflow.operators.python_operator import PythonOperator, BranchPythonOperator

# airflow sagemaker operators
from airflow.providers.amazon.aws.operators.emr import (
    EmrServerlessCreateApplicationOperator,
    EmrServerlessStartJobOperator,
    EmrServerlessDeleteApplicationOperator,
)
from airflow.providers.amazon.aws.operators.s3 import S3ListOperator
from airflow.providers.amazon.aws.operators.sagemaker import (
    SageMakerTuningOperator,
    SageMakerTransformOperator,
    SageMakerEndpointOperator,
)
from airflow.providers.amazon.aws.hooks.s3 import S3Hook

# sagemaker sdk
import boto3
import sagemaker
from sagemaker.sklearn.estimator import SKLearn
from sagemaker.tuner import (
    ContinuousParameter,
    HyperparameterTuner,
    TuningJobCompletionCriteriaConfig,
)

# airflow sagemaker configuration
from sagemaker.workflow.airflow import (
    tuning_config,
    transform_config_from_estimator,
    deploy_config_from_estimator,
)

# ml workflow specific
import pandas as pd
import s3fs
from sklearn.metrics import average_precision_score
import config as cfg

# =============================================================================
# variables
# =============================================================================

ACCOUNT_ID = "<ACCOUNT_ID>"
S3_BUCKET = "airflow-coffeyjam"
DATASET_BUCKET = "datasets-20240228"
EMR_ROLE_ARN = f"arn:aws:iam::{ACCOUNT_ID}:role/EMRServerlessS3RuntimeRole"

DEFAULT_MONITORING_CONFIG = {
    "monitoringConfiguration": {
        "s3MonitoringConfiguration": {"logUri": f"s3://{S3_BUCKET}/logs/"}
    },
}

# =============================================================================
# functions
# =============================================================================

def paths_to_string(**kwargs):
    ti = kwargs["ti"]

    files = ti.xcom_pull(task_ids="list_3s_files")
    datasets = []
    for file in files:
        if file.endswith(".csv"):
            datasets.append("s3://" + DATASET_BUCKET + "/" + file)
    return ",".join(datasets)

def get_sagemaker_role_arn(role_name, region_name):
    iam = boto3.client("iam", region_name=region_name)
    response = iam.get_role(RoleName=role_name)
    return response["Role"]["Arn"]

def get_latest_csv_s3_path(bucket_name, prefix):
    """
    Finds the latest CSV file in the specified S3 bucket and prefix using
    Airflow's S3Hook.

    Args:
        bucket_name (str): The name of the S3 bucket. prefix (str): The prefix
            path within the S3 bucket.

    Returns:
        str: The S3 path of the latest CSV file. Returns None if no CSV files
            are found.
    """
    # Initialize an S3Hook
    s3_hook = S3Hook()

    # List objects within the bucket and prefix
    list_objects = s3_hook.list_keys(
        bucket_name=bucket_name, prefix=prefix, delimiter="/"
    )

    # Filter out objects that are not CSV files
    csv_files = (
        [obj for obj in list_objects if obj.endswith(".csv")] if list_objects else []
    )

    # Initialize an empty list to hold (last_modified, key) tuples
    csv_files_details = []

    for csv_file in csv_files:
        # Get the object's metadata
        obj_metadata = s3_hook.get_key(key=csv_file, bucket_name=bucket_name)
        # Append a tuple of (last_modified, key) for each CSV file
        csv_files_details.append((obj_metadata.last_modified, csv_file))

    # Sort the list of tuples by the last_modified timestamp in descending order
    csv_files_details.sort(key=lambda x: x[0], reverse=True)

    # Get the most recent CSV file's key (if there are any CSV files)
    if csv_files_details:
        latest_csv_key = csv_files_details[0][1]
        # Construct the S3 path for the latest CSV file
        return f"s3://{bucket_name}/{latest_csv_key}"

    return None

def evaluate_model(bucket_name):
    s3 = boto3.client("s3")

    # Get filenames
    pred_filename = f"{get_latest_csv_s3_path(bucket_name, 'test/').split('/')[-1]}.out"
    groundtruth_filename = get_latest_csv_s3_path(bucket_name, "groundtruth/").split(
        "/"
    )[-1]

    # Download prediction file from S3
    s3.download_file(bucket_name, f"predictions/{pred_filename}", pred_filename)

    # Process prediction file to get predictions
    with open(pred_filename, "r") as file:
        file_content = file.read().strip()
    file_content = file_content[1:-1]
    file_content = file_content.split("][")
    file_content = ", ".join(file_content)
    file_content = file_content.split(", ")
    pred = pd.Series([float(i) for i in file_content])

    # Download groundtruth and load into pandas series
    s3.download_file(
        bucket_name, f"groundtruth/{groundtruth_filename}", groundtruth_filename
    )
    y_true = pd.read_csv(groundtruth_filename, header=None, dtype="float").squeeze()

    # Calculate average precision score
    score = average_precision_score(y_true, pred)

    # Download benchmark.txt and compare
    s3.download_file(bucket_name, "benchmark/benchmark.txt", "benchmark.txt")
    with open("benchmark.txt", "r") as file:
        benchmark = float(file.read().strip())

    return score > benchmark

def decide_next_task(**kwargs):
    ti = kwargs["ti"]
    result = ti.xcom_pull(task_ids="evaluate_model_task")
    if result:
        return "deploy_sagemaker_endpoint"
    else:
        return "send_failure_email"

# =============================================================================
# setting up training, tuning and transform configuration
# =============================================================================

# read config file
config = cfg.config

# set configuration for tasks
sess = sagemaker.Session(default_bucket="datasets-20240228")
# role = sagemaker.get_execution_role()
role = "arn:aws:iam::<ACCOUNT_ID>:role/service-role/AmazonSageMaker-ExecutionRole-20240224T215408"
region = sagemaker.Session().boto_region_name

bucket_name = "datasets-20240228"

def prepare_configs(bucket_name, **kwargs):
    ti = kwargs["ti"]

    train_path = get_latest_csv_s3_path(bucket_name, "train/")
    validation_path = get_latest_csv_s3_path(bucket_name, "validation/")
    test_path = get_latest_csv_s3_path(bucket_name, "test/")

    train_df = pd.read_csv(train_path, nrows=1)

    hyperparameters = {
        "experiment_name": "malware-detection",
        "features": " ".join(list(train_df.drop(["test_set", "label"], axis=1).columns)),
        "target": "label",
        "train-file": train_path.split("/")[-1],
        "validation-file": validation_path.split("/")[-1],
    }

    metric_definitions = [{"Name": "avg-precision", "Regex": "avg-precision: ([0-9.]+).*$"}]

    # Create estimator
    estimator = SKLearn(
        role=role,
        instance_count=1,
        instance_type="ml.m5.xlarge",
        source_dir="airflow/pipeline",
        entry_point="train.py",
        git_config={
            "repo": "https://github.com/JamesFCoffey/malware-detection-ml-pipeline.git"
        },
        hyperparameters=hyperparameters,
        metric_definitions=metric_definitions,
        framework_version="1.2-1",
        py_version="py3",
    )

    # create tuner
    tuner = HyperparameterTuner(
        estimator=estimator,
        objective_metric_name="avg-precision",
        objective_type="Maximize",
        hyperparameter_ranges={
            "alpha": ContinuousParameter(0.00001, 0.001),
            "l1_ratio": ContinuousParameter(0.0, 1.0),
        },
        metric_definitions=[
            {"Name": "avg-precision", "Regex": "avg-precision: ([0-9.]+).*$"}
        ],
        max_jobs=2,
        max_parallel_jobs=2,
        completion_criteria_config=TuningJobCompletionCriteriaConfig(
            complete_on_convergence=True
        ),
    )

    # create tuning config
    tuner_config = tuning_config(
        tuner=tuner, inputs={"train": train_path, "validation": validation_path}
    )

    # create transform config
    transform_config = transform_config_from_estimator(
        estimator=estimator,
        task_id="model_tuning",
        task_type="tuning",
        instance_count=1,
        instance_type="ml.m5.xlarge",
        data=test_path,
        data_type="S3Prefix",
        content_type="text/csv",
        strategy="MultiRecord",
        output_path=f"s3://{bucket_name}/predictions",
        split_type="Line",
    )

    # Get the current date and time
    now = datetime.now()

    # Format the date and time as a string in the YYYYMMDD-HHMMSS format
    datetime_str = now.strftime("%Y%m%d-%H%M%S")

    # create endpoint deployment config
    deploy_config = deploy_config_from_estimator(
        estimator=estimator,
        task_id="model_tuning",
        task_type="tuning",
        initial_instance_count=1,
        instance_type="ml.m5.xlarge",
        model_name=f"deployed-model-{datetime_str}"
    )

    # Push to XCom
    ti.xcom_push(key='tuner_config', value=tuner_config)
    ti.xcom_push(key='transform_config', value=transform_config)
    ti.xcom_push(key='deploy_config', value=deploy_config)

def correct_configs(**kwargs):
    # Because of partial_fit, a new submit directory is created for each chunk.
    # This corrects the Jinja code found in the original to directly take the
    # 'sagemaker_submit_directory' from the tuning step instead of using
    # 'sagemaker_job_name'. Otherwise, a you get an error: 'An error occurred
    # (404) when calling the HeadObject operation: Not Found'.

    ti = kwargs["ti"]

    transform_config = ti.xcom_pull(task_ids='prepare_configs', key='transform_config')
    deploy_config = ti.xcom_pull(task_ids='prepare_configs', key='deploy_config')
    sagemaker_submit_directory = ti.xcom_pull(task_ids='model_tuning')['Tuning']['TrainingJobDefinition']['StaticHyperParameters']['sagemaker_submit_directory'].strip('\"')
    sagemaker_trainingjobname = ti.xcom_pull(task_ids='model_tuning')['Tuning']['BestTrainingJob']['TrainingJobName']
    S3OutputPath = ti.xcom_pull(task_ids='model_tuning')['Tuning']['TrainingJobDefinition']['OutputDataConfig']['S3OutputPath']

    transform_config["Model"]["PrimaryContainer"]["Environment"][
        "SAGEMAKER_SUBMIT_DIRECTORY"
    ] = sagemaker_submit_directory
    transform_config['Model']['PrimaryContainer']['ModelDataUrl'] = f'{S3OutputPath}{sagemaker_trainingjobname}/output/model.tar.gz'

    deploy_config["Model"]["PrimaryContainer"]["Environment"][
        "SAGEMAKER_SUBMIT_DIRECTORY"
    ] = sagemaker_submit_directory
    deploy_config['Model']['PrimaryContainer']['ModelDataUrl'] = f'{S3OutputPath}{sagemaker_trainingjobname}/output/model.tar.gz'

    # Push to XCom
    ti.xcom_push(key='transform_config', value=transform_config)
    ti.xcom_push(key='deploy_config', value=deploy_config)

# =============================================================================
# define airflow DAG and tasks
# =============================================================================

# define airflow DAG

with DAG(
    dag_id="sagemaker-ml-pipeline",
    schedule_interval=None,
    start_date=datetime(2021, 1, 1),
    tags=["mlops"],
    catchup=False,
) as dag:
    s3_files = S3ListOperator(
        task_id="list_3s_files",
        bucket=DATASET_BUCKET,
        prefix="raw/",
        do_xcom_push=True,
    )

    paths_string = PythonOperator(
        task_id="paths_string", python_callable=paths_to_string, provide_context=True,
    )

    create_app = EmrServerlessCreateApplicationOperator(
        task_id="create_spark_app",
        job_type="SPARK",
        release_label="emr-6.8.0",
        config={"name": "ml_preprocess"},
    )

    application_id = create_app.output

    spark_job = EmrServerlessStartJobOperator(
        task_id="preprocess_data",
        application_id=application_id,
        execution_role_arn=EMR_ROLE_ARN,
        job_driver={
            "sparkSubmit": {
                "entryPoint": f"s3://{S3_BUCKET}/pipeline/preprocess.py",
                "entryPointArguments": [
                    DATASET_BUCKET,
                    "{{ ti.xcom_pull(task_ids=['paths_string']) }}",
                ],
                "sparkSubmitParameters": (
                    f"--conf spark.archives=s3://{S3_BUCKET}/venv/pyspark_venv.tar.gz#environment "
                    "--conf spark.emr-serverless.driverEnv.PYSPARK_DRIVER_PYTHON=./environment/bin/python "
                    "--conf spark.emr-serverless.driverEnv.PYSPARK_PYTHON=./environment/bin/python "
                    "--conf spark.executorEnv.PYSPARK_PYTHON=./environment/bin/python"
                ),
            }
        },
        waiter_countdown=120 * 60,
        waiter_check_interval_seconds=2 * 60,
        configuration_overrides=DEFAULT_MONITORING_CONFIG,
    )

    delete_app = EmrServerlessDeleteApplicationOperator(
        task_id="delete_app",
        application_id=application_id,
        trigger_rule="all_done",
    )

    prepare_configs_task = PythonOperator(
        task_id='prepare_configs',
        python_callable=prepare_configs,
        op_args=[bucket_name],
        provide_context=True,
    )

    # launch sagemaker hyperparameter job and wait until it completes
    tune_model_task = SageMakerTuningOperator(
        task_id="model_tuning",
        config=prepare_configs_task.output['tuner_config'],
        aws_conn_id="airflow-sagemaker",
        wait_for_completion=True,
        check_interval=30,
    )

    correct_configs_task = PythonOperator(
        task_id='correct_configs',
        python_callable=correct_configs,
        provide_context=True,
    )

    # launch sagemaker batch transform job and wait until it completes
    batch_transform_task = SageMakerTransformOperator(
        task_id="predicting",
        config=correct_configs_task.output['transform_config'],
        aws_conn_id="airflow-sagemaker",
        wait_for_completion=True,
        check_interval=30,
    )

    evaluate_model_task = PythonOperator(
        task_id="evaluate_model_task",
        python_callable=evaluate_model,
        op_args=[bucket_name],
        provide_context=True,
    )

    branch_task = BranchPythonOperator(
        task_id="branch_task", python_callable=decide_next_task, provide_context=True,
    )

    deploy_endpoint_task = SageMakerEndpointOperator(
        task_id="deploy_sagemaker_endpoint",
        config=correct_configs_task.output['deploy_config'],
    )

    end_pipeline = DummyOperator(task_id="end_pipeline")

    # set the dependencies between tasks
    s3_files >> paths_string >> spark_job >> prepare_configs_task >> tune_model_task
    create_app >> spark_job >> delete_app
    tune_model_task >> correct_configs_task >> batch_transform_task >> evaluate_model_task
    evaluate_model_task >> branch_task
    branch_task >> deploy_endpoint_task >> end_pipeline
    branch_task >> end_pipeline
