from datetime import datetime
from typing import Any, Dict, Optional

import boto3
import config as cfg
import pandas as pd
import s3fs
import sagemaker
from sagemaker.sklearn.estimator import SKLearn
from sagemaker.tuner import (
    ContinuousParameter,
    HyperparameterTuner,
    TuningJobCompletionCriteriaConfig,
)
from sagemaker.workflow.airflow import (
    deploy_config_from_estimator,
    transform_config_from_estimator,
    tuning_config,
)
from sklearn.metrics import average_precision_score

from airflow.models import DAG
from airflow.operators.dummy_operator import DummyOperator
from airflow.operators.python_operator import BranchPythonOperator, PythonOperator
from airflow.providers.amazon.aws.hooks.s3 import S3Hook
from airflow.providers.amazon.aws.operators.emr import (
    EmrServerlessCreateApplicationOperator,
    EmrServerlessDeleteApplicationOperator,
    EmrServerlessStartJobOperator,
)
from airflow.providers.amazon.aws.operators.s3 import S3ListOperator
from airflow.providers.amazon.aws.operators.sagemaker import (
    SageMakerEndpointOperator,
    SageMakerTransformOperator,
    SageMakerTuningOperator,
)

# Define global variables for account ID, S3 buckets, and IAM roles.
ACCOUNT_ID = "<ACCOUNT_ID>"
S3_BUCKET = "airflow-coffeyjam"
DATASET_BUCKET = "datasets-20240228"
EMR_ROLE_ARN = f"arn:aws:iam::{ACCOUNT_ID}:role/EMRServerlessS3RuntimeRole"

# Default monitoring configuration for EMR Serverless Applications.
DEFAULT_MONITORING_CONFIG = {
    "monitoringConfiguration": {
        "s3MonitoringConfiguration": {"logUri": f"s3://{S3_BUCKET}/logs/"}
    },
}


def paths_to_string(**kwargs: Dict[str, Any]) -> str:
    """
    Constructs a comma-separated string of S3 paths for files ending with
    '.csv'.

    This function filters and constructs S3 paths for all files ending with
    '.csv' returned by an Airflow task, specifically from Airflow's XCom. It's
    designed to work within an Airflow DAG where the task instance (`ti`) is
    available in the context.

    Args:
        kwargs: Keyword arguments containing the Airflow task instance (`ti`)
                among other possible keys.

    Returns:
        A comma-separated string of S3 paths for each '.csv' file.

    Raises:
        KeyError: If 'ti' key is not found in kwargs, indicating the task
                  instance is missing from the context.

    Note:
        `DATASET_BUCKET` should be set to the actual S3 bucket name where the
        datasets are stored.
    """
    # Extract the Airflow task instance from keyword arguments
    ti = kwargs.get("ti")
    if ti is None:
        raise KeyError("Task instance ('ti') not found in function arguments.")

    # Pull the list of files from a previous task's XCom
    files = ti.xcom_pull(task_ids="list_3s_files")

    # Filter for '.csv' files and construct their S3 paths
    datasets = [
        f"s3://{DATASET_BUCKET}/{file}" for file in files if file.endswith(".csv")
    ]

    # Join the S3 paths into a comma-separated string
    return ",".join(datasets)


def get_sagemaker_role_arn(role_name: str, region_name: str) -> str:
    """
    Retrieves the Amazon Resource Name (ARN) for an AWS IAM role specified by
    its name.

    This function uses the AWS SDK for Python (Boto3) to access the AWS Identity
    and Access Management (IAM) service and fetch the ARN of an IAM role that is
    intended for use with AWS SageMaker services.

    Args:
        role_name: The name of the IAM role to retrieve the ARN for.
        region_name: The AWS region where the IAM role exists.

    Returns:
        The ARN of the specified IAM role as a string.

    Raises:
        ClientError: If the Boto3 IAM client encounters an error when trying to
                     fetch the role.
    """
    # Create a Boto3 client for the IAM service in the specified AWS region
    iam = boto3.client("iam", region_name=region_name)

    # Retrieve the IAM role information
    response = iam.get_role(RoleName=role_name)

    # Extract and return the ARN from the response
    return response["Role"]["Arn"]


def get_latest_csv_s3_path(bucket_name: str, prefix: str) -> Optional[str]:
    """
    Finds the latest CSV file within a specified S3 bucket and prefix.

    This function uses Airflow's S3Hook to list and filter objects in an S3
    bucket, identifying the most recently modified CSV file based on its
    'LastModified' timestamp. If no CSV files are found within the specified
    bucket and prefix, the function returns None.

    Args:
        bucket_name: The name of the S3 bucket to search in.
        prefix: The prefix path within the S3 bucket to filter the search.

    Returns:
        The S3 path of the most recently modified CSV file, or None if no CSV
        files are found.

    Note:
        This function relies on Airflow's S3Hook, which must be available in the
        Airflow environment where this function is executed.
    """
    # Initialize an S3Hook
    s3_hook = S3Hook()

    # List objects within the bucket and prefix
    list_objects = s3_hook.list_keys(
        bucket_name=bucket_name, prefix=prefix, delimiter="/"
    )

    # Filter out objects that are not CSV files
    csv_files = (
        [obj for obj in list_objects if obj.endswith(".csv")] if list_objects else []
    )

    # Initialize an empty list to hold (last_modified, key) tuples
    csv_files_details = []

    for csv_file in csv_files:
        # Get the object's metadata
        obj_metadata = s3_hook.get_key(key=csv_file, bucket_name=bucket_name)
        # Append a tuple of (last_modified, key) for each CSV file
        csv_files_details.append((obj_metadata.last_modified, csv_file))

    # Sort the list of tuples by the last_modified timestamp in descending order
    csv_files_details.sort(key=lambda x: x[0], reverse=True)

    # Get the most recent CSV file's key (if there are any CSV files)
    if csv_files_details:
        latest_csv_key = csv_files_details[0][1]
        # Construct the S3 path for the latest CSV file
        return f"s3://{bucket_name}/{latest_csv_key}"

    return None


def evaluate_model(bucket_name: str) -> bool:
    """
    Evaluates a machine learning model by comparing predictions to ground truth
    values.

    This function downloads a model's prediction results and the corresponding
    ground truth values from an S3 bucket, computes the average precision score
    of the predictions, and compares this score against a benchmark value stored
    in a text file within the same S3 bucket.

    Args:
        bucket_name: The name of the S3 bucket where the prediction results,
                     ground truth values, and benchmark are stored.

    Returns:
        A boolean value indicating whether the model's average precision score
        exceeds the benchmark.

    Note:
        - This function assumes the prediction results are stored in a file
          ending with `.out` in a 'predictions' directory within the specified
          S3 bucket.
        - The ground truth values are expected to be in a CSV format under a
          'groundtruth' directory.
        - The benchmark score is expected to be in a text file named
          'benchmark.txt' in a 'benchmark' directory.
    """
    s3 = boto3.client("s3")

    # Retrieve the filenames for the prediction results and ground truth from
    # the specified S3 bucket
    pred_filename = f"{get_latest_csv_s3_path(bucket_name, 'test/').split('/')[-1]}.out"
    groundtruth_filename = get_latest_csv_s3_path(bucket_name, "groundtruth/").split(
        "/"
    )[-1]

    # Download the prediction results file
    s3.download_file(bucket_name, f"predictions/{pred_filename}", pred_filename)

    # Read and process the prediction results file to obtain a Pandas Series of
    # predictions
    with open(pred_filename, "r") as file:
        file_content = file.read().strip()
    file_content = file_content[1:-1]
    file_content = file_content.split("][")
    file_content = ", ".join(file_content)
    file_content = file_content.split(", ")
    pred = pd.Series([float(i) for i in file_content])

    # Download the ground truth file and load it into a Pandas Series
    s3.download_file(
        bucket_name, f"groundtruth/{groundtruth_filename}", groundtruth_filename
    )
    y_true = pd.read_csv(groundtruth_filename, header=None, dtype="float").squeeze()

    # Calculate the average precision score for the predictions
    score = average_precision_score(y_true, pred)

    # Download the benchmark score and compare it with the model's score
    s3.download_file(bucket_name, "benchmark/benchmark.txt", "benchmark.txt")
    with open("benchmark.txt", "r") as file:
        benchmark = float(file.read().strip())

    # Return True if the model's score exceeds the benchmark, False otherwise
    return score > benchmark


def decide_next_task(**kwargs: Dict[str, Any]) -> str:
    """
    Determines the next task in a workflow based on the result of a model
    evaluation task.

    This function is intended to be used within an Apache Airflow DAG. It uses
    the Airflow task instance (`ti`) to pull the result from a previous task
    that evaluates a machine learning model. Depending on the evaluation result,
    this function will return the name of the next task to execute: deploying
    the model to a SageMaker endpoint if the evaluation was successful, or
    sending a failure notification otherwise.

    Args:
        kwargs: Keyword arguments containing the Airflow task instance (`ti`)
                among other possible keys.

    Returns:
        A string representing the name of the next task to execute. This can be
        either 'deploy_sagemaker_endpoint' if the model evaluation was
        successful, or 'send_failure_email' if the evaluation indicated a
        failure.

    Raises:
        KeyError: If 'ti' key is not found in kwargs, indicating the task
                  instance is missing from the context.

    Note:
        This function relies on the 'evaluate_model_task' to have been executed
        previously in the workflow, and its result to be pushed to Airflow's
        XCom.
    """
    # Extract the Airflow task instance from the provided keyword arguments
    ti = kwargs.get("ti")
    if ti is None:
        raise KeyError("Task instance ('ti') not found in function arguments.")

    # Pull the result of the model evaluation task using XCom
    result = ti.xcom_pull(task_ids="evaluate_model_task")

    # Decide the next task based on the evaluation result
    if result:
        return "deploy_sagemaker_endpoint"
    else:
        return "send_failure_email"


# =============================================================================
# setting up training, tuning and transform configuration
# =============================================================================

# Read the configuration file and set up SageMaker and S3 settings.
config = cfg.config
sess = sagemaker.Session(default_bucket="datasets-20240228")
# role = sagemaker.get_execution_role()
role = "arn:aws:iam::<ACCOUNT_ID>:role/service-role/AmazonSageMaker-ExecutionRole-20240224T215408"
region = sagemaker.Session().boto_region_name

bucket_name = "datasets-20240228"


def prepare_configs(bucket_name: str, **kwargs: Dict[str, Any]) -> None:
    """
    Prepares and pushes configurations for SageMaker training, tuning, and batch
    transform jobs to Airflow's XCom.

    This function gathers paths to training, validation, and test datasets from
    an S3 bucket, sets up hyperparameters, creates a SageMaker SKLearn
    estimator, and configures a hyperparameter tuner. It also prepares
    configurations for batch transformation and endpoint deployment, and pushes
    these configurations to Airflow's XCom for use in subsequent tasks.

    Args:
        bucket_name: Name of the S3 bucket containing the training, validation,
                     and test datasets.
        **kwargs: Keyword arguments containing the Airflow task instance ('ti').

    Raises:
        KeyError: If 'ti' key is not found in kwargs, indicating the task
                  instance is missing from the context.
    """
    # Extract the Airflow task instance from keyword arguments
    ti = kwargs.get("ti")
    if ti is None:
        raise KeyError("Task instance ('ti') not found in function arguments.")

    # Retrieve S3 paths for training, validation, and test datasets
    train_path = get_latest_csv_s3_path(bucket_name, "train/")
    validation_path = get_latest_csv_s3_path(bucket_name, "validation/")
    test_path = get_latest_csv_s3_path(bucket_name, "test/")

    # Read the first row of the training dataset to extract feature columns
    train_df = pd.read_csv(train_path, nrows=1)

    # Define hyperparameters for the training job
    hyperparameters = {
        "experiment_name": "malware-detection",
        "features": " ".join(
            list(train_df.drop(["test_set", "label"], axis=1).columns)
        ),
        "target": "label",
        "train-file": train_path.split("/")[-1],
        "validation-file": validation_path.split("/")[-1],
    }

    # Metric definitions for monitoring the training job
    metric_definitions = [
        {"Name": "avg-precision", "Regex": "avg-precision: ([0-9.]+).*$"}
    ]

    # Initialize a SageMaker SKLearn estimator for training
    estimator = SKLearn(
        role=role,  # 'role' should be defined globally or passed as an argument
        instance_count=1,
        instance_type="ml.m5.xlarge",
        source_dir="airflow/pipeline",
        entry_point="train.py",
        git_config={
            "repo": "https://github.com/JamesFCoffey/malware-detection-ml-pipeline.git"
        },
        hyperparameters=hyperparameters,
        metric_definitions=metric_definitions,
        framework_version="1.2-1",
        py_version="py3",
    )

    # Set up a hyperparameter tuner
    tuner = HyperparameterTuner(
        estimator=estimator,
        objective_metric_name="avg-precision",
        objective_type="Maximize",
        hyperparameter_ranges={
            "alpha": ContinuousParameter(0.00001, 0.001),
            "l1_ratio": ContinuousParameter(0.0, 1.0),
        },
        metric_definitions=[
            {"Name": "avg-precision", "Regex": "avg-precision: ([0-9.]+).*$"}
        ],
        max_jobs=2,
        max_parallel_jobs=2,
        completion_criteria_config=TuningJobCompletionCriteriaConfig(
            complete_on_convergence=True
        ),
    )

    # Create tuning config
    tuner_config = tuning_config(
        tuner=tuner, inputs={"train": train_path, "validation": validation_path}
    )

    # Create transform config
    transform_config = transform_config_from_estimator(
        estimator=estimator,
        task_id="model_tuning",
        task_type="tuning",
        instance_count=1,
        instance_type="ml.m5.xlarge",
        data=test_path,
        data_type="S3Prefix",
        content_type="text/csv",
        strategy="MultiRecord",
        output_path=f"s3://{bucket_name}/predictions",
        split_type="Line",
    )

    # Get the current date and time
    now = datetime.now()

    # Format the date and time as a string in the YYYYMMDD-HHMMSS format
    datetime_str = now.strftime("%Y%m%d-%H%M%S")

    # Create endpoint deployment config
    deploy_config = deploy_config_from_estimator(
        estimator=estimator,
        task_id="model_tuning",
        task_type="tuning",
        initial_instance_count=1,
        instance_type="ml.m5.xlarge",
        model_name=f"deployed-model-{datetime_str}",
    )

    # Push configurations to Airflow's XCom
    ti.xcom_push(key="tuner_config", value=tuner_config)
    ti.xcom_push(key="transform_config", value=transform_config)
    ti.xcom_push(key="deploy_config", value=deploy_config)


def correct_configs(**kwargs: Dict[str, Any]) -> None:
    """
    Updates the SageMaker transform and deploy configurations to prevent a 404
    error.

    This function addresses an issue where a new S3 submission directory is
    created for each chunk of data during the model training process, leading to
    a mismatch in the expected S3 path in subsequent SageMaker operations. By
    directly setting the 'sagemaker_submit_directory' and 'ModelDataUrl' from
    the model tuning step, this function ensures the correct S3 paths are used,
    avoiding a '404: Not Found' error when SageMaker attempts to access the
    model artifacts.

    Args:
        **kwargs: Keyword arguments containing the Airflow task instance (`ti`).

    Note:
        This function must be called after the model tuning task and before any
        tasks that require accessing the trained model artifacts, such as model
        transformation or endpoint deployment tasks.
    """
    # Extract the Airflow task instance from the provided keyword arguments
    ti = kwargs.get("ti")
    if ti is None:
        raise KeyError("Task instance ('ti') not found in function arguments.")

    # Retrieve the transformation and deployment configurations from XCom
    transform_config = ti.xcom_pull(task_ids="prepare_configs", key="transform_config")
    deploy_config = ti.xcom_pull(task_ids="prepare_configs", key="deploy_config")

    # Extract necessary information from the model tuning task
    sagemaker_submit_directory = ti.xcom_pull(task_ids="model_tuning")["Tuning"][
        "TrainingJobDefinition"
    ]["StaticHyperParameters"]["sagemaker_submit_directory"].strip('"')
    sagemaker_trainingjobname = ti.xcom_pull(task_ids="model_tuning")["Tuning"][
        "BestTrainingJob"
    ]["TrainingJobName"]
    S3OutputPath = ti.xcom_pull(task_ids="model_tuning")["Tuning"][
        "TrainingJobDefinition"
    ]["OutputDataConfig"]["S3OutputPath"]

    # Update the transformation and deployment configurations with the correct
    # S3 paths
    transform_config["Model"]["PrimaryContainer"]["Environment"][
        "SAGEMAKER_SUBMIT_DIRECTORY"
    ] = sagemaker_submit_directory
    transform_config["Model"]["PrimaryContainer"]["ModelDataUrl"] = (
        f"{S3OutputPath}{sagemaker_trainingjobname}/output/model.tar.gz"
    )

    deploy_config["Model"]["PrimaryContainer"]["Environment"][
        "SAGEMAKER_SUBMIT_DIRECTORY"
    ] = sagemaker_submit_directory
    deploy_config["Model"]["PrimaryContainer"]["ModelDataUrl"] = (
        f"{S3OutputPath}{sagemaker_trainingjobname}/output/model.tar.gz"
    )

    # Push the updated configurations back to XCom for use in subsequent tasks
    ti.xcom_push(key="transform_config", value=transform_config)
    ti.xcom_push(key="deploy_config", value=deploy_config)


# Define the DAG and set its properties.
with DAG(
    dag_id="sagemaker-ml-pipeline",
    schedule_interval=None,
    start_date=datetime(2021, 1, 1),
    tags=["mlops"],
    catchup=False,
) as dag:
    s3_files = S3ListOperator(
        task_id="list_3s_files",
        bucket=DATASET_BUCKET,
        prefix="raw/",
        do_xcom_push=True,
    )

    paths_string = PythonOperator(
        task_id="paths_string",
        python_callable=paths_to_string,
        provide_context=True,
    )

    create_app = EmrServerlessCreateApplicationOperator(
        task_id="create_spark_app",
        job_type="SPARK",
        release_label="emr-6.8.0",
        config={"name": "ml_preprocess"},
    )

    application_id = create_app.output

    spark_job = EmrServerlessStartJobOperator(
        task_id="preprocess_data",
        application_id=application_id,
        execution_role_arn=EMR_ROLE_ARN,
        job_driver={
            "sparkSubmit": {
                "entryPoint": f"s3://{S3_BUCKET}/pipeline/preprocess.py",
                "entryPointArguments": [
                    DATASET_BUCKET,
                    "{{ ti.xcom_pull(task_ids=['paths_string']) }}",
                ],
                "sparkSubmitParameters": (
                    f"--conf spark.archives=s3://{S3_BUCKET}/venv/pyspark_venv.tar.gz#environment "
                    "--conf spark.emr-serverless.driverEnv.PYSPARK_DRIVER_PYTHON=./environment/bin/python "
                    "--conf spark.emr-serverless.driverEnv.PYSPARK_PYTHON=./environment/bin/python "
                    "--conf spark.executorEnv.PYSPARK_PYTHON=./environment/bin/python"
                ),
            }
        },
        waiter_countdown=120 * 60,
        waiter_check_interval_seconds=2 * 60,
        configuration_overrides=DEFAULT_MONITORING_CONFIG,
    )

    delete_app = EmrServerlessDeleteApplicationOperator(
        task_id="delete_app",
        application_id=application_id,
        trigger_rule="all_done",
    )

    prepare_configs_task = PythonOperator(
        task_id="prepare_configs",
        python_callable=prepare_configs,
        op_args=[bucket_name],
        provide_context=True,
    )

    # Launch sagemaker hyperparameter job and wait until it completes
    tune_model_task = SageMakerTuningOperator(
        task_id="model_tuning",
        config=prepare_configs_task.output["tuner_config"],
        aws_conn_id="airflow-sagemaker",
        wait_for_completion=True,
        check_interval=30,
    )

    correct_configs_task = PythonOperator(
        task_id="correct_configs",
        python_callable=correct_configs,
        provide_context=True,
    )

    # Launch sagemaker batch transform job and wait until it completes
    batch_transform_task = SageMakerTransformOperator(
        task_id="predicting",
        config=correct_configs_task.output["transform_config"],
        aws_conn_id="airflow-sagemaker",
        wait_for_completion=True,
        check_interval=30,
    )

    evaluate_model_task = PythonOperator(
        task_id="evaluate_model_task",
        python_callable=evaluate_model,
        op_args=[bucket_name],
        provide_context=True,
    )

    branch_task = BranchPythonOperator(
        task_id="branch_task",
        python_callable=decide_next_task,
        provide_context=True,
    )

    deploy_endpoint_task = SageMakerEndpointOperator(
        task_id="deploy_sagemaker_endpoint",
        config=correct_configs_task.output["deploy_config"],
    )

    end_pipeline = DummyOperator(task_id="end_pipeline")

    # Set the task dependencies.
    s3_files >> paths_string >> spark_job >> prepare_configs_task >> tune_model_task
    create_app >> spark_job >> delete_app
    (
        tune_model_task
        >> correct_configs_task
        >> batch_transform_task
        >> evaluate_model_task
    )
    evaluate_model_task >> branch_task
    branch_task >> deploy_endpoint_task >> end_pipeline
    branch_task >> end_pipeline
