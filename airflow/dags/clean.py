from datetime import datetime

# airflow operators
from airflow import DAG
from airflow.providers.amazon.aws.operators.emr import (
    EmrServerlessCreateApplicationOperator,
    EmrServerlessStartJobOperator,
    EmrServerlessDeleteApplicationOperator,
)
from airflow.providers.amazon.aws.operators.s3 import S3ListOperator
from airflow.operators.python import PythonOperator

ACCOUNT_ID = '<ACCOUNT_ID>'
S3_BUCKET = 'airflow-coffeyjam'
DATASET_BUCKET = 'datasets-20240228'
EMR_ROLE_ARN = f'arn:aws:iam::{ACCOUNT_ID}:role/EMRServerlessS3RuntimeRole'

DEFAULT_MONITORING_CONFIG = {
    'monitoringConfiguration': {
        's3MonitoringConfiguration': {'logUri': f's3://{S3_BUCKET}/logs/'}
    },
}

def paths_to_string(ti):
   files = ti.xcom_pull(task_ids='list_3s_files')
   datasets = []
   for file in files:
       if file.endswith('.csv'):
            datasets.append('s3://' + DATASET_BUCKET + '/' + file)
   return ','.join(datasets)

with DAG(
    dag_id='endtoend_ml_pipeline',
    schedule_interval=None,
    start_date=datetime(2021, 1, 1),
    tags=['mlops'],
    catchup=False,
) as dag:
    s3_files = S3ListOperator(
        task_id='list_3s_files',
        bucket=DATASET_BUCKET,
        prefix='raw/',
        do_xcom_push=True,
    )

    paths_string = PythonOperator(
        task_id='paths_string',
        python_callable=paths_to_string
    )
    
    create_app = EmrServerlessCreateApplicationOperator(
        task_id='create_spark_app',
        job_type='SPARK',
        release_label='emr-6.8.0',
        config={'name': 'ml_preprocess'},
    )

    application_id = create_app.output

    spark_job = EmrServerlessStartJobOperator(
        task_id='preprocess_data',
        application_id=application_id,
        execution_role_arn=EMR_ROLE_ARN,
        job_driver={
            'sparkSubmit': {
                'entryPoint': f's3://{S3_BUCKET}/pipeline/preprocess.py',
                'entryPointArguments': [DATASET_BUCKET, "{{ ti.xcom_pull(task_ids=[\'paths_string\']) }}"],
                'sparkSubmitParameters': (
                f'--conf spark.archives=s3://{S3_BUCKET}/venv/pyspark_venv.tar.gz#environment '
                '--conf spark.emr-serverless.driverEnv.PYSPARK_DRIVER_PYTHON=./environment/bin/python '
                '--conf spark.emr-serverless.driverEnv.PYSPARK_PYTHON=./environment/bin/python '
                '--conf spark.executorEnv.PYSPARK_PYTHON=./environment/bin/python'
                )
            }
        },
        waiter_countdown = 90 * 60,
        configuration_overrides=DEFAULT_MONITORING_CONFIG,
    )

    delete_app = EmrServerlessDeleteApplicationOperator(
        task_id='delete_app',
        application_id=application_id,
        trigger_rule='all_done',
    )

    s3_files >> paths_string >> spark_job
    create_app >> spark_job >> delete_app