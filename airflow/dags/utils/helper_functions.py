from datetime import datetime
from typing import Any, Dict, Optional

import boto3
import pandas as pd
import s3fs
from sagemaker.sklearn.estimator import SKLearn
from sagemaker.tuner import (
    ContinuousParameter,
    HyperparameterTuner,
    TuningJobCompletionCriteriaConfig,
)
from sagemaker.workflow.airflow import (
    deploy_config_from_estimator,
    transform_config_from_estimator,
    tuning_config,
)
from sklearn.metrics import average_precision_score

from airflow.providers.amazon.aws.hooks.s3 import S3Hook


def paths_to_string(datasets_bucket: str, **kwargs: Dict[str, Any]) -> str:
    """
    Constructs a comma-separated string of S3 paths for files ending with
    '.csv'.

    This function uses an Airflow task instance (`ti`) to pull the list of file
    names from Airflow's XCom and filters for those ending in '.csv'. It
    constructs a full S3 path for each of these files using the provided S3
    bucket name. It is designed to work within an Airflow DAG where the task
    instance (`ti`) is available in the context.

    Args:
        datasets_bucket: The name of the S3 bucket where the files are stored.
        **kwargs: Keyword arguments that must include the Airflow task instance
                  (`ti`).

    Returns:
        A comma-separated string of complete S3 paths for each '.csv' file
        found.

    Raises:
        KeyError: If the 'ti' key is not found in kwargs, indicating that the
                  task instance is missing from the context.

    Notes:
        - This function is typically used in scenarios where file paths need to
          be dynamically passed to other tasks based on the outputs of previous
          tasks in an Airflow DAG.
        - Ensure that the 'datasets_bucket' accurately reflects the target S3
          bucket to avoid errors in path resolution.
    """
    # Extract the Airflow task instance from keyword arguments
    ti = kwargs.get("ti")
    if ti is None:
        raise KeyError("Task instance ('ti') not found in function arguments.")

    # Pull the list of files from a previous task's XCom
    files = ti.xcom_pull(task_ids="list_3s_files")

    # Filter for '.csv' files and construct their S3 paths
    datasets = [
        f"s3://{datasets_bucket}/{file}" for file in files if file.endswith(".csv")
    ]

    # Join the S3 paths into a comma-separated string
    return ",".join(datasets)


def get_latest_csv_s3_path(bucket_name: str, prefix: str) -> Optional[str]:
    """
    Finds the latest CSV file within a specified S3 bucket and prefix.

    This function uses Airflow's S3Hook to list and filter objects in an S3
    bucket, identifying the most recently modified CSV file based on its
    'LastModified' timestamp. If no CSV files are found within the specified
    bucket and prefix, the function returns None.

    Args:
        bucket_name: The name of the S3 bucket to search in.
        prefix: The prefix path within the S3 bucket to filter the search.

    Returns:
        The S3 path of the most recently modified CSV file, or None if no CSV
        files are found.

    Note:
        This function relies on Airflow's S3Hook, which must be available in the
        Airflow environment where this function is executed.
    """
    # Initialize an S3Hook
    s3_hook = S3Hook()

    # List objects within the bucket and prefix
    list_objects = s3_hook.list_keys(
        bucket_name=bucket_name, prefix=prefix, delimiter="/"
    )

    # Filter out objects that are not CSV files
    csv_files = (
        [obj for obj in list_objects if obj.endswith(".csv")] if list_objects else []
    )

    # Initialize an empty list to hold (last_modified, key) tuples
    csv_files_details = []

    for csv_file in csv_files:
        # Get the object's metadata
        obj_metadata = s3_hook.get_key(key=csv_file, bucket_name=bucket_name)
        # Append a tuple of (last_modified, key) for each CSV file
        csv_files_details.append((obj_metadata.last_modified, csv_file))

    # Sort the list of tuples by the last_modified timestamp in descending order
    csv_files_details.sort(key=lambda x: x[0], reverse=True)

    # Get the most recent CSV file's key (if there are any CSV files)
    if csv_files_details:
        latest_csv_key = csv_files_details[0][1]
        # Construct the S3 path for the latest CSV file
        return f"s3://{bucket_name}/{latest_csv_key}"

    return None


def evaluate_model(bucket_name: str) -> bool:
    """
    Evaluates a machine learning model by comparing predictions to ground truth
    values.

    This function downloads a model's prediction results and the corresponding
    ground truth values from an S3 bucket, computes the average precision score
    of the predictions, and compares this score against a benchmark value stored
    in a text file within the same S3 bucket.

    Args:
        bucket_name: The name of the S3 bucket where the prediction results,
                     ground truth values, and benchmark are stored.

    Returns:
        A boolean value indicating whether the model's average precision score
        exceeds the benchmark.

    Note:
        - This function assumes the prediction results are stored in a file
          ending with `.out` in a 'predictions' directory within the specified
          S3 bucket.
        - The ground truth values are expected to be in a CSV format under a
          'groundtruth' directory.
        - The benchmark score is expected to be in a text file named
          'benchmark.txt' in a 'benchmark' directory.
    """
    s3 = boto3.client("s3")

    # Retrieve the filenames for the prediction results and ground truth from
    # the specified S3 bucket
    pred_filename = f"{get_latest_csv_s3_path(bucket_name, 'test/').split('/')[-1]}.out"
    groundtruth_filename = get_latest_csv_s3_path(bucket_name, "groundtruth/").split(
        "/"
    )[-1]

    # Download the prediction results file
    s3.download_file(bucket_name, f"predictions/{pred_filename}", pred_filename)

    # Read and process the prediction results file to obtain a Pandas Series of
    # predictions
    with open(pred_filename, "r") as file:
        file_content = file.read().strip()
    file_content = file_content[1:-1]
    file_content = file_content.split("][")
    file_content = ", ".join(file_content)
    file_content = file_content.split(", ")
    pred = pd.Series([float(i) for i in file_content])

    # Download the ground truth file and load it into a Pandas Series
    s3.download_file(
        bucket_name, f"groundtruth/{groundtruth_filename}", groundtruth_filename
    )
    y_true = pd.read_csv(groundtruth_filename, header=None, dtype="float").squeeze()

    # Calculate the average precision score for the predictions
    score = average_precision_score(y_true, pred)

    # Download the benchmark score and compare it with the model's score
    s3.download_file(bucket_name, "benchmark/benchmark.txt", "benchmark.txt")
    with open("benchmark.txt", "r") as file:
        benchmark = float(file.read().strip())

    # Return True if the model's score exceeds the benchmark, False otherwise
    return score > benchmark


def decide_next_task(**kwargs: Dict[str, Any]) -> str:
    """
    Determines the next task in a workflow based on the result of a model
    evaluation task.

    This function is intended to be used within an Apache Airflow DAG. It uses
    the Airflow task instance (`ti`) to pull the result from a previous task
    that evaluates a machine learning model. Depending on the evaluation result,
    this function will return the name of the next task to execute: deploying
    the model to a SageMaker endpoint if the evaluation was successful, or
    sending a failure notification otherwise.

    Args:
        kwargs: Keyword arguments containing the Airflow task instance (`ti`)
                among other possible keys.

    Returns:
        A string representing the name of the next task to execute. This can be
        either 'deploy_sagemaker_endpoint' if the model evaluation was
        successful, or 'send_failure_email' if the evaluation indicated a
        failure.

    Raises:
        KeyError: If 'ti' key is not found in kwargs, indicating the task
                  instance is missing from the context.

    Note:
        This function relies on the 'evaluate_model_task' to have been executed
        previously in the workflow, and its result to be pushed to Airflow's
        XCom.
    """
    # Extract the Airflow task instance from the provided keyword arguments
    ti = kwargs.get("ti")
    if ti is None:
        raise KeyError("Task instance ('ti') not found in function arguments.")

    # Pull the result of the model evaluation task using XCom
    result = ti.xcom_pull(task_ids="evaluate_model_task")

    # Decide the next task based on the evaluation result
    if result:
        return "deploy_sagemaker_endpoint"
    else:
        return "send_failure_email"


def prepare_configs(bucket_name: str, role: str, **kwargs: Dict[str, Any]) -> None:
    """
    Prepares and pushes configurations for SageMaker training, tuning, and batch
    transform jobs to Airflow's XCom.

    This function gathers paths to training, validation, and test datasets from
    an S3 bucket, sets up hyperparameters, creates a SageMaker SKLearn
    estimator, and configures a hyperparameter tuner. It also prepares
    configurations for batch transformation and endpoint deployment, and pushes
    these configurations to Airflow's XCom for use in subsequent tasks.

    Args:
        bucket_name: Name of the S3 bucket containing the training, validation,
                     and test datasets.
        role: AWS IAM role string used by SageMaker to access AWS resources.
        **kwargs: Keyword arguments containing the Airflow task instance ('ti').

    Raises:
        KeyError: If 'ti' key is not found in kwargs, indicating the task
                  instance is missing from the context.
    """
    # Extract the Airflow task instance from keyword arguments
    ti = kwargs.get("ti")
    if ti is None:
        raise KeyError("Task instance ('ti') not found in function arguments.")

    # Retrieve S3 paths for training, validation, and test datasets
    train_path = get_latest_csv_s3_path(bucket_name, "train/")
    validation_path = get_latest_csv_s3_path(bucket_name, "validation/")
    test_path = get_latest_csv_s3_path(bucket_name, "test/")

    # Read the first row of the training dataset to extract feature columns
    train_df = pd.read_csv(train_path, nrows=1)

    # Define hyperparameters for the training job
    hyperparameters = {
        "experiment_name": "malware-detection",
        "features": " ".join(
            list(train_df.drop(["test_set", "label"], axis=1).columns)
        ),
        "target": "label",
        "train-file": train_path.split("/")[-1],
        "validation-file": validation_path.split("/")[-1],
    }

    # Metric definitions for monitoring the training job
    metric_definitions = [
        {"Name": "avg-precision", "Regex": "avg-precision: ([0-9.]+).*$"}
    ]

    # Initialize a SageMaker SKLearn estimator for training
    estimator = SKLearn(
        role=role,  # 'role' should be defined globally or passed as an argument
        instance_count=1,
        instance_type="ml.m5.xlarge",
        source_dir="airflow/pipeline",
        entry_point="train.py",
        git_config={
            "repo": "https://github.com/JamesFCoffey/malware-detection-ml-pipeline.git"
        },
        hyperparameters=hyperparameters,
        metric_definitions=metric_definitions,
        framework_version="1.2-1",
        py_version="py3",
    )

    # Set up a hyperparameter tuner
    tuner = HyperparameterTuner(
        estimator=estimator,
        objective_metric_name="avg-precision",
        objective_type="Maximize",
        hyperparameter_ranges={
            "alpha": ContinuousParameter(0.00001, 0.001),
            "l1_ratio": ContinuousParameter(0.0, 1.0),
        },
        metric_definitions=[
            {"Name": "avg-precision", "Regex": "avg-precision: ([0-9.]+).*$"}
        ],
        max_jobs=2,
        max_parallel_jobs=2,
        completion_criteria_config=TuningJobCompletionCriteriaConfig(
            complete_on_convergence=True
        ),
    )

    # Create tuning config
    tuner_config = tuning_config(
        tuner=tuner, inputs={"train": train_path, "validation": validation_path}
    )

    # Create transform config
    transform_config = transform_config_from_estimator(
        estimator=estimator,
        task_id="model_tuning",
        task_type="tuning",
        instance_count=1,
        instance_type="ml.m5.xlarge",
        data=test_path,
        data_type="S3Prefix",
        content_type="text/csv",
        strategy="MultiRecord",
        output_path=f"s3://{bucket_name}/predictions",
        split_type="Line",
    )

    # Get the current date and time
    now = datetime.now()

    # Format the date and time as a string in the YYYYMMDD-HHMMSS format
    datetime_str = now.strftime("%Y%m%d-%H%M%S")

    # Create endpoint deployment config
    deploy_config = deploy_config_from_estimator(
        estimator=estimator,
        task_id="model_tuning",
        task_type="tuning",
        initial_instance_count=1,
        instance_type="ml.m5.xlarge",
        model_name=f"deployed-model-{datetime_str}",
    )

    # Push configurations to Airflow's XCom
    ti.xcom_push(key="tuner_config", value=tuner_config)
    ti.xcom_push(key="transform_config", value=transform_config)
    ti.xcom_push(key="deploy_config", value=deploy_config)


def correct_configs(**kwargs: Dict[str, Any]) -> None:
    """
    Updates the SageMaker transform and deploy configurations to prevent a 404
    error.

    This function addresses an issue where a new S3 submission directory is
    created for each chunk of data during the model training process, leading to
    a mismatch in the expected S3 path in subsequent SageMaker operations. By
    directly setting the 'sagemaker_submit_directory' and 'ModelDataUrl' from
    the model tuning step, this function ensures the correct S3 paths are used,
    avoiding a '404: Not Found' error when SageMaker attempts to access the
    model artifacts.

    Args:
        **kwargs: Keyword arguments containing the Airflow task instance (`ti`).

    Note:
        This function must be called after the model tuning task and before any
        tasks that require accessing the trained model artifacts, such as model
        transformation or endpoint deployment tasks.
    """
    # Extract the Airflow task instance from the provided keyword arguments
    ti = kwargs.get("ti")
    if ti is None:
        raise KeyError("Task instance ('ti') not found in function arguments.")

    # Retrieve the transformation and deployment configurations from XCom
    transform_config = ti.xcom_pull(task_ids="prepare_configs", key="transform_config")
    deploy_config = ti.xcom_pull(task_ids="prepare_configs", key="deploy_config")

    # Extract necessary information from the model tuning task
    sagemaker_submit_directory = ti.xcom_pull(task_ids="model_tuning")["Tuning"][
        "TrainingJobDefinition"
    ]["StaticHyperParameters"]["sagemaker_submit_directory"].strip('"')
    sagemaker_trainingjobname = ti.xcom_pull(task_ids="model_tuning")["Tuning"][
        "BestTrainingJob"
    ]["TrainingJobName"]
    S3OutputPath = ti.xcom_pull(task_ids="model_tuning")["Tuning"][
        "TrainingJobDefinition"
    ]["OutputDataConfig"]["S3OutputPath"]

    # Update the transformation and deployment configurations with the correct
    # S3 paths
    transform_config["Model"]["PrimaryContainer"]["Environment"][
        "SAGEMAKER_SUBMIT_DIRECTORY"
    ] = sagemaker_submit_directory
    transform_config["Model"]["PrimaryContainer"]["ModelDataUrl"] = (
        f"{S3OutputPath}{sagemaker_trainingjobname}/output/model.tar.gz"
    )

    deploy_config["Model"]["PrimaryContainer"]["Environment"][
        "SAGEMAKER_SUBMIT_DIRECTORY"
    ] = sagemaker_submit_directory
    deploy_config["Model"]["PrimaryContainer"]["ModelDataUrl"] = (
        f"{S3OutputPath}{sagemaker_trainingjobname}/output/model.tar.gz"
    )

    # Push the updated configurations back to XCom for use in subsequent tasks
    ti.xcom_push(key="transform_config", value=transform_config)
    ti.xcom_push(key="deploy_config", value=deploy_config)
