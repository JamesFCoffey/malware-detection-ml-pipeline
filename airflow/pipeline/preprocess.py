import sys  # Access system-specific parameters and functions

import pyspark.sql.functions as F  # Import PySpark functions module as F for convenience
from pyspark.ml.feature import (
    StringIndexer,
)
from pyspark.sql import (
    DataFrame,
    SparkSession,
    Window,  # For defining window specifications
)

# Import various functions from pyspark.sql.functions
from pyspark.sql.functions import (
    avg,
    col,
    count,
    from_unixtime,
    max,
    min,
    to_timestamp,
)


def window_ops(
    column: str,
    operation: str = "count",
    time: str = "datetime",
    minutes: int = 1,
    partition_by: str = None,
) -> F.Column:
    """Performs a rolling aggregation (count or average) on a specified column
    over a time-based window.

    This function applies a rolling aggregation defined by `operation` on the
    `column` within a window of `minutes` prior to each timestamp in the `time`
    column. The data can be optionally partitioned by another column using
    `partition_by`.

    Args:
        column: The name of the column to perform the aggregation on.
        operation: The type of aggregation operation to perform. Can be 'count'
                   or 'avg'. Defaults to 'count'.
        time: The name of the column containing timestamp data. Defaults to
              'datetime'.
        minutes: The size of the rolling window in minutes. Defaults to 1
                 minute.
        partition_by: Optional; the name of the column to partition the data by
                      before applying the rolling aggregation. If not provided,
                      the data is partitioned by `column`.

    Returns:
        A PySpark Column object representing the rolling aggregate for each row
        in the DataFrame.

    Raises:
        ValueError: If the `operation` specified is not supported ('count' or
        'avg').
    """

    # Default partition_by to the target column if not specified
    if partition_by is None:
        partition_by = column

    # Define the window specification for rolling aggregation
    window = (
        Window.partitionBy(F.col(partition_by))
        .orderBy(
            F.col(time).cast("long")
        )  # Casting time column to long type for time-based operations
        .rangeBetween(
            -60 * minutes, -1
        )  # Defines the bounds of the window relative to the current row's timestamp
    )

    # Map operation names to their corresponding PySpark SQL functions
    operations = {
        "count": F.count(column).over(window),
        "avg": F.avg(column).over(window),
    }

    # Return the operation's result if it's defined, else raise an error
    if operation in operations:
        return operations[operation]
    else:
        raise ValueError(f"{operation} not defined")


def oneHot(df: DataFrame, column: str) -> dict:
    """Performs one-hot encoding on a specified column of a DataFrame.

    This function creates a dictionary of new columns representing the one-hot
    encoded values of the distinct entries in the specified column. The first
    distinct value encountered is skipped to avoid the dummy variable trap.

    Args:
        df: The PySpark DataFrame containing the column to be one-hot encoded.
        column: The name of the column to be one-hot encoded.

    Returns:
        A dictionary where keys are new column names for the one-hot encoded
        values and values are the corresponding PySpark Column expressions. Each
        key is named as '<column>_<value>', where <value> is a distinct value
        from the specified column, excluding the first distinct value
        encountered.

    Note:
        This function does not modify the original DataFrame `df`. Instead, it
        returns a dictionary of PySpark Column expressions that can be used to
        augment `df` with one-hot encoded columns.
    """

    # Retrieve distinct values of the specified column and convert to a list
    values = df.select(column).distinct().toPandas()[column].tolist()
    values.sort()  # Sort the values for consistent ordering

    # Remove the first value to avoid the dummy variable trap in one-hot
    # encoding
    values.pop(0)

    # Initialize an empty dictionary to hold the new one-hot encoded column
    # expressions
    new_columns = {}

    # Iterate over the remaining distinct values to create new one-hot encoded
    # columns
    for value in values:
        # For each value, create a new column name in the format
        # '<column>_<value>' The column expression assigns 1.0 where the
        # original column's value matches the current distinct value, and 0.0
        # otherwise.
        new_columns[f"{column}_{value}"] = F.when(col(column) == value, 1.0).otherwise(
            0.0
        )

    # Return the dictionary of new one-hot encoded column expressions
    return new_columns


def main():
    """Main function to preprocess data using PySpark."""

    # Ensure there's at least one argument provided
    if len(sys.argv) < 2:
        print("Usage: python preprocess.py <s3_bucket_name> <list_of_s3_file_paths>")
        sys.exit(1)  # Exit with error if not

    # The second element in sys.argv is the first argument
    s3_bucket_name = sys.argv[1]

    # Initialize Spark session
    spark = SparkSession.builder.appName("ml_preprocess").getOrCreate()
    spark.sparkContext.setLogLevel(
        "ERROR"
    )  # Set log level to ERROR to reduce verbosity

    s3_files = sys.argv[2]  # Get the list of S3 file paths
    datasets = (
        s3_files.replace("['", "").replace("']", "").split(",")
    )  # Clean and split the file paths

    # Read CSV files into a Spark DataFrame
    df = spark.read.option("delimiter", "|").csv(
        datasets, inferSchema=True, header=True
    )

    # Ensure feature names have clear and obvious meaning
    df = df.toDF(
        "unix_timestamp",
        "unique_id",
        "source_ip",
        "source_port",
        "dest_ip",
        "dest_port",
        "proto",
        "service",
        "duration",
        "source_bytes",
        "dest_bytes",
        "conn_state",
        "local_source",
        "local_dest",
        "missed_bytes",
        "history",
        "source_pkts",
        "source_ip_bytes",
        "dest_pkts",
        "dest_ip_bytes",
        "tunnel_parents",
        "label",
        "detailed-label",
    )

    # Convert unix timestamp to datetime
    df = df.withColumn("datetime", from_unixtime("unix_timestamp")).withColumn(
        "datetime", to_timestamp("datetime")
    )
    df = df.drop("unix_timestamp")  # Drop the original unix timestamp column

    # Ensure missing data is properly represented
    df = df.replace("-", None)

    # Drop duplicates
    df = df.dropDuplicates()

    # Drop empty columns
    empty_columns = ["local_source", "local_dest", "tunnel_parents"]
    df = df.drop(*empty_columns)

    # Drop unnecessary columns
    unnecessary_columns = ["unique_id", "detailed-label"]
    df = df.drop(*unnecessary_columns)

    # Convert columns to appropriate data types
    df = df.withColumns(
        {
            "source_ip": col("source_ip").cast("string"),
            "source_port": col("source_port").cast("string"),
            "dest_ip": col("dest_ip").cast("string"),
            "dest_port": col("dest_port").cast("string"),
            "proto": col("proto").cast("string"),
            "service": col("service").cast("string"),
            "duration": col("duration").cast("double"),
            "source_bytes": col("source_bytes").cast("double"),
            "dest_bytes": col("dest_bytes").cast("double"),
            "conn_state": col("conn_state").cast("string"),
            "missed_bytes": col("missed_bytes").cast("double"),
            "history": col("history").cast("string"),
            "source_pkts": col("source_pkts").cast("double"),
            "source_ip_bytes": col("source_ip_bytes").cast("double"),
            "dest_pkts": col("dest_pkts").cast("double"),
            "dest_ip_bytes": col("dest_ip_bytes").cast("double"),
            "label": col("label").cast("string"),
        }
    )

    # Handle mising numeric data and add indicator features
    df = df.withColumns(
        {
            "is_duration_defined": F.when(
                F.isnan("duration") | col("duration").isNull(), 0.0
            ).otherwise(1.0),
            "is_source_bytes_defined": F.when(
                F.isnan("source_bytes") | col("source_bytes").isNull(), 0.0
            ).otherwise(1.0),
            "is_dest_bytes_defined": F.when(
                F.isnan("dest_bytes") | col("dest_bytes").isNull(), 0.0
            ).otherwise(1.0),
        }
    )

    # Fill in missing numeric data with the mean
    df = df.fillna(
        {
            "duration": df.select(avg("duration")).first()[0],
            "source_bytes": df.select(avg("source_bytes")).first()[0],
            "dest_bytes": df.select(avg("dest_bytes")).first()[0],
        }
    )

    # Handle missing categorical data
    # Add indicator features and set missing values to 'missing'
    df = df.withColumns(
        {
            "is_service_defined": F.when(
                F.isnan("service") | col("service").isNull(), 0.0
            ).otherwise(1.0),
            "is_history_defined": F.when(
                F.isnan("history") | col("history").isNull(), 0.0
            ).otherwise(1.0),
        }
    ).fillna({"service": "missing", "history": "missing"})

    # Set categorical values that occur less than 5 times to 'other'
    set_as_other = (
        df.groupBy("history").count().where("count < 5").toPandas()["history"].tolist()
    )
    df = df.withColumn(
        "history",
        F.when(F.col("history").isin(set_as_other), "other").otherwise(col("history")),
    )

    # Select only data labeled either 'Benign' or 'Malicious'
    df = df.where("label in ('Benign', 'Malicious')")

    # Create two new features showing number of visits in 1m and 30m sliding
    # windows
    df = df.withColumns(
        {
            "visits_src_ip_1m": window_ops("source_ip", minutes=1),
            "visits_src_ip_30m": window_ops("source_ip", minutes=30),
        }
    )

    # Drop unnecessary features
    df = df.drop("dest_ip", "datetime")

    # Index categorical features to integers
    indexer = StringIndexer(
        inputCols=["proto", "service", "conn_state", "history"],
        outputCols=["protoIdx", "serviceIdx", "conn_stateIdx", "historyIdx"],
    )
    df = indexer.fit(df).transform(df).drop("proto", "service", "conn_state", "history")

    # One-hot encode the categorical features
    df = df.withColumns(oneHot(df, "protoIdx")).drop("protoIdx")
    df = df.withColumns(oneHot(df, "serviceIdx")).drop("serviceIdx")
    df = df.withColumns(oneHot(df, "conn_stateIdx")).drop("conn_stateIdx")
    df = df.withColumns(oneHot(df, "historyIdx")).drop("historyIdx")

    # Convert the labels to boolean floats
    df = df.withColumn(
        "label", F.when(F.col("label") == "Malicious", 1.0).otherwise(0.0)
    )

    # Apply logarithmic scaling
    df = df.withColumns(
        {
            "duration": F.log(F.col("duration") + 1),
            "source_bytes": F.log(F.col("source_bytes") + 1),
            "dest_bytes": F.log(F.col("dest_bytes") + 1),
            "missed_bytes": F.log(F.col("missed_bytes") + 1),
            "source_pkts": F.log(F.col("source_pkts") + 1),
            "source_ip_bytes": F.log(F.col("source_ip_bytes") + 1),
            "dest_pkts": F.log(F.col("dest_pkts") + 1),
            "dest_ip_bytes": F.log(F.col("dest_ip_bytes") + 1),
        }
    )

    # Apply min-max scaling
    min_max_df = df.agg(
        F.min("visits_src_ip_1m").alias("min_visits_1m"),
        F.max("visits_src_ip_1m").alias("max_visits_1m"),
        F.min("visits_src_ip_30m").alias("min_visits_30m"),
        F.max("visits_src_ip_30m").alias("max_visits_30m"),
    ).collect()[0]

    min_visits_1m = min_max_df["min_visits_1m"]
    max_visits_1m = min_max_df["max_visits_1m"]
    min_visits_30m = min_max_df["min_visits_30m"]
    max_visits_30m = min_max_df["max_visits_30m"]

    df = df.withColumns(
        {
            "visits_src_ip_1m": (F.col("visits_src_ip_1m") - min_visits_1m)
            / (max_visits_1m - min_visits_1m),
            "visits_src_ip_30m": (F.col("visits_src_ip_30m") - min_visits_30m)
            / (max_visits_30m - min_visits_30m),
        }
    )

    # Split into train and test sets
    df = df.withColumn("source_ip_hash", F.hash("source_ip"))
    df = df.withColumn(
        "test_set", F.when(F.col("source_ip_hash") < -381107000, 1.0).otherwise(0.0)
    )
    df = df.drop("source_ip", "source_ip_hash")
    df_validation = df.filter(F.col("test_set") == 1.0)
    df_train = df.filter(F.col("test_set") != 1.0)

    # Write train and test data to S3
    df_validation.coalesce(1).write.csv(
        f"s3://{s3_bucket_name}/validation", mode="overwrite", header=True
    )
    df_train.coalesce(1).write.csv(
        f"s3://{s3_bucket_name}/train", mode="overwrite", header=True
    )

    # Just for purpose of demonstration, create a separate test set for
    # comparing models. It must be saved without he 'label' and 'test_set'
    # columns. It cannot use headers too. The 'label' column must be saved to a
    # separate CSV as ground truth. The separate validation set above is
    # unnecessary as the scikit-learn SGDClassifier automatically uses a
    # fraction of the training data as a validation set (default 0.1).
    # Therefore, the data will be reused here.
    y_true = df_validation.select("label")
    y_true.coalesce(1).write.csv(
        f"s3://{s3_bucket_name}/groundtruth", mode="overwrite", header=False
    )
    df_test = df_validation.drop("label", "test_set")
    df_test.coalesce(1).write.csv(
        f"s3://{s3_bucket_name}/test", mode="overwrite", header=False
    )

    return


if __name__ == "__main__":
    main()
