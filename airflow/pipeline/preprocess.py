import sys
from pyspark.sql import SparkSession
from pyspark.sql.functions import (
    from_unixtime,
    to_timestamp,
    min,
    max,
    avg,
    col,
    count,
)
from pyspark.sql import Window
import pyspark.sql.functions as F
from pyspark.ml.feature import StringIndexer


def window_ops(column: str, operation: str ='count', time: str = 'datetime', minutes: int = 1, partition_by = None):
    """Counts or averages via a rolling aggregate over a specified window.

    Args:
        column (str): The column to perform the aggregation on.
        operation (str): The aggregation operation ('count' or 'avg').
        time (str): The column containing timestamp data.
        minutes (int): The window size in minutes for the rolling aggregate.
        partition_by (str, optional): The column to partition by. Defaults to
            None, in which case it uses `column`.

    Returns:
        Column: A PySpark Column object representing the rolling aggregate.

    Raises:
        ValueError: If the specified operation is not supported.
    """

    if partition_by is None:
        partition_by = column

    window = Window.partitionBy(F.col(partition_by)).orderBy(F.col(time).cast('long')).rangeBetween(-60 * minutes, -1)

    operations = {
        'count': F.count(column).over(window),
        'avg': F.avg(column).over(window),
    }

    if operation in operations:
        return operations[operation]
    else:
        raise ValueError(f"{operation} not defined")

def oneHot(df, column):
    values = df.select(column).distinct().toPandas()[column].tolist()
    values.sort()
    values.pop(0)

    new_columns = {}
    for value in values:
        new_columns[f'{column}_{value}'] = F.when(col(column) == value, 1.0).otherwise(0.0)
    return new_columns

def main():
    # Check if at least one argument is provided (excluding the script name)
    if len(sys.argv) < 2:
        print('Usage: python preprocess.py <s3_bucket_name> <list_of_s3_file_paths>')
        sys.exit(1)  # Exit the script with a non-zero exit code to indicate an error

    # The second element in sys.argv is the first argument
    s3_bucket_name = sys.argv[1]

    # Connect to Spark applciation
    spark = (
        SparkSession.builder.appName('ml_preprocess')
        .getOrCreate()
    )
    spark.sparkContext.setLogLevel('ERROR')

    # Colelct data from S3
    s3_files = sys.argv[2]
    datasets = s3_files.replace("['", '').replace("']", '').split(',')

    # Read CSV files into a Spark DataFrame
    df = spark.read.option('delimiter', '|').csv(datasets, inferSchema = True, header = True)

    # Ensure feature names have clear and obvious meaning
    df = df.toDF(
        'unix_timestamp',
        'unique_id',
        'source_ip',
        'source_port',
        'dest_ip',
        'dest_port',
        'proto',
        'service',
        'duration',
        'source_bytes',
        'dest_bytes',
        'conn_state',
        'local_source',
        'local_dest',
        'missed_bytes',
        'history',
        'source_pkts',
        'source_ip_bytes',
        'dest_pkts',
        'dest_ip_bytes',
        'tunnel_parents',
        'label',
        'detailed-label',
    )

    # Ensure time variable uses clear representation
    df = df.withColumn('datetime', from_unixtime('unix_timestamp')).withColumn('datetime', to_timestamp('datetime'))
    df = df.drop('unix_timestamp')

    # Ensure missing data is properly represented
    df = df.replace('-', None)

    # Drop duplicates
    df = df.dropDuplicates()

    # Drop empty columns
    empty_columns = ['local_source', 'local_dest', 'tunnel_parents']
    df = df.drop(*empty_columns)

    # Drop unnecessary columns
    unnecessary_columns = ['unique_id', 'detailed-label']
    df = df.drop(*unnecessary_columns)

    # Convert columns to appropriate data types
    df = df.withColumns({
        'source_ip': col('source_ip').cast('string'),
        'source_port': col('source_port').cast('string'),
        'dest_ip': col('dest_ip').cast('string'),
        'dest_port': col('dest_port').cast('string'),
        'proto': col('proto').cast('string'),
        'service': col('service').cast('string'),
        'duration': col('duration').cast('double'),
        'source_bytes': col('source_bytes').cast('double'),
        'dest_bytes': col('dest_bytes').cast('double'),
        'conn_state': col('conn_state').cast('string'),
        'missed_bytes': col('missed_bytes').cast('double'),
        'history': col('history').cast('string'),
        'source_pkts': col('source_pkts').cast('double'),
        'source_ip_bytes': col('source_ip_bytes').cast('double'),
        'dest_pkts': col('dest_pkts').cast('double'),
        'dest_ip_bytes': col('dest_ip_bytes').cast('double'),
        'label': col('label').cast('string'),
    })

    # Handle mising numeric data
    # Add indicator features
    df = df.withColumns({
        'is_duration_defined': F.when(F.isnan('duration') | col('duration').isNull(), 0.0).otherwise(1.0),
        'is_source_bytes_defined': F.when(F.isnan('source_bytes') | col('source_bytes').isNull(), 0.0).otherwise(1.0),
        'is_dest_bytes_defined': F.when(F.isnan('dest_bytes') | col('dest_bytes').isNull(), 0.0).otherwise(1.0),
    })

    # Fill in missing numeric data with the mean
    df = df.fillna({
        'duration': df.select(avg('duration')).first()[0],
        'source_bytes': df.select(avg('source_bytes')).first()[0],
        'dest_bytes': df.select(avg('dest_bytes')).first()[0],
    })

    # Handle missing categorical data
    # Add indicator features and set missing values to 'missing'
    df = df.withColumns({
        'is_service_defined': F.when(F.isnan('service') | col('service').isNull(), 0.0).otherwise(1.0),
        'is_history_defined': F.when(F.isnan('history') | col('history').isNull(), 0.0).otherwise(1.0),
    }).fillna({'service': 'missing', 'history': 'missing'})

    # Set categorical values that occur less than 5 times to 'other'
    set_as_other = df.groupBy('history').count().where('count < 5').toPandas()['history'].tolist()
    df = df.withColumn('history', F.when(F.col('history').isin(set_as_other), 'other').otherwise(col('history')))

    # Select only data labeled either 'Benign' or 'Malicious'
    df = df.where("label in ('Benign', 'Malicious')")

    # Create two new features showing number of visits in 1m and 30m sliding
    # windows
    df = df.withColumns({
        'visits_src_ip_1m': window_ops('source_ip', minutes=1),
        'visits_src_ip_30m': window_ops('source_ip', minutes=30),
    })

    # Drop unnecessary features
    df = df.drop('dest_ip', 'datetime')

    # Index categorical features to integers
    indexer = StringIndexer(inputCols=['proto', 'service', 'conn_state', 'history'],
                            outputCols=['protoIdx', 'serviceIdx', 'conn_stateIdx', 'historyIdx'])
    df = indexer.fit(df).transform(df).drop('proto', 'service', 'conn_state', 'history')

    # One-hot encode the categorical features
    df = df.withColumns(oneHot(df, 'protoIdx')).drop('protoIdx')
    df = df.withColumns(oneHot(df, 'serviceIdx')).drop('serviceIdx')
    df = df.withColumns(oneHot(df, 'conn_stateIdx')).drop('conn_stateIdx')
    df = df.withColumns(oneHot(df, 'historyIdx')).drop('historyIdx')

    # Convert the labels to boolean floats
    df = df.withColumn('label', F.when(F.col('label') == 'Malicious', 1.0).otherwise(0.0))

    # Apply logarithmic scaling
    df = df.withColumns({
        'duration': F.log(F.col('duration') + 1),
        'source_bytes': F.log(F.col('source_bytes') + 1),
        'dest_bytes': F.log(F.col('dest_bytes') + 1),
        'missed_bytes': F.log(F.col('missed_bytes') + 1),
        'source_pkts': F.log(F.col('source_pkts') + 1),
        'source_ip_bytes': F.log(F.col('source_ip_bytes') + 1),
        'dest_pkts': F.log(F.col('dest_pkts') + 1),
        'dest_ip_bytes': F.log(F.col('dest_ip_bytes') + 1),
    })

    # Apply min-max scaling
    min_max_df = df.agg(
        F.min("visits_src_ip_1m").alias("min_visits_1m"),
        F.max("visits_src_ip_1m").alias("max_visits_1m"),
        F.min("visits_src_ip_30m").alias("min_visits_30m"),
        F.max("visits_src_ip_30m").alias("max_visits_30m"),
    ).collect()[0]

    min_visits_1m = min_max_df['min_visits_1m']
    max_visits_1m = min_max_df['max_visits_1m']
    min_visits_30m = min_max_df['min_visits_30m']
    max_visits_30m = min_max_df['max_visits_30m']

    df = df.withColumns({
        'visits_src_ip_1m': (F.col('visits_src_ip_1m') - min_visits_1m) / (max_visits_1m - min_visits_1m),
        'visits_src_ip_30m': (F.col('visits_src_ip_30m') - min_visits_30m) / (max_visits_30m - min_visits_30m),
    })

    # Split into train and test sets
    df = df.withColumn('source_ip_hash', F.hash('source_ip'))
    df = df.withColumn('test_set', F.when(F.col('source_ip_hash') < -381107000, 1.0).otherwise(0.0))
    df = df.drop('source_ip', 'source_ip_hash')
    df_validation = df.filter(F.col('test_set') == 1.0)
    df_train = df.filter(F.col('test_set') != 1.0)

    # Write train and test data to S3
    df_validation.coalesce(1).write.csv(f's3://{s3_bucket_name}/validation', mode='overwrite', header=True)
    df_train.coalesce(1).write.csv(f's3://{s3_bucket_name}/train', mode='overwrite', header=True)

    # Just for purpose of demonstration, create a separate test set for
    # comparing models. It must be saved without he 'label' and 'test_set'
    # columns. It cannot use headers too. The 'label' column must be saved to a
    # separate CSV as ground truth. The separate validation set above is
    # unnecessary as the scikit-learn SGDClassifier automatically uses a
    # fraction of the training data as a validation set (default 0.1).
    # Therefore, the data will be reused here.
    y_true = df_validation.select('label')
    y_true.coalesce(1).write.csv(f's3://{s3_bucket_name}/groundtruth', mode='overwrite', header=False)
    df_test = df_validation.drop('label', 'test_set')
    df_test.coalesce(1).write.csv(f's3://{s3_bucket_name}/test', mode='overwrite', header=False)

    return

if __name__ == "__main__":
    main()
