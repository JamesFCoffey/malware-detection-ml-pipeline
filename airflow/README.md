# Airflow Project Setup Documentation

This README provides a step-by-step guide on setting up a Managed Apache
Airflow (MWAA) environment using AWS and configuring an Airflow project.

## Directory Structure

This project contains the necessary files and scripts to run Airflow DAGs:

```
airflow/
├── Dockerfile                - Docker configuration for setting up Airflow.
├── README.md                 - Documentation for setting up and managing the Airflow project.
├── benchmark.txt             - Benchmark data for use within DAGs or for testing.
├── constraints.txt           - Constraints file for Python package versions.
├── dags/                     - Contains all the DAG (Directed Acyclic Graph) files for Airflow.
│   ├── classic_syntax.py     - Airflow DAG written in classic syntax.
│   └── utils/                - Utilities and configurations for DAGs.
│       ├── __init__.py       - Initializes the utils directory as a Python package.
│       ├── config.py         - Configuration settings for AWS and Airflow resources.
│       └── helper_functions.py - Helper functions utilized by DAGs.
├── iam/                      - IAM (Identity and Access Management) policies and trust relationships.
│   ├── AmazonMWAA-MyAirflowEnvironment-access-policy.json - Access policy specifically for the Airflow environment.
│   ├── emr-access-policy.json    - Access policies for EMR (Elastic MapReduce).
│   └── emr-serverless-trust-policy.json - Trust policies for EMR Serverless.
├── pipeline/                 - Scripts for data preprocessing and machine learning model training.
│   ├── preprocess.py         - Script for preprocessing data.
│   └── train.py              - Script for training machine learning models.
└── requirements.txt          - Specifies Python packages required by Airflow.
```

## AWS S3 Configuration

1. **Log in to the AWS Console** and navigate to **S3**.
2. **Create a new bucket** for Airflow:
   - Click "Create Bucket".
   - Name the bucket `airflow-<username>`.
   - Keep other settings as default and click "Create bucket".
3. **Upload the `requirements.txt`**:
   - Place the `requirements.txt` file at the root of your new bucket.
4. **Upload DAGs**:
   - Navigate to the `airflow` directory from this GitHub repository you've
     cloned.
   - Upload the entire `dags` directory, including all subdirectories and files,
     to your new S3 bucket under `airflow-<username>/dags/`. This should
     include:
     - `classic_syntax.py` - Sample DAG file.
     - `utils/` - Directory containing utility scripts and the configuration
       file.
5. **Upload Pipeline Scripts**:
   - Additionally, upload the `pipeline` directory containing the
     `preprocess.py` script to `airflow-<username>/pipeline/`. Ensure it mirrors
     the structure in your repository:
     - `pipeline/`
       - `preprocess.py` - Script for data preprocessing.
6. **Create a secondary S3 bucket** for datasets and outputs:
   - Name the secondary bucket `airflow-<username>-data`.
   - Create the following directories inside this bucket: `benchmark/`,
     `groundtruth/`, `predictions/`, `raw/`, `test/`, `train/`, `validation/`.
   - Upload `benchmark.txt` from the repository to the `benchmark/` directory.
   - Download the Kaggle dataset "Malware Detection in Network Traffic Data"
     from
     [here](https://www.kaggle.com/datasets/agungpambudi/network-malware-detection-connection-analysis)
     and upload CSV files to the `raw/` directory.
      - **Dataset Credit**: Stratosphere Laboratory. A labeled dataset with
        malicious and benign IoT network traffic. January 22th. Agustin
        Parmisano, Sebastian Garcia, Maria Jose Erquiaga.
        [https://www.stratosphereips.org/datasets-iot23](https://www.stratosphereips.org/datasets-iot23).

### Notes for Customization and Understanding Dependencies

1. **Training Script from GitHub**:
   - The `train.py` script is referenced from a GitHub repository in the
     `prepare_configs` function defined in
     `airflow/dags/utils/helper_functions.py`. This function configures a
     SageMaker SKLearn estimator with the script sourced directly from:

     ```python
     estimator = SKLearn(
         role=role,
         instance_count=1,
         instance_type="ml.m5.xlarge",
         source_dir="airflow/pipeline",
         entry_point="train.py",
         git_config={
             "repo": "https://github.com/JamesFCoffey/malware-detection-ml-pipeline.git"
         },
         hyperparameters=hyperparameters,
         metric_definitions=metric_definitions,
         framework_version="1.2-1",
         py_version="py3",
     )
     ```

     If you need to use a different version of the `train.py` or a different
     repository, modify the `git_config` parameter accordingly.

2. **Requirements File Dependency**:
   - The `requirements.txt` file references a `constraints.txt` located in a
     GitHub repository. This is specified within the `requirements.txt` as:

     ```
     --constraint "https://raw.githubusercontent.com/JamesFCoffey/malware-detection-ml-pipeline/main/airflow/constraints.txt"
     ```

     This ensures that all Python packages are installed with versions
     compatible with the constraints specified in the `constraints.txt`. If you
     wish to customize or update the constraints, you may edit this URL or
     modify the `constraints.txt` file directly in the repository.

## Setting Up Managed Apache Airflow (MWAA)

1. **Navigate to Managed Apache Airflow** in the AWS Console and create a new
   environment:
   - Set the environment name to "MyAirflowEnvironment".
   - Select Airflow version "2.8.1".
   - Schedule a weekly maintenance window at a convenient time.

2. **DAG Code in Amazon S3**:
   - Set the S3 bucket to `airflow-<username>`.
   - Set the DAGs folder to `airflow-<username>/dags`.
   - Set the requirements file to `airflow-<username>/requirements.txt`.
   - Click "Next"  to proceed to "Configure Advanced Settings".

3. **Networking**:
   - If you do not already have a dedicated VPC for MWAA, I recommend selecting
     "Create MWAA VPC". This choice will ensure the correct configuration of
     your Airflow resources within AWS.
     - In the MWAA setup interface, select "Create MWAA VPC". This action will
       open AWS CloudFormation in a new tab.
     - Proceed with the default settings in CloudFormation and click "Create
       stack". This process typically takes about 2.5 minutes.
     - Once the stack creation completes, navigate to the "Resources" tab within
       the CloudFormation stack.
     - Scroll to find the Logical ID labeled "VPC" and note down the value of
       the Physical ID associated with it.
     - Return to the MWAA environment configuration tab in your browser.
     - Click the refresh button under "Virtual private cloud (VPC)" and then set
       the VPC to the noted Physical ID. Subnets 1 and Subnet 2 should be
       automatically set by the CloudFormation stack.
   - Set web server access to "Public network (Internet accessible)".
      - **Note**: While "Public network (Internet accessible)" is convenient, it
        may not be suitable for all security requirements. Depending on the
        sensitivity of your workflows and organizational security policies,
        consider restricting access to a more controlled network setting.
   - Select "Create new security group".
   - Leave "Service managed endpoints" selected.

4. **Defaults**
   - Keep "Environment class" set to `mw1.small` with the following settings.
      - Maximum worker count = `10`
      - Minimum worker count = `1`
      - Scheduler count = `2`
   - Under "Encryption", keep "Customize encryption settings (advanced)"
     unselected.
   - "Monitoring" should be kept to "Airflow task logs" at a log level "INFO".
   - "Airflow configuration options" and "Tags" should be empty.

5. **Permissions**:
   - Select "Create a new role". It should be of the format
     "AmazonMWAA-MyAirflowEnvironment-\<xxxxxx\>".
   - Click "Next"  to proceed to "Review and create".

6. **Review and Create the Environment**:
   - Ensure all settings are correct.
   - Click "Create environment".
   - Continue with the the instruction steps below as the environment setup
     should take about 26 minutes.

## Setting Up EMRServerlessS3RuntimeRole in IAM

### Create an IAM Policy

1. **Navigate to IAM**: In the AWS Console, navigate to the Identity
   and Access Management (IAM) service.
2. **Access Management**: On the left-hand menu, click on **Policies** under
   "Access management".
3. **Create a New Policy**:
   - Click on **Create policy**.
   - Select the **JSON** tab to start with a blank policy template.
   - Open the `emr-access-policy.json` file found in your cloned repo under
     `airflow/iam/`.
   - Modify the resource ARNs in the JSON file to match the names of your S3
     buckets:
     - Replace `"arn:aws:s3:::<AIRFLOW_S3_BUCKET_NAME>"` and
       `"arn:aws:s3:::<DATASET_S3_BUCKET_NAME>"` with your actual bucket names.
   - Copy the modified JSON content and paste it into the policy editor in the
     AWS console.
   - Click **Next**.
   - Enter a meaningful name for this policy, e.g.,
     `EMRServerlessS3AccessPolicy`.
   - Optionally, provide a description for the policy.
   - Click **Create policy**.

### Create an IAM Role

1. **Create Role**:
   - In the IAM service, click on **Roles** under "Access management" on the
     left-hand menu.
   - Click **Create role**.
   - Select **Custom trust policy** under "Trusted entity type".
   - Open the `emr-serverless-trust-policy.json` file from your `airflow/iam/`
     directory.
   - Copy the content of the trust policy and paste it into the policy editor in
     the AWS console.
   - Click **Next**.
2. **Attach Permissions**:
   - Under "Permissions policies", search for the policy you created earlier
     (`EMRServerlessS3AccessPolicy`).
   - Select the checkbox next to the policy to attach it to the role.
   - Click **Next**.
3. **Finalize Role Creation**:
   - Enter a meaningful name for the role, such as `EMRServerlessS3RuntimeRole`.
   - Provide a description, e.g., "Role with policies for using EMR Serverless
     with an Airflow pipeline."
   - Click **Create role**.

## Setting Up SageMaker

### Create a SageMaker Domain

1. **Navigate to Amazon SageMaker**:
   - Log into the AWS Console.
   - Open the Amazon SageMaker service.

2. **Set Up Domain**:
   - Click on **Domains** under "Admin configurations" on the left-hand menu.
   - Click **Create domain**.
   - Select **Set up for single user (Quick setup)**.
   - Click **Set up**.

3. **Domain Configuration**:
   - Once the domain has been created, click on it to view the **Domain
     details**.
   - Navigate to the **Domain settings** tab.
   - Note the **Execution role** listed there which follows the format
     `AmazonSageMaker-ExecutionRole-<xxxxxxxxxxxx>`. This role will be modified
     in the next steps to grant additional permissions.

### Modify the SageMaker Execution Role

1. **Access IAM for Role Configuration**:
   - Navigate to **Identity and Access Management (IAM)** in the AWS Console.
   - Click on **Roles** under "Access management" in the left-hand menu.

2. **Locate and Modify the Role**:
   - Use the search function to find the SageMaker execution role noted earlier.
     You might need to click the **refresh roles** button if the newly created
     role does not appear immediately.
   - Click on the role name.

3. **Add Permissions**:
   - Under **Permissions policies**, click **Add permissions**.
   - Choose **Attach policies** from the drop-down menu.
   - In the search bar, type and select **AmazonS3FullAccess** to allow the role
     to interact fully with Amazon S3 resources, which is crucial for storing
     and retrieving data and model artifacts.
     - **Note**: Ensure to review and adjust the permissions according to your
       organizational security policies, as granting full S3 access can expose
       sensitive data if not properly managed.
   - Click **Add permissions**.

## Modify the Amazon MWAA Execution Role

### Navigate to the MWAA Environment

1. **Access MWAA**:
   - In the AWS Console, navigate to **Amazon MWAA**.

2. **View Environment Details**:
   - Click on **Environments** in the left-hand menu.
   - Select the name of your Airflow environment, typically
     "MyAirflowEnvironment" if you have followed the setup instructions.
   - Scroll to the bottom of the environment details page to the **Permissions**
     section.
   - Note the **Execution Role** listed under "Permissions," typically in the
     format `AmazonMWAA-MyAirflowEnvironment-<xxxxxx>`.

### Modify the IAM Role

1. **Access IAM**:
   - Open a new tab in your browser and navigate to **Identity and Access
     Management (IAM)** within the AWS Console.

2. **Find and Modify the Role**:
   - Click on **Roles** under "Access management" on the left-hand menu.
   - Use the search function to find the execution role you noted earlier.
   - Click on the role name.

3. **Edit Role Permissions**:
   - Under **Permissions policies**, find the only policy listed and click on
     the policy name.
   - On the policy summary page, click **Edit**.
   - Switch to the **JSON** tab in the "Policy editor".

4. **Update Policy JSON**:
   - Open the file `AmazonMWAA-MyAirflowEnvironment-access-policy.json` from the
     `airflow/iam/` directory in your local repo.
   - Replace the placeholders in the policy JSON with actual values:
     - `<AWS_ACCOUNT_ID>` with your AWS Account ID.
     - `<REGION>` with your AWS Region.
     - `<AIRFLOW_S3_BUCKET_NAME>` with your Airflow S3 bucket name, typically
       `airflow-<username>`.
     - `<DATASET_S3_BUCKET_NAME>` with your dataset S3 bucket name, typically
       `airflow-<username>-data`.
     - `<EMR_ROLE_NAME>` and `<SAGEMAKER_ROLE_NAME>` with the actual role names
       you have configured previously for EMR and SageMaker, respectively.
   - Copy the modified policy JSON into the AWS policy editor.

5. **Save Changes**:
   - After ensuring all placeholders are correctly replaced, click **Review
     policy**.
   - Verify the changes and then click **Save changes** to update the policy.

### Notes on Configuration

- **S3 Bucket Names**: The `<AIRFLOW_S3_BUCKET_NAME>` and
  `<DATASET_S3_BUCKET_NAME>` are your primary and dataset-specific S3 buckets,
  respectively, used for storing everything from logs to datasets.
- **Role Names**: The `<EMR_ROLE_NAME>` and `<SAGEMAKER_ROLE_NAME>` should
  reflect the roles created to give EMR and SageMaker the necessary permissions
  for operation.
- **Region Specific**: Ensure the `<REGION>` placeholder matches the AWS region
  in which your services are hosted to avoid any regional discrepancies in
  resource access.

## Pipeline Configuration

### Edit the `config.py` File

1. **Retrieve the Config File**:
   - The `config.py` file, which was previously uploaded to your
     `airflow-<username>` S3 bucket, needs to be updated with actual values
     you've collected throughout the setup process.
   - Download the file from the S3 bucket located at `dags/utils/config.py` or
     access it directly in your cloned repository at
     `airflow/dags/utils/config.py`.

2. **Edit Configuration Values**:
   - Open the `config.py` file in a text editor of your choice.
   - Replace the placeholders with the actual configuration values:
     - `<AWS_ACCOUNT_ID>`: Your AWS account ID.
     - `<AIRFLOW_S3_BUCKET_NAME>`: The name of the S3 bucket used by Airflow to
       store logs and data, typically `airflow-<username>`.
     - `<DATASET_S3_BUCKET_NAME>`: The name of the S3 bucket dedicated to
       storing datasets for ML training, typically `airflow-<username>-data`.
     - `<EMR_ROLE_NAME>`: The role name configured for EMR Serverless
       applications.
     - `<SAGEMAKER_ROLE_NAME>`: The role name configured for AWS SageMaker,
       included in the SageMaker execution role ARN.
   - Ensure that the ARN formats in the file reflect the exact ARN formats used
     in AWS IAM for EMR and SageMaker roles.

3. **Example of Edited `config.py`**:
   ```python
   # config.py
   config = {
       "account_id": "123456789012",
       "airflow_bucket": "airflow-johndoe",
       "dataset_bucket": "airflow-johndoe-data",
       "emr_role_arn": "arn:aws:iam::123456789012:role/EMRServerlessS3RuntimeRole",
       "default_monitoring_config": {
           "monitoringConfiguration": {
               "s3MonitoringConfiguration": {"logUri": "s3://airflow-johndoe/logs/"}
           },
       },
       "sagemaker_role_arn": "arn:aws:iam::123456789012:role/service-role/AmazonSageMaker-ExecutionRole-20240224T215408",
   }
   ```

### Update the Config File in S3

1. **Upload the Updated Config File**:
   - After making the necessary changes to the `config.py`, save the file.
   - Upload the updated file back to your `airflow-<username>` S3 bucket,
     specifically to the path `dags/utils/config.py`.
   - You can use the AWS Management Console, AWS CLI, or any S3 client tool to
     upload the file.

2. **Verify the Upload**:
   - Ensure that the updated `config.py` is correctly placed in the S3 bucket.
   - You can check this by navigating to the `airflow-<username>` bucket in the
     AWS S3 console and locating the file under `dags/utils/`.


## Create Virtual Environment for EMR Serverless

For more details, refer to the AWS documentation on ["Using Python libraries
with EMR
Serverless"](https://docs.aws.amazon.com/emr/latest/EMR-Serverless-UserGuide/using-python-libraries.html).

### Building the Docker Image

1. **Navigate to the Directory**:
   - Open your terminal and navigate to the `airflow/` directory where the
     `Dockerfile` is located. This `Dockerfile` should be set up to create a
     PySpark environment. Make sure your Dockerfile is properly configured to
     install PySpark and package it.

2. **Build the Image**:
   - Run the following Docker command to build the image and output the
     necessary environment files:
     ```bash
     docker build --output . .
     ```
   - This command tells Docker to build the image based on the Dockerfile in the
     current directory and output the result (in this case, a virtual
     environment archive) to the current directory.

### Handling the Output

3. **Locate the Archive**:
   - After the build process completes, look for the `pyspark_venv.tar.gz` file
     in your current directory. This archive contains the virtual environment
     set up by the Docker build process.

4. **Upload the Archive to S3**:
   - Use the AWS CLI to upload the virtual environment archive to your Airflow
     S3 bucket.

     ```bash
     aws s3 cp pyspark_venv.tar.gz s3://airflow-<username>/venv/
     ```

   - Make sure your AWS CLI is configured with sufficient permissions to perform
     the upload.

### Optional Cleanup

5. **Remove the Virtual Environment Directory**:
   - If you wish to clean up your local directory and remove the virtual
     environment that was created during the Docker build process, you can
     safely delete the directory:

     ```bash
     rm pyspark_venv.tar.gz
     ```

## Accessing the Airflow UI

- Once the environment is ready, open the Airflow UI from the MWAA console.
- Trigger the `sagemaker-ml-pipeline` DAG by clicking on the "Trigger DAG"
  button.
- The DAG should take under 2 hours to complete.